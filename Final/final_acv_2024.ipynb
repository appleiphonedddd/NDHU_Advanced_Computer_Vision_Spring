{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_1 : \"find_this_mii\"\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "1. **Read Frame and Select ROI (Region of Interest)**:\n",
    "   $$\n",
    "   \\text{frame} = \\text{cap.read}(\\text{frame\\_number})\n",
    "   $$\n",
    "   $$\n",
    "   \\text{ROI} = \\text{selectROI}(\\text{frame})\n",
    "   $$\n",
    "   Here, the user selects a rectangular region $ROI$ in the frame. Let's denote this region by its coordinates $(x, y, w, h)$, where $x$ and $y$ are the top-left coordinates, and $w$ and $h$ are the width and height of the region.\n",
    "\n",
    "   The template image $T$ is then:\n",
    "   $$\n",
    "   T = \\text{frame}[y:y+h, x:x+w]\n",
    "   $$\n",
    "\n",
    "\n",
    "2. **Frame Processing Loop**:\n",
    "   For each frame $F_{\\text{i}}$ in the video (where $i$ is the frame index), the loop processes until frame number 5000:\n",
    "   $$\n",
    "   F_i = \\text{cap.read}(i)\n",
    "   $$\n",
    "   If \\(i > 5000\\), exit the loop.\n",
    "\n",
    "3. **Apply Gaussian Blur**:\n",
    "   $$\n",
    "   F_i^{\\text{blur}} = \\text{GaussianBlur}(F_i, (5, 5), 0)\n",
    "   $$\n",
    "   This step applies a Gaussian filter to the frame to reduce noise.\n",
    "\n",
    "4. **Template Matching**:\n",
    "   $$\n",
    "   \\text{res} = \\text{matchTemplate}(F_i^{\\text{blur}}, T, \\text{TM\\_CCOEFF\\_NORMED})\n",
    "   $$\n",
    "   The `matchTemplate` function computes the normalized cross-correlation between the template $T$ and the current frame $F_i^blur$. The result is a matrix $res$ where each element represents the correlation coefficient at that point.\n",
    "   \n",
    "   $$\n",
    "   (\\min_{\\text{val}}, \\max_{\\text{val}}, \\min_{\\text{loc}}, \\max_{\\text{loc}}) = \\text{minMaxLoc}(\\text{res})\n",
    "   $$\n",
    "   \n",
    "   This function finds the minimum and maximum values and their locations in the result matrix. We are interested in $loc$, the location of the highest correlation.\n",
    "\n",
    "   Let:\n",
    "   $$\n",
    "   \\text{top\\_left} = \\max_{\\text{loc}}\n",
    "   $$\n",
    "   $$\n",
    "   \\text{bottom\\_right} = (\\text{top\\_left}[0] + w, \\text{top\\_left}[1] + h)\n",
    "   $$\n",
    "\n",
    "In summary, the mathematical operations involve:\n",
    "- Extracting a template image from a specific region in a frame.\n",
    "- Applying Gaussian blur to reduce noise.\n",
    "- Using normalized cross-correlation to find the best match of the template in subsequent frames.\n",
    "- Identifying the location of the best match and drawing a rectangle around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "1A. Input images from video file WiiPlay.mp4 with level 15 (frame number between 4820 and 5000).<br> \\\n",
    "1B. (5pts) Acquire a <b>face template</b> from the first frame (frame number = 4820).<br>\\\n",
    "1C. (10pts) Try to detect the face the same as the template on subsequent frames, draw a <b>red</b> rectangle around the detected face, and show the output images in the <b>\"find_this_mii\"</b> window.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    }
   ],
   "source": [
    "#game_1 : \"find_this_mii\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Set the current video frame to 4820\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 4820)\n",
    "\n",
    "# Capture a single frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Select a region of interest (ROI) manually from the captured frame for template matching\n",
    "r = cv2.selectROI(frame)\n",
    "template = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if frame cannot be read or the current frame exceeds 5000\n",
    "    if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) > 5000:\n",
    "        break\n",
    "\n",
    "    # Apply Gaussian Blur to the frame to reduce noise for better template matching\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Perform template matching to find the template in the frame\n",
    "    res = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # Find the location of the template in the frame\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    top_left = max_loc\n",
    "    h, w, _ = template.shape\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    # Draw a rectangle around the template in the frame\n",
    "    cv2.rectangle(frame, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame with the rectangle around the template\n",
    "    cv2.imshow('find_this_mii', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Demo Video of game_1 : \"find_this_mii](https://drive.google.com/file/d/1VIsyU3AEM1pPiEQ2tlJHoIHCoZ2YTVhm/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_2 : \"find_two_look_alike\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Convert the frame to the HSV color space to detect skin colors.\n",
    "   - Apply a binary mask to isolate skin-colored areas.\n",
    "\n",
    "2. **Morphological Operations:**\n",
    "   - Erode and dilate the mask to remove noise.\n",
    "   - Apply Gaussian blur to smooth the mask.\n",
    "\n",
    "3. **Contour Detection:**\n",
    "   - Find contours in the mask.\n",
    "   - Filter contours based on area and aspect ratio to identify potential faces.\n",
    "\n",
    "4. **Face Similarity Detection:**\n",
    "   - Compare each detected face with every other detected face using template matching.\n",
    "   - Mark pairs of faces that are similar.\n",
    "\n",
    "\n",
    "## Mathematical Explanation of Face Detection:\n",
    "\n",
    "#### Convert to HSV:\n",
    "- Frame $F$ is converted from BGR to HSV.\n",
    "  $$\n",
    "  F_{HSV} = \\text{HSV}(F)\n",
    "  $$\n",
    "\n",
    "#### Binary Mask:\n",
    "- Create a binary mask $M$ using threshold values for skin color.\n",
    "  $$\n",
    "  M = \\begin{cases}\n",
    "  1, & \\text{if } \\text{lower\\_skin} \\leq F_{HSV} \\leq \\text{upper\\_skin} \\\\\n",
    "  0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "#### Morphological Operations:\n",
    "- Erode $M$ with a kernel $K$:\n",
    "  $$\n",
    "  M' = \\text{erode}(M, K)\n",
    "  $$\n",
    "- Dilate $M'$ with $K$:\n",
    "  $$\n",
    "  M'' = \\text{dilate}(M', K)\n",
    "  $$\n",
    "- Apply Gaussian blur:\n",
    "  $$\n",
    "  M_{smooth} = \\text{GaussianBlur}(M'', (3, 3), 0)\n",
    "  $$\n",
    "\n",
    "#### Contour Detection:\n",
    "- Find contours $C$ in $M_{smooth} $:\n",
    "  $$\n",
    "  C = \\text{contours}(M_{smooth})\n",
    "  $$\n",
    "- For each contour   $c \\in C$:\n",
    "  - Compute bounding rectangle $( x, y, w, h )$.\n",
    "  - Compute area $A$ and aspect ratio $R$:\n",
    "    $$\n",
    "    A = w \\times h\n",
    "    $$\n",
    "    $$\n",
    "    R = \\frac{w}{h}\n",
    "    $$\n",
    "  - Filter based on area and aspect ratio:\n",
    "    $$\n",
    "    \\text{if } \\text{min\\_area} < A < \\text{max\\_area} \\text{ and } \\text{aspect\\_ratio\\_range}[0] < R < \\text{aspect\\_ratio\\_range}[1] \\text{, append to faces}\n",
    "    $$\n",
    "\n",
    "## Mathematical Explanation of Face Similarity Matching:\n",
    "\n",
    "- For each pair of faces $f_i, f_j$:\n",
    "  - Extract face regions $f_i$ and $f_j$ from frame.\n",
    "  - Resize $f_j$ to match dimensions of $f_i$:\n",
    "    $$\n",
    "    F_{j,resized} = \\text{resize}(F_j, (w_i, h_i))\n",
    "    $$\n",
    "  - Convert to grayscale:\n",
    "    $$\n",
    "    F_{i,gray} = \\text{gray}(F_i)\n",
    "    $$\n",
    "    $$\n",
    "    F_{j,gray} = \\text{gray}(F_{j,resized})\n",
    "    $$\n",
    "  - Perform template matching:\n",
    "    $$\n",
    "    \\text{result} = \\text{matchTemplate}(F_{i,gray}, F_{j,gray}, \\text{TM\\_CCOEFF\\_NORMED})\n",
    "    $$\n",
    "  - Find maximum correlation value $max_val$:\n",
    "    $$\n",
    "    \\text{max\\_val} = \\max(\\text{result})\n",
    "    $$\n",
    "  - If $max_val$ > 0.32, consider faces similar and highlight them.\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with rectangles around detected faces and similar-looking faces highlighted with a different color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "2A. Input images from video file WiiPlay.mp4 with level 8 (frame number between 2180 and 2380).<br>\\\n",
    "2B. (5pts) Detect <b>pedestrians</b> on each frame and draw a <b>green</b> rectangle around your detection.<br>\\\n",
    "2C. (5pts) Detect <b>faces</b> on each frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "2D. (10pts) Try to find two faces look like each other, draw a <b>red</b> rectangle around each of the two faces, and show the output images in the <b>\"find_two_look_alike\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_2 : \"find_two_look_alike\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range):\n",
    "    \"\"\"\n",
    "    Detects faces in the given frame based on skin color and filters them by area and aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        frame (numpy.ndarray): The image frame in which to detect faces.\n",
    "        lower_skin (numpy.ndarray): The lower bound of the HSV color range for skin detection.\n",
    "        upper_skin (numpy.ndarray): The upper bound of the HSV color range for skin detection.\n",
    "        min_area (int): The minimum area of a detected face.\n",
    "        max_area (int): The maximum area of a detected face.\n",
    "        aspect_ratio_range (tuple): The range of acceptable aspect ratios for detected faces.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of detected faces and the processed frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the frame to the HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a binary mask of the skin\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Create a structuring element for morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "\n",
    "    # Erode and dilate the mask to remove noise\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the mask\n",
    "    mask = cv2.GaussianBlur(mask, (3, 3), 0)\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    faces = []\n",
    "    for contour in contours:\n",
    "\n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = w / float(h)\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Filter contours based on aspect ratio and area\n",
    "        if aspect_ratio_range[0] < aspect_ratio < aspect_ratio_range[1] and min_area < area < max_area:\n",
    "            faces.append((x, y, w, h))\n",
    "\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x-10, y-10), (x+w+10, y+h+10), (255, 0, 0), 2)\n",
    "    \n",
    "    return faces, frame\n",
    "\n",
    "def find_similar_faces(faces, frame):\n",
    "    \"\"\"\n",
    "    Find similar-looking faces in the given frame using template matching.\n",
    "\n",
    "    Args:\n",
    "        faces (list): A list of detected faces as (x, y, w, h) tuples.\n",
    "        frame (numpy.ndarray): The image frame in which to find similar faces.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of pairs of similar faces as ((x1, y1, w1, h1), (x2, y2, w2, h2)) tuples.\n",
    "    \"\"\"\n",
    "    similar_faces = []\n",
    "\n",
    "    for i in range(len(faces)):\n",
    "        (x1, y1, w1, h1) = faces[i]\n",
    "        face1 = frame[y1:y1+h1, x1:x1+w1]\n",
    "\n",
    "        for j in range(i + 1, len(faces)):\n",
    "            (x2, y2, w2, h2) = faces[j]\n",
    "            face2 = frame[y2:y2+h2, x2:x2+w2]\n",
    "\n",
    "            # Resize face2 to match the size of face1\n",
    "            face2_resized = cv2.resize(face2, (w1, h1))\n",
    "\n",
    "            # Convert faces to grayscale\n",
    "            face1_gray = cv2.cvtColor(face1, cv2.COLOR_BGR2GRAY)\n",
    "            face2_gray = cv2.cvtColor(face2_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Perform template matching\n",
    "            result = cv2.matchTemplate(face1_gray, face2_gray, cv2.TM_CCOEFF_NORMED)\n",
    "            _, max_val, _, _ = cv2.minMaxLoc(result)\n",
    "\n",
    "            # Check if the faces are similar based on a threshold\n",
    "            if max_val > 0.32:\n",
    "                similar_faces.append(((x1, y1, w1, h1), (x2, y2, w2, h2)))\n",
    "\n",
    "    return similar_faces\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Initialize HOG descriptor for people detection\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# Set the range of frames to process\n",
    "start_frame = 2180\n",
    "end_frame = 2380\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "# Define HSV color range for skin detection\n",
    "lower_skin = np.array([0, 48, 80], dtype='uint8')\n",
    "upper_skin = np.array([20, 255, 255], dtype='uint8')\n",
    "min_area = 300\n",
    "max_area = 1000\n",
    "aspect_ratio_range = (0.3, 3)\n",
    "\n",
    "# Initialize the faces list\n",
    "faces = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the frame is not read correctly or the end frame is reached\n",
    "    if not ret or current_frame > end_frame:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect people in the frame using HOG\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces, frame = detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range)\n",
    "\n",
    "    # Find similar faces in the frame\n",
    "    similar_faces = find_similar_faces(faces, frame)\n",
    "\n",
    "    for(face1, face2)in similar_faces:\n",
    "        (x1, y1, w1, h1) = face1\n",
    "        (x2, y2, w2, h2) = face2\n",
    "        cv2.rectangle(frame, (x1-10, y1-10), (x1+w1+10, y1+h1+10), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (x2-10, y2-10), (x2+w2+10, y2+h2+10), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame \n",
    "    cv2.imshow('find_two_look_alike', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Demo Video of game_2: \"find_two_look_alike\"](https://drive.google.com/file/d/1MxuWQbj5SEjfAsjeWfBkNV5UmdNABRRv/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_3 : \"find_the_fastest_character\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Load the video and set the frame range to analyze.\n",
    "   - Initialize the HOG descriptor for detecting people.\n",
    "\n",
    "2. **Detection and Tracking:**\n",
    "   - Detect people in the first frame and initialize bounding boxes.\n",
    "   - Create and initialize a multi-object tracker.\n",
    "\n",
    "3. **Frame-by-Frame Analysis:**\n",
    "   - Read each frame and update the tracker.\n",
    "   - Calculate the speed of each tracked person using the Euclidean distance.\n",
    "   - Identify and highlight the fastest person in the current frame.\n",
    "\n",
    "4. **Visualization:**\n",
    "   - Draw bounding boxes around all detected and tracked people.\n",
    "   - Display the current frame number on the video.\n",
    "\n",
    "### Mathematical Explanation of Speed Calculation:\n",
    "\n",
    "#### Euclidean Distance:\n",
    "- For each tracked person, calculate the speed between the current frame $(x, y)$ and the previous frame $(\\text{prev\\_x}, \\text{prev\\_y})$:\n",
    "  $$\n",
    "  \\text{speed} = \\sqrt{(x - \\text{prev\\_x})^2 + (y - \\text{prev\\_y})^2}\n",
    "  $$\n",
    "\n",
    "#### Steps:\n",
    "- Initialize positions:\n",
    "  - Let $ (x, y)$ be the current position.\n",
    "  - Let $ (\\text{prev\\_x}, \\text{prev\\_y})$ be the previous position.\n",
    "\n",
    "- Calculate speed for each tracked person:\n",
    "  $$\n",
    "  \\text{speed} = \\sqrt{(x - \\text{prev\\_x})^2 + (y - \\text{prev\\_y})^2}\n",
    "  $$\n",
    "\n",
    "- Identify the fastest speed:\n",
    "  - Track the maximum speed recorded so far.\n",
    "  - Update the fastest speed if the current speed is greater.\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with:\n",
    "- Bounding boxes around all detected and tracked people.\n",
    "- The fastest person highlighted with a different color.\n",
    "- The current frame number displayed on the video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "3A. Input images from video file WiiPlay.mp4 with level 9 (frame number between 2480 and 2600).<br>\\\n",
    "3B. (5pts) <b>Detect </b>faces(or pedestrians) on the first frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "3C. (10pts) <b>Track </b>faces(or pedestrians) on subsequent frames and draw a <b>green</b> rectangle around your tracking.<br>\\\n",
    "3D. (5pts) Try to find out the fastest character, draw a <b>red</b> rectangle around the fastest character, and show the output images in the <b>\"find_the_fastest_character\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_3 : \"find_the_fastest_character\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Define the frame range to analyze\n",
    "start_frame = 2480\n",
    "end_frame = 2600\n",
    "\n",
    "# Set the video to start at the specified frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video\")\n",
    "    exit()\n",
    "\n",
    "# Detect people in the first frame\n",
    "boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "\n",
    "# Draw bounding boxes around detected people\n",
    "for (x, y, w, h) in boxes:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Initialize the multi-object tracker\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "for box in boxes:\n",
    "    tracker = cv2.legacy.TrackerMIL_create()\n",
    "    trackers.add(tracker, frame, tuple(box))\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "fastest_speed = 0\n",
    "fastest_box = None\n",
    "\n",
    "# Store previous positions of detected people\n",
    "prev_position = [tuple(box) for box in boxes]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    # Update tracker and get updated positions\n",
    "    success, tracked_boxes = trackers.update(frame)\n",
    "\n",
    "    if success:\n",
    "        speeds = []\n",
    "\n",
    "        # Calculate speed for each tracked person\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "\n",
    "            # Calculate speed based on the distance moved\n",
    "            prev_x, prev_y, _, _ = prev_position[i]\n",
    "\n",
    "            # Calculate speed as the Euclidean distance between the current and previous positions\n",
    "            speed = ((x - prev_x) ** 2 + (y - prev_y) ** 2) ** 0.5\n",
    "\n",
    "            # Append the speed to the list\n",
    "            speeds.append(speed)\n",
    "\n",
    "            # Update the previous position\n",
    "            prev_position[i] = (x, y, w, h)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Identify the fastest speed and corresponding person\n",
    "        max_speed = max(speeds)\n",
    "        if max_speed > fastest_speed:\n",
    "            fastest_speed = max_speed\n",
    "            fastest_box = tracked_boxes[speeds.index(max_speed)]\n",
    "\n",
    "    # Highlight the fastest person\n",
    "    if fastest_box is not None:\n",
    "\n",
    "        # Draw bounding box for the fastest person\n",
    "        x, y, w, h = [int(v) for v in fastest_box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw bounding boxes for all tracked people\n",
    "        for box in tracked_boxes:\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Detect people in the current frame and draw bounding boxes\n",
    "    detected_boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    for (x, y, w, h) in detected_boxes:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the current frame number on the video\n",
    "    text = f'Current_Frame: {current_frame}'\n",
    "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "    text_x = frame.shape[1] - text_size[0] - 10\n",
    "    text_y = 30\n",
    "    cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('find_the_fastest_character', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Demo Video of game_3: \"find_the_fastest_character\"](https://drive.google.com/file/d/1VhatQnO62YvWtV7ovntemcju5bkoZnKt/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_4 : \"find_two_odds\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Capture frames from the video between specified start and end frames.\n",
    "   - Convert frames to grayscale and resize them for faster processing.\n",
    "\n",
    "2. **Optical Flow Calculation:**\n",
    "   - Use the Farneback method to calculate the optical flow between consecutive frames.\n",
    "   - Compute the flow vector and its magnitude for each pixel.\n",
    "\n",
    "3. **Flow Direction Analysis:**\n",
    "   - Identify points with significant flow magnitude.\n",
    "   - Calculate the direction of the flow vector at these points.\n",
    "   - Store points with significant motion along with their flow direction.\n",
    "\n",
    "4. **Odd Character Detection:**\n",
    "   - Compute the median direction of all significant flow directions.\n",
    "   - Calculate the deviation of each point's direction from the median direction.\n",
    "   - Identify the two points with the largest deviations and highlight them.\n",
    "\n",
    "5. **Visualization:**\n",
    "   - Draw arrows on the frame to indicate the direction of motion.\n",
    "   - Highlight the two most deviated points with red rectangles.\n",
    "   - Display the frame with annotations and the optical flow magnitude.\n",
    "\n",
    "### Mathematical Explanation of Optical Flow Calculation:\n",
    "\n",
    "#### Optical Flow Vector Calculation:\n",
    "- The optical flow vector at pixel $(x, y)$ is:\n",
    "  $$\n",
    "  \\text{opt\\_flow}(x, y) = \\begin{bmatrix}\n",
    "  u(x, y) \\\\\n",
    "  v(x, y)\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  where $u(x, y)$ and $v(x, y)$ are the horizontal and vertical components of the flow.\n",
    "\n",
    "#### Magnitude of Optical Flow:\n",
    "- The magnitude of the flow vector is:\n",
    "  $$\n",
    "  \\|\\text{opt\\_flow}(x, y)\\| = \\sqrt{u(x, y)^2 + v(x, y)^2}\n",
    "  $$\n",
    "\n",
    "### Mathematical Explanation of Flow Direction Analysis:\n",
    "\n",
    "#### Flow Direction Calculation:\n",
    "- The direction of the flow vector at pixel $(x, y)$ is:\n",
    "  $$\n",
    "  \\theta(x, y) = \\arctan2(v(x, y), u(x, y))\n",
    "  $$\n",
    "\n",
    "#### Identifying Significant Motion:\n",
    "- Points with flow magnitude greater than a threshold (e.g., 0.4) are considered to have significant motion.\n",
    "\n",
    "#### Median Direction and Deviation:\n",
    "- The median direction of significant flow directions is:\n",
    "  $$\n",
    "  \\theta_{\\text{median}} = \\text{median}(\\theta_1, \\theta_2, \\ldots, \\theta_n)\n",
    "  $$\n",
    "- The deviation of each point's direction from the median direction is:\n",
    "  $$\n",
    "  \\text{deviation}_i = |\\theta_i - \\theta_{\\text{median}}|\n",
    "  $$\n",
    "\n",
    "### Odd Character Detection:\n",
    "- Identify the two points with the largest deviations:\n",
    "  $$\n",
    "  \\text{odd\\_indices} = \\text{argsort}(\\text{deviations})[-2:]\n",
    "  $$\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with arrows indicating the direction of motion, and the two most deviated points highlighted with red rectangles. The optical flow magnitude is also displayed as a separate image for reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "4A. Input images from video file WiiPlay.mp4 with level 6 (frame number between 1650 and 1800).<br>\\\n",
    "4B. (10pts) Compute and show <b>optical flows</b> on each frame using <b>blue</b> arrows.<br>\\\n",
    "4C. (5pts) Try to detect two odd character who face the opposite direction from everyone else, draw a <b>red</b> rectangle around each of the two character, and show the output images in the <b>\"find_two_odds\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_4 : \"find_two_odds\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def display_flow(img, flow, stride=10):\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # List to store points with significant optical flow\n",
    "    odd_characters = []\n",
    "\n",
    "    # Iterate through the image with a step size of `stride` pixels\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "\n",
    "            # Get the flow vector at the current point\n",
    "            flow_at_point = flow[y, x]\n",
    "\n",
    "            # Check if the magnitude of flow is significant\n",
    "            if np.linalg.norm(flow_at_point) > 0.4:\n",
    "\n",
    "                # Calculate the direction of flow\n",
    "                direction = np.arctan2(flow_at_point[1], flow_at_point[0])\n",
    "\n",
    "                # Store the point and its direction\n",
    "                odd_characters.append((x, y, direction))\n",
    "\n",
    "                # Start point of the arrow\n",
    "                pt1 = (x, y)\n",
    "\n",
    "                # Calculate the end point of the arrow\n",
    "                delta = flow_at_point.astype(np.int32)[::-1]\n",
    "\n",
    "                # Scale the arrow for better visibility and draw it on the image\n",
    "                pt2 = (pt1[0] + delta[0] * 2, pt1[1] + delta[1] * 2)\n",
    "                cv2.arrowedLine(img, pt1, pt2, (255, 0, 0), 1, cv2.LINE_AA, 0, 0.1)\n",
    "\n",
    "    # If there are more than 2 points with significant optical flow\n",
    "    if len(odd_characters) > 2:\n",
    "\n",
    "        # Extract the directions of the points\n",
    "        directions = np.array([c[2] for c in odd_characters])\n",
    "\n",
    "        # Calculate the median direction\n",
    "        median_direction = np.median(directions)\n",
    "\n",
    "        # Calculate the deviation from the median direction\n",
    "        deviations = np.abs(directions - median_direction)\n",
    "\n",
    "        # Get the indices of the two most deviated points\n",
    "        odd_indices = deviations.argsort()[-2:]\n",
    "\n",
    "        # Draw a red rectangle around the two most deviated points\n",
    "        for idx in odd_indices:\n",
    "            x, y, _ = odd_characters[idx]\n",
    "            cv2.rectangle(img, (x-30, y-30), (x+30, y+30), (0, 0, 255), 3)\n",
    "\n",
    "    # Calculate the magnitude of the optical flow\n",
    "    norm_opt_flow = np.linalg.norm(flow, axis=2)\n",
    "\n",
    "    # Normalize for display\n",
    "    norm_opt_flow = cv2.normalize(norm_opt_flow, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Display the image with arrows and boxes\n",
    "    cv2.imshow('find_two_odds', img)\n",
    "\n",
    "    # Display the optical flow magnitude\n",
    "    cv2.imshow('optical flow magnitude', norm_opt_flow)\n",
    "\n",
    "    # Check if the user pressed 'q' to quit\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "\n",
    "# Set the frame range to analyze\n",
    "start_frame = 1650\n",
    "end_frame = 1800\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Read the first frame\n",
    "_, prev_frame = cap.read()\n",
    "\n",
    "# Convert to grayscale\n",
    "prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Resize the frame for faster processing\n",
    "prev_frame = cv2.resize(prev_frame, (0,0), None, 0.5, 0.5)\n",
    "\n",
    "# Flag to indicate the first frame\n",
    "first_frame = True\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "# Initialize FPS counter\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    status_cap, frame = cap.read()\n",
    "    if not status_cap or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (0,0), None, 0.5, 0.5)\n",
    "\n",
    "    # Start the timer for FPS calculation\n",
    "    timer = cv2.getTickCount()\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if first_frame:\n",
    "\n",
    "        # Calculate the initial optical flow using Farneback method\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, \n",
    "                                                pyr_scale=0.4, levels=6, winsize=20, \n",
    "                                                iterations=15, poly_n=7, poly_sigma=1.5, \n",
    "                                                flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "        # Unset the first frame flag\n",
    "        first_frame = False\n",
    "    else:\n",
    "        # Calculate the optical flow using the previous flow as initial estimate\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, opt_flow, \n",
    "                                                pyr_scale=0.4, levels=6, winsize=20, \n",
    "                                                iterations=15, poly_n=7, poly_sigma=1.5, \n",
    "                                                flags=cv2.OPTFLOW_USE_INITIAL_FLOW)\n",
    "    # Calculate FPS\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Update the previous frame\n",
    "    prev_frame = np.copy(gray)\n",
    "\n",
    "    # Display the flow and check if 'q' is pressed  \n",
    "    if display_flow(frame, opt_flow):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "    # Check if 'q' is pressed to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Demo of picture game_4](game_4-1.png \"Demo of picture game_4 \")\n",
    "![Demo of picture game_4](game_4-2.png \"Demo of picture game_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "5A. Input continuous images from 'car.mp4'.<br>\\\n",
    "5B. (6pts) For each frame, detect every car using YOLOv8 trained data 'yolov8n.pt'. (mark with red rectangles)<br>\\\n",
    "5C. (6pts) For each car, detect a licence plate using 'license_plate_detector.pt'. (mark with blue rectangle)<br>\\\n",
    "5D. (6pts) For each licence plate, OCR using Tesseract. Print the recognized licence plate number above each detected licence plate. (putText() in green color)<br>\\\n",
    "5E. (12pts) Use whatever you learned this semester to improve the result. Write a simple report on your method and observations.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_5 : \"Licence Plate Recognition\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Capture frames from the video between specified start and end frames.\n",
    "   - Convert frames to grayscale and apply preprocessing techniques to enhance image quality.\n",
    "\n",
    "2. **Vehicle Detection:**\n",
    "   - Use a YOLO model to detect vehicles in each frame.\n",
    "   - Store the bounding boxes and confidence scores for detected vehicles.\n",
    "\n",
    "3. **License Plate Detection:**\n",
    "   - Use another YOLO model to detect license plates within the detected vehicles.\n",
    "   - Extract the regions corresponding to the license plates.\n",
    "\n",
    "4. **Optical Character Recognition (OCR):**\n",
    "   - Preprocess the extracted license plate regions to improve OCR performance.\n",
    "   - Use Tesseract OCR to extract text from the license plate regions.\n",
    "\n",
    "5. **Optical Character Recognition (OCR):**\n",
    "   - Compute the confidence scores of the detected license plates.\n",
    "   - Identify the two license plates with the lowest confidence scores and highlight them.\n",
    "\n",
    "6. **Optical Character Recognition (OCR):**\n",
    "   - Draw bounding boxes around detected vehicles and license plates.\n",
    "   - Highlight the two license plates with the lowest confidence scores with red rectangles.\n",
    "   - Display the frame with annotations and the OCR results.\n",
    "\n",
    "## Mathematical Explanation of Image Preprocessing:\n",
    "\n",
    "#### Grayscale Conversion\n",
    "The color image $ I_{\\text{rgb}}$ is converted to a grayscale image $ I_{\\text{gray}}$ using the following formula:\n",
    "\n",
    "$$ I_{\\text{gray}}(x, y) = 0.299 \\cdot I_{\\text{rgb}}(x, y, R) + 0.587 \\cdot I_{\\text{rgb}}(x, y, G) + 0.114 \\cdot I_{\\text{rgb}}(x, y, B) $$\n",
    "\n",
    "#### Histogram Equalization\n",
    "Enhances the contrast of the grayscale image $ I_{\\text{gray}}$:\n",
    "\n",
    "$$ I_{\\text{equalized}}(x, y) = \\frac{I_{\\text{gray}}(x, y) - \\min(I_{\\text{gray}})}{\\max(I_{\\text{gray}}) - \\min(I_{\\text{gray}})} \\times 255 $$\n",
    "\n",
    "#### Bilateral Filtering\n",
    "Applies a bilateral filter to smooth the image $ I_{\\text{equalized}} $ while preserving edges:\n",
    "\n",
    "$$ I_{\\text{filtered}}(x, y) = \\frac{1}{W} \\sum_{(i,j) \\in S} I_{\\text{equalized}}(i, j) \\cdot \\exp\\left( -\\frac{(i-x)^2 + (j-y)^2}{2\\sigma_s^2} - \\frac{|I_{\\text{equalized}}(i, j) - I_{\\text{equalized}}(x, y)|^2}{2\\sigma_r^2} \\right) $$\n",
    "\n",
    "#### Adaptive Thresholding (Otsu's Method)\n",
    "Converts the filtered image $ I_{\\text{filtered}} $ into a binary image $ I_{\\text{binary}} $:\n",
    "\n",
    "$$ I_{\\text{binary}}(x, y) = \n",
    "\\begin{cases} \n",
    "255 & \\text{if } I_{\\text{filtered}}(x, y) \\geq T_{\\text{Otsu}} \\\\\n",
    "0 & \\text{if } I_{\\text{filtered}}(x, y) < T_{\\text{Otsu}}\n",
    "\\end{cases} $$\n",
    "\n",
    "#### Morphological Operations\n",
    "**Dilation**: Expands white regions in $ I_{\\text{binary}} $ by a structuring element $ K $:\n",
    "\n",
    "$$ I_{\\text{dilated}} = I_{\\text{binary}} \\oplus K $$\n",
    "\n",
    "**Erosion**: Shrinks white regions in $ I_{\\text{dilated}} $ by the same structuring element $K$:\n",
    "\n",
    "$$ I_{\\text{eroded}} = I_{\\text{dilated}} \\ominus K $$\n",
    "\n",
    "These operations help in reducing noise and closing gaps in the binary image.\n",
    "\n",
    "## Mathematical Explanation of Object Detection:\n",
    "\n",
    "#### YOLO Model for Cars\n",
    "The YOLO (You Only Look Once) model processes the frame and provides detections in the form of bounding boxes $(x_1, y_1, x_2, y_2)$, confidence scores, and class IDs.\n",
    "\n",
    "For each detected object:\n",
    "$$ \\text{score} = \\text{confidence score}, \\quad \\text{class\\_id} \\in \\text{vehicles} $$\n",
    "\n",
    "#### YOLO Model for License Plates\n",
    "The YOLO model detects license plates similarly, providing bounding boxes and scores.\n",
    "\n",
    "For each detected license plate:\n",
    "$$ \\text{score} = \\text{confidence score} $$\n",
    "\n",
    "Crop the region defined by $(x_1, y_1, x_2, y_2)$ from the frame and preprocess it for OCR.\n",
    "\n",
    "\n",
    "## Mathematical Explanation of Optical Character Recognition (OCR)\n",
    "\n",
    "#### Tesseract OCR\n",
    "Processes the preprocessed license plate image to extract text:\n",
    "\n",
    "$$ \\text{license\\_plate\\_text} = \\text{Tesseract}(I_{\\text{processed}}) $$\n",
    "\n",
    "Append the bounding box coordinates and extracted text to the license plate annotations list.\n",
    "\n",
    "## Annotation Drawing \n",
    "\n",
    "#### Draw Bounding Boxes and Text\n",
    "For each annotation $(x_1, y_1, x_2, y_2, \\text{text})$:\n",
    "\n",
    "$$ \\text{cv2.rectangle}(I_{\\text{frame}}, (x_1, y_1), (x_2, y_2), \\text{color}, 2) $$\n",
    "\n",
    "If text is provided:\n",
    "\n",
    "$$ \\text{cv2.putText}(I_{\\text{frame}}, \\text{text}, (x_1, y_1 - 10), \\text{font}, \\text{scale}, \\text{color}, \\text{thickness}) $$\n",
    "\n",
    "## Video Processing Loop\n",
    "\n",
    "#### Read Frames\n",
    "Read each frame from the video and process every $n$-th frame to reduce computation load:\n",
    "\n",
    "$$ \\text{frame\\_count} \\mod 5 = 0 $$\n",
    "\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with bounding boxes around detected vehicles and license plates. The two license plates with the lowest confidence scores are highlighted with red rectangles. The OCR results are also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infor/miniconda3/envs/CV/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 22 cars, 1 bus, 2 trucks, 104.9ms\n",
      "Speed: 7.3ms preprocess, 104.9ms inference, 878.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 97.8ms\n",
      "Speed: 4.2ms preprocess, 97.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 145.8ms\n",
      "Speed: 10.6ms preprocess, 145.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.0ms\n",
      "Speed: 3.4ms preprocess, 80.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 104.8ms\n",
      "Speed: 5.6ms preprocess, 104.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.0ms\n",
      "Speed: 2.6ms preprocess, 80.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 102.1ms\n",
      "Speed: 9.1ms preprocess, 102.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 79.6ms\n",
      "Speed: 2.6ms preprocess, 79.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 2 buss, 1 truck, 94.6ms\n",
      "Speed: 3.7ms preprocess, 94.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 77.7ms\n",
      "Speed: 3.0ms preprocess, 77.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "#game_5: Licence Plate Recognition\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to draw bounding boxes and text\n",
    "def draw_annotations(frame, annotations, color):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and text on the given frame.\n",
    "\n",
    "    Args:\n",
    "        frame (numpy.ndarray): The image frame on which to draw.\n",
    "        annotations (list): List of annotations, each containing (x1, y1, x2, y2, text).\n",
    "        color (tuple): Color of the bounding boxes (B, G, R).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for (x1, y1, x2, y2, text) in annotations:\n",
    "\n",
    "        # Draw rectangle (bounding box) around detected object\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "        if text:\n",
    "            # Calculate width and height of the text box\n",
    "            (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)\n",
    "\n",
    "            # Draw a filled rectangle as background for text\n",
    "            cv2.rectangle(frame, (int(x1), int(y1) - 30), (int(x1) + w, int(y1)), (0, 0, 0), -1)\n",
    "\n",
    "            # Put the text on top of the background\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "# Function to preprocess image for OCR\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocesses the image to enhance OCR performance.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The preprocessed binary image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Enhance the contrast of the grayscale image\n",
    "    enhanced = cv2.equalizeHist(gray)\n",
    "\n",
    "    # Apply bilateral filter to remove noise and keep edges sharp\n",
    "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "\n",
    "    # Apply adaptive thresholding to get a binary image\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Create a kernel for dilation and erosion\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "    # Apply dilation to fill small holes\n",
    "    binary = cv2.dilate(binary, kernel, iterations=1)\n",
    "\n",
    "    # Apply erosion to remove noise\n",
    "    binary = cv2.erode(binary, kernel, iterations=1)\n",
    "\n",
    "    return binary\n",
    "\n",
    "# Specify the Tesseract executable path\n",
    "# If your os is Windows, the path should be 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# Load models\n",
    "car_detector = YOLO('yolov8n.pt')\n",
    "license_plate_detector = YOLO('license_plate_detector.pt')\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('car.mp4')\n",
    "\n",
    "# Define the list of vehicle class IDs (as per your model's class mapping)\n",
    "vehicles = [2, 3, 5, 7]\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Read frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # Process every nth frame to reduce computation\n",
    "    if frame_count % 5 != 0:\n",
    "        continue\n",
    "\n",
    "    # Detect vehicles\n",
    "    car_detections = car_detector(frame)[0]\n",
    "    car_annotations = []\n",
    "\n",
    "    for detection in car_detections.boxes.data.tolist():\n",
    "\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        if int(class_id) in vehicles:\n",
    "\n",
    "            # Append car detection details to annotations list\n",
    "            car_annotations.append((x1, y1, x2, y2, f'Car: {int(score * 100)}%'))\n",
    "\n",
    "    # Detect license plates in the frame\n",
    "    license_plate_detections = license_plate_detector(frame)[0]\n",
    "    license_plate_annotations = []\n",
    "\n",
    "    for license_plate in license_plate_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = license_plate\n",
    "\n",
    "        # Crop the detected license plate region\n",
    "        license_plate_crop = frame[int(y1):int(y2), int(x1): int(x2), :]\n",
    "\n",
    "        # Preprocess the cropped license plate image for OCR\n",
    "        processed_image = preprocess_image(license_plate_crop)\n",
    "\n",
    "        # Perform OCR on the processed license plate image\n",
    "        license_plate_text = pytesseract.image_to_string(processed_image, config='--psm 13').strip()\n",
    "\n",
    "        # Append license plate detection details to annotations list\n",
    "        license_plate_annotations.append((x1, y1, x2, y2, license_plate_text))\n",
    "    \n",
    "    # Red rectangles for cars\n",
    "    draw_annotations(frame, car_annotations, (0, 0, 255))\n",
    "\n",
    "    # Blue rectangles for license plates\n",
    "    draw_annotations(frame, license_plate_annotations, (255, 0, 0))  \n",
    "\n",
    "    # Resize the frame for display\n",
    "    frame_resized = cv2.resize(frame, (800, 450))\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Video', frame_resized)\n",
    "\n",
    "    # Display the preprocessed image for OCR\n",
    "    cv2.imshow('License Plate Detection', processed_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (5pts) Any comments regarding the final exam? Which steps you believe you have completed? Which steps bother you?<br> \n",
    "7. (5pts) Any suggestion to teaching assistants to improve this class? Any suggestion to teacher to improve this class?<br>\n",
    "\n",
    "\n",
    "### My Answer\n",
    "\n",
    "6. In the final exam, I completed all the first questions, and I was left with the most difficult functions that had not yet been optimized. \n",
    "\n",
    "   There are five questions in total, and I think the most difficult part of the second to fifth questions is to deal with the frame of the video, because in addition to the basic computer vision basics, such as filtering, noise reduction, edge detection, and morphology, we also need to overcome the problems of the film itself to improve the saturation and other related technologies, so that we can present the best results\n",
    "\n",
    "7. After a semester, I think that the TA system of advanced computer vision is working very well, and basically every TA has     helped me to grasp the basic key knowledge in this class\n",
    "\n",
    "   As for the suggestion for this class, I think for the final exam, we can take the students to team up to participate in the CVPR Data CV Challenge, which is organized by a very famous computer vision workshop to test whether you can produce a good computer vision work in a limited time\n",
    "\n",
    "   \n",
    "   - [CVPR](https://sites.google.com/view/vdu-cvpr24/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [OpenCV Tutorial](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "\n",
    "- [Cv2.threshold Parameters document](https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#threshold)\n",
    "\n",
    "- [Bilateral Filtering theory](https://people.csail.mit.edu/sparis/publi/2009/fntcgv/Paris_09_Bilateral_filtering.pdf)\n",
    "\n",
    "- [OpticalFlowFarneback Parameters](https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga5d10ebbd59fe09c5f650289ec0ece5af)\n",
    "\n",
    "- [OpenCV Tutorial](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "\n",
    "- [Template Matching function reference code and theory](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html)\n",
    "\n",
    "- [OpenCV selectROI function reference code and theory](https://www.geeksforgeeks.org/python-opencv-selectroi-function/)\n",
    "\n",
    "- [detect_faces function reference code and theory](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n",
    "\n",
    "- [Eroding and Dilating reference code and theory](https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html)\n",
    "\n",
    "- [Histogram Equalization reference code and theory](https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html)\n",
    "\n",
    "- [find_similar_faces function theory](https://www.idiap.ch/~marcel/labs/faceverif.php)\n",
    "\n",
    "- [find_similar_faces function reference code](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/)\n",
    "\n",
    "- [preprocess_image function reference code](https://stackoverflow.com/questions/70942221/bilateral-filter-error-in-opencv-for-some-images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
