{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_1 : \"find_this_mii\"\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "1. **Read Frame and Select ROI (Region of Interest)**:\n",
    "   $$\n",
    "   \\text{frame} = \\text{cap.read}(\\text{frame\\_number})\n",
    "   $$\n",
    "   $$\n",
    "   \\text{ROI} = \\text{selectROI}(\\text{frame})\n",
    "   $$\n",
    "   Here, the user selects a rectangular region $ROI$ in the frame. Let's denote this region by its coordinates $(x, y, w, h)$, where $x$ and $y$ are the top-left coordinates, and $w$ and $h$ are the width and height of the region.\n",
    "\n",
    "   The template image $T$ is then:\n",
    "   $$\n",
    "   T = \\text{frame}[y:y+h, x:x+w]\n",
    "   $$\n",
    "\n",
    "\n",
    "2. **Frame Processing Loop**:\n",
    "   For each frame $F_{\\text{i}}$ in the video (where $i$ is the frame index), the loop processes until frame number 5000:\n",
    "   $$\n",
    "   F_i = \\text{cap.read}(i)\n",
    "   $$\n",
    "   If \\(i > 5000\\), exit the loop.\n",
    "\n",
    "3. **Apply Gaussian Blur**:\n",
    "   $$\n",
    "   F_i^{\\text{blur}} = \\text{GaussianBlur}(F_i, (5, 5), 0)\n",
    "   $$\n",
    "   This step applies a Gaussian filter to the frame to reduce noise.\n",
    "\n",
    "4. **Template Matching**:\n",
    "   $$\n",
    "   \\text{res} = \\text{matchTemplate}(F_i^{\\text{blur}}, T, \\text{TM\\_CCOEFF\\_NORMED})\n",
    "   $$\n",
    "   The `matchTemplate` function computes the normalized cross-correlation between the template $T$ and the current frame $F_i^blur$. The result is a matrix `res` where each element represents the correlation coefficient at that point.\n",
    "   \n",
    "   $$\n",
    "   (\\min_{\\text{val}}, \\max_{\\text{val}}, \\min_{\\text{loc}}, \\max_{\\text{loc}}) = \\text{minMaxLoc}(\\text{res})\n",
    "   $$\n",
    "   \n",
    "   This function finds the minimum and maximum values and their locations in the result matrix. We are interested in `loc`, the location of the highest correlation.\n",
    "\n",
    "   Let:\n",
    "   $$\n",
    "   \\text{top\\_left} = \\max_{\\text{loc}}\n",
    "   $$\n",
    "   $$\n",
    "   \\text{bottom\\_right} = (\\text{top\\_left}[0] + w, \\text{top\\_left}[1] + h)\n",
    "   $$\n",
    "\n",
    "In summary, the mathematical operations involve:\n",
    "- Extracting a template image from a specific region in a frame.\n",
    "- Applying Gaussian blur to reduce noise.\n",
    "- Using normalized cross-correlation to find the best match of the template in subsequent frames.\n",
    "- Identifying the location of the best match and drawing a rectangle around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "1A. Input images from video file WiiPlay.mp4 with level 15 (frame number between 4820 and 5000).<br> \\\n",
    "1B. (5pts) Acquire a <b>face template</b> from the first frame (frame number = 4820).<br>\\\n",
    "1C. (10pts) Try to detect the face the same as the template on subsequent frames, draw a <b>red</b> rectangle around the detected face, and show the output images in the <b>\"find_this_mii\"</b> window.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    }
   ],
   "source": [
    "#game_1 : \"find_this_mii\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Set the current video frame to 4820\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 4820)\n",
    "\n",
    "# Capture a single frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Select a region of interest (ROI) manually from the captured frame for template matching\n",
    "r = cv2.selectROI(frame)\n",
    "template = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if frame cannot be read or the current frame exceeds 5000\n",
    "    if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) > 5000:\n",
    "        break\n",
    "\n",
    "    # Apply Gaussian Blur to the frame to reduce noise for better template matching\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Perform template matching to find the template in the frame\n",
    "    res = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # Find the location of the template in the frame\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    top_left = max_loc\n",
    "    h, w, _ = template.shape\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    # Draw a rectangle around the template in the frame\n",
    "    cv2.rectangle(frame, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame with the rectangle around the template\n",
    "    cv2.imshow('find_this_mii', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_2 : \"find_two_look_alike\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Convert the frame to the HSV color space to detect skin colors.\n",
    "   - Apply a binary mask to isolate skin-colored areas.\n",
    "\n",
    "2. **Morphological Operations:**\n",
    "   - Erode and dilate the mask to remove noise.\n",
    "   - Apply Gaussian blur to smooth the mask.\n",
    "\n",
    "3. **Contour Detection:**\n",
    "   - Find contours in the mask.\n",
    "   - Filter contours based on area and aspect ratio to identify potential faces.\n",
    "\n",
    "4. **Face Similarity Detection:**\n",
    "   - Compare each detected face with every other detected face using template matching.\n",
    "   - Mark pairs of faces that are similar.\n",
    "\n",
    "\n",
    "## Mathematical Explanation of Face Detection:\n",
    "\n",
    "#### Convert to HSV:\n",
    "- Frame $F$ is converted from BGR to HSV.\n",
    "  $$\n",
    "  F_{HSV} = \\text{HSV}(F)\n",
    "  $$\n",
    "\n",
    "#### Binary Mask:\n",
    "- Create a binary mask $M$ using threshold values for skin color.\n",
    "  $$\n",
    "  M = \\begin{cases}\n",
    "  1, & \\text{if } \\text{lower\\_skin} \\leq F_{HSV} \\leq \\text{upper\\_skin} \\\\\n",
    "  0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "#### Morphological Operations:\n",
    "- Erode $M$ with a kernel $K$:\n",
    "  $$\n",
    "  M' = \\text{erode}(M, K)\n",
    "  $$\n",
    "- Dilate $M'$ with $K$:\n",
    "  $$\n",
    "  M'' = \\text{dilate}(M', K)\n",
    "  $$\n",
    "- Apply Gaussian blur:\n",
    "  $$\n",
    "  M_{smooth} = \\text{GaussianBlur}(M'', (3, 3), 0)\n",
    "  $$\n",
    "\n",
    "#### Contour Detection:\n",
    "- Find contours $C$ in $M_{smooth} $:\n",
    "  $$\n",
    "  C = \\text{contours}(M_{smooth})\n",
    "  $$\n",
    "- For each contour   $c \\in C$:\n",
    "  - Compute bounding rectangle \\( x, y, w, h \\).\n",
    "  - Compute area $A$ and aspect ratio $R$:\n",
    "    $$\n",
    "    A = w \\times h\n",
    "    $$\n",
    "    $$\n",
    "    R = \\frac{w}{h}\n",
    "    $$\n",
    "  - Filter based on area and aspect ratio:\n",
    "    $$\n",
    "    \\text{if } \\text{min\\_area} < A < \\text{max\\_area} \\text{ and } \\text{aspect\\_ratio\\_range}[0] < R < \\text{aspect\\_ratio\\_range}[1] \\text{, append to faces}\n",
    "    $$\n",
    "\n",
    "## Mathematical Explanation of Face Similarity Matching:\n",
    "\n",
    "- For each pair of faces $f_i, f_j$:\n",
    "  - Extract face regions $f_i$ and $f_j$ from frame.\n",
    "  - Resize $f_j$ to match dimensions of $f_i$:\n",
    "    $$\n",
    "    F_{j,resized} = \\text{resize}(F_j, (w_i, h_i))\n",
    "    $$\n",
    "  - Convert to grayscale:\n",
    "    $$\n",
    "    F_{i,gray} = \\text{gray}(F_i)\n",
    "    $$\n",
    "    $$\n",
    "    F_{j,gray} = \\text{gray}(F_{j,resized})\n",
    "    $$\n",
    "  - Perform template matching:\n",
    "    $$\n",
    "    \\text{result} = \\text{matchTemplate}(F_{i,gray}, F_{j,gray}, \\text{TM\\_CCOEFF\\_NORMED})\n",
    "    $$\n",
    "  - Find maximum correlation value $max_val$:\n",
    "    $$\n",
    "    \\text{max\\_val} = \\max(\\text{result})\n",
    "    $$\n",
    "  - If $max_val$ > 0.4, consider faces similar and highlight them.\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with rectangles around detected faces and similar-looking faces highlighted with a different color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "2A. Input images from video file WiiPlay.mp4 with level 8 (frame number between 2180 and 2380).<br>\\\n",
    "2B. (5pts) Detect <b>pedestrians</b> on each frame and draw a <b>green</b> rectangle around your detection.<br>\\\n",
    "2C. (5pts) Detect <b>faces</b> on each frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "2D. (10pts) Try to find two faces look like each other, draw a <b>red</b> rectangle around each of the two faces, and show the output images in the <b>\"find_two_look_alike\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_2 : \"find_two_look_alike\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range):\n",
    "    \"\"\"\n",
    "    Detects faces in the given frame based on skin color and filters them by area and aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        frame (numpy.ndarray): The image frame in which to detect faces.\n",
    "        lower_skin (numpy.ndarray): The lower bound of the HSV color range for skin detection.\n",
    "        upper_skin (numpy.ndarray): The upper bound of the HSV color range for skin detection.\n",
    "        min_area (int): The minimum area of a detected face.\n",
    "        max_area (int): The maximum area of a detected face.\n",
    "        aspect_ratio_range (tuple): The range of acceptable aspect ratios for detected faces.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of detected faces and the processed frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the frame to the HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a binary mask of the skin\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Create a structuring element for morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "\n",
    "    # Erode and dilate the mask to remove noise\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the mask\n",
    "    mask = cv2.GaussianBlur(mask, (3, 3), 0)\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    faces = []\n",
    "    for contour in contours:\n",
    "\n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = w / float(h)\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Filter contours based on aspect ratio and area\n",
    "        if aspect_ratio_range[0] < aspect_ratio < aspect_ratio_range[1] and min_area < area < max_area:\n",
    "            faces.append((x, y, w, h))\n",
    "\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x-10, y-10), (x+w+10, y+h+10), (255, 0, 0), 2)\n",
    "    \n",
    "    return faces, frame\n",
    "\n",
    "def find_similar_faces(faces, frame):\n",
    "    \"\"\"\n",
    "    Find similar-looking faces in the given frame using template matching.\n",
    "\n",
    "    Args:\n",
    "        faces (list): A list of detected faces as (x, y, w, h) tuples.\n",
    "        frame (numpy.ndarray): The image frame in which to find similar faces.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of pairs of similar faces as ((x1, y1, w1, h1), (x2, y2, w2, h2)) tuples.\n",
    "    \"\"\"\n",
    "    similar_faces = []\n",
    "\n",
    "    for i in range(len(faces)):\n",
    "        (x1, y1, w1, h1) = faces[i]\n",
    "        face1 = frame[y1:y1+h1, x1:x1+w1]\n",
    "\n",
    "        for j in range(i + 1, len(faces)):\n",
    "            (x2, y2, w2, h2) = faces[j]\n",
    "            face2 = frame[y2:y2+h2, x2:x2+w2]\n",
    "\n",
    "            # Resize face2 to match the size of face1\n",
    "            face2_resized = cv2.resize(face2, (w1, h1))\n",
    "\n",
    "            # Convert faces to grayscale\n",
    "            face1_gray = cv2.cvtColor(face1, cv2.COLOR_BGR2GRAY)\n",
    "            face2_gray = cv2.cvtColor(face2_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Perform template matching\n",
    "            result = cv2.matchTemplate(face1_gray, face2_gray, cv2.TM_CCOEFF_NORMED)\n",
    "            _, max_val, _, _ = cv2.minMaxLoc(result)\n",
    "\n",
    "            # Check if the faces are similar based on a threshold\n",
    "            if max_val > 0.4:  # Adjust the threshold as needed\n",
    "                similar_faces.append(((x1, y1, w1, h1), (x2, y2, w2, h2)))\n",
    "\n",
    "    return similar_faces\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Initialize HOG descriptor for people detection\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# Set the range of frames to process\n",
    "start_frame = 2180\n",
    "end_frame = 2380\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "# Define HSV color range for skin detection\n",
    "lower_skin = np.array([0, 48, 80], dtype='uint8')\n",
    "upper_skin = np.array([20, 255, 255], dtype='uint8')\n",
    "min_area = 300\n",
    "max_area = 1000\n",
    "aspect_ratio_range = (0.5, 2)\n",
    "\n",
    "# Initialize the faces list\n",
    "faces = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the frame is not read correctly or the end frame is reached\n",
    "    if not ret or current_frame > end_frame:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect people in the frame using HOG\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces, frame = detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range)\n",
    "\n",
    "    # Find similar faces in the frame\n",
    "    similar_faces = find_similar_faces(faces, frame)\n",
    "\n",
    "    for(face1, face2)in similar_faces:\n",
    "        (x1, y1, w1, h1) = face1\n",
    "        (x2, y2, w2, h2) = face2\n",
    "        cv2.rectangle(frame, (x1-10, y1-10), (x1+w1+10, y1+h1+10), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (x2-10, y2-10), (x2+w2+10, y2+h2+10), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame \n",
    "    cv2.imshow('find_two_look_alike', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_3 : \"find_the_fastest_character\"\n",
    "\n",
    "### Steps:\n",
    "1. **Preprocessing:**\n",
    "   - Load the video and set the frame range to analyze.\n",
    "   - Initialize the HOG descriptor for detecting people.\n",
    "\n",
    "2. **Detection and Tracking:**\n",
    "   - Detect people in the first frame and initialize bounding boxes.\n",
    "   - Create and initialize a multi-object tracker.\n",
    "\n",
    "3. **Frame-by-Frame Analysis:**\n",
    "   - Read each frame and update the tracker.\n",
    "   - Calculate the speed of each tracked person using the Euclidean distance.\n",
    "   - Identify and highlight the fastest person in the current frame.\n",
    "\n",
    "4. **Visualization:**\n",
    "   - Draw bounding boxes around all detected and tracked people.\n",
    "   - Display the current frame number on the video.\n",
    "\n",
    "### Mathematical Explanation of Speed Calculation:\n",
    "\n",
    "#### Euclidean Distance:\n",
    "- For each tracked person, calculate the speed between the current frame $(x, y)$ and the previous frame $(\\text{prev\\_x}, \\text{prev\\_y})$:\n",
    "  $$\n",
    "  \\text{speed} = \\sqrt{(x - \\text{prev\\_x})^2 + (y - \\text{prev\\_y})^2}\n",
    "  $$\n",
    "\n",
    "#### Steps:\n",
    "- Initialize positions:\n",
    "  - Let $ (x, y)$ be the current position.\n",
    "  - Let $ (\\text{prev\\_x}, \\text{prev\\_y})$ be the previous position.\n",
    "\n",
    "- Calculate speed for each tracked person:\n",
    "  $$\n",
    "  \\text{speed} = \\sqrt{(x - \\text{prev\\_x})^2 + (y - \\text{prev\\_y})^2}\n",
    "  $$\n",
    "\n",
    "- Identify the fastest speed:\n",
    "  - Track the maximum speed recorded so far.\n",
    "  - Update the fastest speed if the current speed is greater.\n",
    "\n",
    "### Final Output:\n",
    "The video frame is displayed with:\n",
    "- Bounding boxes around all detected and tracked people.\n",
    "- The fastest person highlighted with a different color.\n",
    "- The current frame number displayed on the video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "3A. Input images from video file WiiPlay.mp4 with level 9 (frame number between 2480 and 2600).<br>\\\n",
    "3B. (5pts) <b>Detect </b>faces(or pedestrians) on the first frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "3C. (10pts) <b>Track </b>faces(or pedestrians) on subsequent frames and draw a <b>green</b> rectangle around your tracking.<br>\\\n",
    "3D. (5pts) Try to find out the fastest character, draw a <b>red</b> rectangle around the fastest character, and show the output images in the <b>\"find_the_fastest_character\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_3 : \"find_the_fastest_character\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Define the frame range to analyze\n",
    "start_frame = 2480\n",
    "end_frame = 2600\n",
    "\n",
    "# Set the video to start at the specified frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video\")\n",
    "    exit()\n",
    "\n",
    "# Detect people in the first frame\n",
    "boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "\n",
    "# Draw bounding boxes around detected people\n",
    "for (x, y, w, h) in boxes:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Initialize the multi-object tracker\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "for box in boxes:\n",
    "    tracker = cv2.legacy.TrackerMIL_create()\n",
    "    trackers.add(tracker, frame, tuple(box))\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "fastest_speed = 0\n",
    "fastest_box = None\n",
    "\n",
    "# Store previous positions of detected people\n",
    "prev_position = [tuple(box) for box in boxes]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    # Update tracker and get updated positions\n",
    "    success, tracked_boxes = trackers.update(frame)\n",
    "\n",
    "    if success:\n",
    "        speeds = []\n",
    "\n",
    "        # Calculate speed for each tracked person\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "\n",
    "            # Calculate speed based on the distance moved\n",
    "            prev_x, prev_y, _, _ = prev_position[i]\n",
    "\n",
    "            # Calculate speed as the Euclidean distance between the current and previous positions\n",
    "            speed = ((x - prev_x) ** 2 + (y - prev_y) ** 2) ** 0.5\n",
    "\n",
    "            # Append the speed to the list\n",
    "            speeds.append(speed)\n",
    "\n",
    "            # Update the previous position\n",
    "            prev_position[i] = (x, y, w, h)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Identify the fastest speed and corresponding person\n",
    "        max_speed = max(speeds)\n",
    "        if max_speed > fastest_speed:\n",
    "            fastest_speed = max_speed\n",
    "            fastest_box = tracked_boxes[speeds.index(max_speed)]\n",
    "\n",
    "    # Highlight the fastest person\n",
    "    if fastest_box is not None:\n",
    "\n",
    "        # Draw bounding box for the fastest person\n",
    "        x, y, w, h = [int(v) for v in fastest_box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw bounding boxes for all tracked people\n",
    "        for box in tracked_boxes:\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Detect people in the current frame and draw bounding boxes\n",
    "    detected_boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    for (x, y, w, h) in detected_boxes:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the current frame number on the video\n",
    "    text = f'Current_Frame: {current_frame}'\n",
    "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "    text_x = frame.shape[1] - text_size[0] - 10\n",
    "    text_y = 30\n",
    "    cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('find_the_fastest_character', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "4A. Input images from video file WiiPlay.mp4 with level 6 (frame number between 1650 and 1800).<br>\\\n",
    "4B. (10pts) Compute and show <b>optical flows</b> on each frame using <b>blue</b> arrows.<br>\\\n",
    "4C. (5pts) Try to detect two odd character who face the opposite direction from everyone else, draw a <b>red</b> rectangle around each of the two character, and show the output images in the <b>\"find_two_odds\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_4 : \"find_two_odds\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def display_flow(img, flow, stride=10):\n",
    "    height, width = img.shape[:2]\n",
    "    odd_characters = []\n",
    "\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "            flow_at_point = flow[y, x]\n",
    "            if np.linalg.norm(flow_at_point) > 0.5:\n",
    "                direction = np.arctan2(flow_at_point[1], flow_at_point[0])\n",
    "                odd_characters.append((x, y, direction))\n",
    "                pt1 = (x, y)\n",
    "                delta = flow_at_point.astype(np.int32)[::-1]\n",
    "                pt2 = (pt1[0] + delta[0] * 2, pt1[1] + delta[1] * 2)\n",
    "                cv2.arrowedLine(img, pt1, pt2, (255, 0, 0), 1, cv2.LINE_AA, 0, 0.1)\n",
    "\n",
    "    if len(odd_characters) > 2:\n",
    "        directions = np.array([c[2] for c in odd_characters])\n",
    "        median_direction = np.median(directions)\n",
    "        deviations = np.abs(directions - median_direction)\n",
    "        odd_indices = deviations.argsort()[-2:]\n",
    "\n",
    "        for idx in odd_indices:\n",
    "            x, y, _ = odd_characters[idx]\n",
    "            cv2.rectangle(img, (x-15, y-15), (x+15, y+15), (0, 0, 255), 2)\n",
    "\n",
    "    norm_opt_flow = np.linalg.norm(flow, axis=2)\n",
    "    norm_opt_flow = cv2.normalize(norm_opt_flow, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.imshow('find_two_odds', img)\n",
    "    cv2.imshow('optical flow magnitude', norm_opt_flow)\n",
    "\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "start_frame = 1650\n",
    "end_frame = 1800\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "_ , prev_frame = cap.read()\n",
    "prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_frame = cv2.resize(prev_frame, (0,0), None, 0.5, 0.5)\n",
    "first_frame = True\n",
    "\n",
    "current_frame = start_frame\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    status_cap, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (0,0), None, 0.5, 0.5)\n",
    "\n",
    "    if not status_cap or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    timer = cv2.getTickCount()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if first_frame:\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, \n",
    "                                                pyr_scale=0.5, levels=5, winsize=15, \n",
    "                                                iterations=10, poly_n=5, poly_sigma=1.2, \n",
    "                                                flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "        first_frame = False\n",
    "    else:\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, opt_flow, \n",
    "                                                pyr_scale=0.5, levels=5, winsize=15, \n",
    "                                                iterations=10, poly_n=5, poly_sigma=1.2, \n",
    "                                                flags=cv2.OPTFLOW_USE_INITIAL_FLOW)\n",
    "\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    prev_frame = np.copy(gray)\n",
    "    \n",
    "    if display_flow(frame, opt_flow):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "5A. Input continuous images from 'car.mp4'.<br>\\\n",
    "5B. (6pts) For each frame, detect every car using YOLOv8 trained data 'yolov8n.pt'. (mark with red rectangles)<br>\\\n",
    "5C. (6pts) For each car, detect a licence plate using 'license_plate_detector.pt'. (mark with blue rectangle)<br>\\\n",
    "5D. (6pts) For each licence plate, OCR using Tesseract. Print the recognized licence plate number above each detected licence plate. (putText() in green color)<br>\\\n",
    "5E. (12pts) Use whatever you learned this semester to improve the result. Write a simple report on your method and observations.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 94.6ms\n",
      "Speed: 5.9ms preprocess, 94.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 90.9ms\n",
      "Speed: 4.3ms preprocess, 90.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pnesivsu\n",
      "License Plate: “RNAI NRU\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 106.1ms\n",
      "Speed: 4.9ms preprocess, 106.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 108.2ms\n",
      "Speed: 3.1ms preprocess, 108.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pnesivsu\n",
      "License Plate: “RNAI NRU\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 97.5ms\n",
      "Speed: 3.6ms preprocess, 97.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.6ms\n",
      "Speed: 3.5ms preprocess, 81.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pes vsu)\n",
      "License Plate: JENA NR\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 92.2ms\n",
      "Speed: 3.7ms preprocess, 92.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 87.3ms\n",
      "Speed: 3.6ms preprocess, 87.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: EWALD NRU_\n",
      "License Plate: CG\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "from ultralytics import YOLO\n",
    "import pytesseract\n",
    "\n",
    "def draw_annotations(frame, annotations, color, font_scale=0.9):\n",
    "    for (x1, y1, x2, y2, text) in annotations:\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        if text:\n",
    "            cv2.putText(frame, text, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), 2)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to remove noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to get a binary image\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Optionally apply morphology operations to clean up the image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    return binary\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# Load models\n",
    "car_detector = YOLO('yolov8n.pt')\n",
    "license_plate_detector = YOLO('license_plate_detector.pt')\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('car.mp4')\n",
    "\n",
    "# Define the list of vehicle class IDs (as per your model's class mapping)\n",
    "vehicles = [2, 3, 5, 7]\n",
    "\n",
    "# Read frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles\n",
    "    car_detections = car_detector(frame)[0]\n",
    "    car_annotations = []\n",
    "    for detection in car_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        if int(class_id) in vehicles:\n",
    "            car_annotations.append((x1, y1, x2, y2, f'Car: {int(score * 100)}%'))\n",
    "\n",
    "    # Detect license plates\n",
    "    license_plate_detections = license_plate_detector(frame)[0]\n",
    "    license_plate_annotations = []\n",
    "    for license_plate in license_plate_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = license_plate\n",
    "        license_plate_crop = frame[int(y1):int(y2), int(x1): int(x2), :]\n",
    "        processed_image = preprocess_image(license_plate_crop)\n",
    "        # Adjust Tesseract OCR configuration for better accuracy\n",
    "        custom_config = r'--oem 3 --psm 8'\n",
    "        license_plate_text = pytesseract.image_to_string(processed_image, config=custom_config).strip()\n",
    "        license_plate_annotations.append((x1, y1, x2, y2, license_plate_text))\n",
    "\n",
    "        text = pytesseract.image_to_string(processed_image, config=custom_config).strip()\n",
    "        print(f'License Plate: {text}')\n",
    "\n",
    "    draw_annotations(frame, car_annotations, (0, 0, 255), font_scale = 0.9)  # Red rectangles for cars\n",
    "    draw_annotations(frame, license_plate_annotations, (255, 0, 0), font_scale = 2)  # Blue rectangles for license plates\n",
    "    \n",
    "    frame_resized = cv2.resize(frame, (800, 450))\n",
    "    \n",
    "    cv2.imshow('License Plate Detection', frame_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (5pts) Any comments regarding the final exam? Which steps you believe you have completed? Which steps bother you?<br> \n",
    "7. (5pts) Any suggestion to teaching assistants to improve this class? Any suggestion to teacher to improve this class?<br>\n",
    "\n",
    "\n",
    "### My Answer\n",
    "\n",
    "6. In the final exam, I completed all the first questions, and I was left with the most difficult functions that had not yet been optimized. \n",
    "\n",
    "   There are five questions in total, and I think the most difficult part of the second to fifth questions is to deal with the frame of the video, because in addition to the basic computer vision basics, such as filtering, noise reduction, edge detection, and morphology, we also need to overcome the problems of the film itself to improve the saturation and other related technologies, so that we can present the best results\n",
    "\n",
    "7. After a semester, I think that the TA system of advanced computer vision is working very well, and basically every TA has     helped me to grasp the basic key knowledge in this class\n",
    "\n",
    "   As for the suggestion for this class, I think for the final exam, we can take the students to team up to participate in the CVPR Data CV Challenge, which is organized by a very famous computer vision workshop to test whether you can produce a good computer vision work in a limited time\n",
    "\n",
    "   \n",
    "   - [CVPR](https://sites.google.com/view/vdu-cvpr24/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [OpenCV Tutorial](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "\n",
    "- [Template Matching function reference code and theory](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html)\n",
    "\n",
    "- [OpenCV selectROI function reference code and theory](https://www.geeksforgeeks.org/python-opencv-selectroi-function/)\n",
    "\n",
    "- [detect_faces function reference code and theory in GitHub](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n",
    "\n",
    "- [compare_faces function reference code and theory](https://stackoverflow.com/questions/23195522/opencv-fastest-method-to-check-if-two-images-are-100-same-or-not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 93.4ms\n",
      "Speed: 3.3ms preprocess, 93.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 84.6ms\n",
      "Speed: 2.5ms preprocess, 84.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 134.5ms\n",
      "Speed: 9.1ms preprocess, 134.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 89.1ms\n",
      "Speed: 3.4ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 100.4ms\n",
      "Speed: 6.5ms preprocess, 100.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 72.7ms\n",
      "Speed: 2.5ms preprocess, 72.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 95.1ms\n",
      "Speed: 4.4ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.8ms\n",
      "Speed: 3.2ms preprocess, 70.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 2 buss, 1 truck, 98.1ms\n",
      "Speed: 5.9ms preprocess, 98.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.5ms\n",
      "Speed: 3.8ms preprocess, 70.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 2 buss, 1 truck, 96.4ms\n",
      "Speed: 5.3ms preprocess, 96.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.5ms\n",
      "Speed: 3.9ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 2 buss, 1 truck, 89.2ms\n",
      "Speed: 3.4ms preprocess, 89.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 83.0ms\n",
      "Speed: 2.4ms preprocess, 83.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 1 bus, 2 trucks, 106.1ms\n",
      "Speed: 4.5ms preprocess, 106.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 82.0ms\n",
      "Speed: 2.7ms preprocess, 82.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 87.2ms\n",
      "Speed: 5.7ms preprocess, 87.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 79.9ms\n",
      "Speed: 2.5ms preprocess, 79.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 3 buss, 1 truck, 94.7ms\n",
      "Speed: 6.1ms preprocess, 94.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.2ms\n",
      "Speed: 3.5ms preprocess, 81.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 1 truck, 98.8ms\n",
      "Speed: 3.4ms preprocess, 98.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.6ms\n",
      "Speed: 3.3ms preprocess, 80.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 1 truck, 95.2ms\n",
      "Speed: 4.9ms preprocess, 95.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 73.3ms\n",
      "Speed: 4.2ms preprocess, 73.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 2 buss, 1 truck, 80.4ms\n",
      "Speed: 4.8ms preprocess, 80.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 69.8ms\n",
      "Speed: 3.5ms preprocess, 69.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 1 truck, 101.4ms\n",
      "Speed: 4.5ms preprocess, 101.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 78.8ms\n",
      "Speed: 3.1ms preprocess, 78.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 93.0ms\n",
      "Speed: 4.8ms preprocess, 93.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 71.0ms\n",
      "Speed: 2.7ms preprocess, 71.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 2 trucks, 95.0ms\n",
      "Speed: 4.7ms preprocess, 95.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 71.5ms\n",
      "Speed: 2.9ms preprocess, 71.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 1 truck, 113.2ms\n",
      "Speed: 5.3ms preprocess, 113.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.6ms\n",
      "Speed: 2.4ms preprocess, 83.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 2 trucks, 101.8ms\n",
      "Speed: 6.1ms preprocess, 101.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 80.5ms\n",
      "Speed: 3.1ms preprocess, 80.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 2 buss, 2 trucks, 116.1ms\n",
      "Speed: 4.0ms preprocess, 116.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 91.6ms\n",
      "Speed: 3.5ms preprocess, 91.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 2 trucks, 99.9ms\n",
      "Speed: 3.3ms preprocess, 99.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 84.6ms\n",
      "Speed: 3.2ms preprocess, 84.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 3 trucks, 95.3ms\n",
      "Speed: 4.1ms preprocess, 95.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 69.2ms\n",
      "Speed: 2.4ms preprocess, 69.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 3 trucks, 104.5ms\n",
      "Speed: 5.4ms preprocess, 104.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 67.9ms\n",
      "Speed: 3.3ms preprocess, 67.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 2 buss, 2 trucks, 106.3ms\n",
      "Speed: 4.0ms preprocess, 106.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 76.6ms\n",
      "Speed: 2.4ms preprocess, 76.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 115.5ms\n",
      "Speed: 5.2ms preprocess, 115.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.5ms\n",
      "Speed: 3.2ms preprocess, 80.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 2 buss, 2 trucks, 114.0ms\n",
      "Speed: 5.7ms preprocess, 114.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 67.2ms\n",
      "Speed: 3.3ms preprocess, 67.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 2 buss, 1 truck, 186.3ms\n",
      "Speed: 22.2ms preprocess, 186.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 68.9ms\n",
      "Speed: 2.8ms preprocess, 68.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 2 buss, 1 truck, 111.3ms\n",
      "Speed: 3.5ms preprocess, 111.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.1ms\n",
      "Speed: 2.3ms preprocess, 70.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 1 bus, 3 trucks, 103.6ms\n",
      "Speed: 4.0ms preprocess, 103.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 79.1ms\n",
      "Speed: 3.1ms preprocess, 79.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 3 trucks, 108.7ms\n",
      "Speed: 9.3ms preprocess, 108.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 96.8ms\n",
      "Speed: 3.6ms preprocess, 96.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 100.7ms\n",
      "Speed: 6.5ms preprocess, 100.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 82.8ms\n",
      "Speed: 2.2ms preprocess, 82.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 102.3ms\n",
      "Speed: 4.8ms preprocess, 102.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.5ms\n",
      "Speed: 3.1ms preprocess, 83.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 107.4ms\n",
      "Speed: 4.4ms preprocess, 107.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.0ms\n",
      "Speed: 2.6ms preprocess, 83.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 cars, 1 bus, 1 truck, 98.1ms\n",
      "Speed: 4.3ms preprocess, 98.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 82.6ms\n",
      "Speed: 3.7ms preprocess, 82.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to draw bounding boxes and text\n",
    "def draw_annotations(frame, annotations, color):\n",
    "    for (x1, y1, x2, y2, text) in annotations:\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        if text:\n",
    "            # Draw a filled rectangle as background for text\n",
    "            (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)\n",
    "            cv2.rectangle(frame, (int(x1), int(y1) - 30), (int(x1) + w, int(y1)), (0, 0, 0), -1)\n",
    "            # Put the text on top of the background\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "# Function to preprocess image for OCR\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    enhanced = cv2.equalizeHist(gray)\n",
    "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    binary = cv2.dilate(binary, kernel, iterations=1)\n",
    "    binary = cv2.erode(binary, kernel, iterations=1)\n",
    "    return binary\n",
    "\n",
    "# Specify the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# Load models\n",
    "car_detector = YOLO('yolov8n.pt')\n",
    "license_plate_detector = YOLO('license_plate_detector.pt')\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('car.mp4')\n",
    "\n",
    "# Define the list of vehicle class IDs (as per your model's class mapping)\n",
    "vehicles = [2, 3, 5, 7]\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Read frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # Process every nth frame to reduce computation\n",
    "    if frame_count % 5 != 0:\n",
    "        continue\n",
    "\n",
    "    # Detect vehicles\n",
    "    car_detections = car_detector(frame)[0]\n",
    "    car_annotations = []\n",
    "    for detection in car_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        if int(class_id) in vehicles:\n",
    "            car_annotations.append((x1, y1, x2, y2, f'Car: {int(score * 100)}%'))\n",
    "\n",
    "    # Detect license plates\n",
    "    license_plate_detections = license_plate_detector(frame)[0]\n",
    "    license_plate_annotations = []\n",
    "    for license_plate in license_plate_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = license_plate\n",
    "        license_plate_crop = frame[int(y1):int(y2), int(x1): int(x2), :]\n",
    "        processed_image = preprocess_image(license_plate_crop)\n",
    "        license_plate_text = pytesseract.image_to_string(processed_image, config='--psm 8').strip()\n",
    "        license_plate_annotations.append((x1, y1, x2, y2, license_plate_text))\n",
    "\n",
    "    draw_annotations(frame, car_annotations, (0, 0, 255))  # Red rectangles for cars\n",
    "    draw_annotations(frame, license_plate_annotations, (255, 0, 0))  # Blue rectangles for license plates\n",
    "    \n",
    "    frame_resized = cv2.resize(frame, (800, 450))\n",
    "    \n",
    "    cv2.imshow('Frame', frame_resized)\n",
    "\n",
    "    # Show OCR\n",
    "    cv2.imshow('License Plate Detection', processed_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
