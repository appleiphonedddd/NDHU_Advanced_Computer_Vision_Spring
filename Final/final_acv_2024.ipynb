{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Explanation\n",
    "\n",
    "1. **Read Frame and Select ROI (Region of Interest)**:\n",
    "   $$\n",
    "   \\text{frame} = \\text{cap.read}(\\text{frame\\_number})\n",
    "   $$\n",
    "   $$\n",
    "   \\text{ROI} = \\text{selectROI}(\\text{frame})\n",
    "   $$\n",
    "   Here, the user selects a rectangular region (ROI) in the frame. Let's denote this region by its coordinates \\((x, y, w, h)\\), where \\(x\\) and \\(y\\) are the top-left coordinates, and \\(w\\) and \\(h\\) are the width and height of the region.\n",
    "\n",
    "   The template image \\(T\\) is then:\n",
    "   $$\n",
    "   T = \\text{frame}[y:y+h, x:x+w]\n",
    "   $$\n",
    "\n",
    "\n",
    "2. **Frame Processing Loop**:\n",
    "   For each frame $F_{\\text{i}}$ in the video (where ${\\text{(i)}}$ is the frame index), the loop processes until frame number 5000:\n",
    "   $$\n",
    "   F_i = \\text{cap.read}(i)\n",
    "   $$\n",
    "   If \\(i > 5000\\), exit the loop.\n",
    "\n",
    "3. **Apply Gaussian Blur**:\n",
    "   $$\n",
    "   F_i^{\\text{blur}} = \\text{GaussianBlur}(F_i, (5, 5), 0)\n",
    "   $$\n",
    "   This step applies a Gaussian filter to the frame to reduce noise.\n",
    "\n",
    "4. **Template Matching**:\n",
    "   $$\n",
    "   \\text{res} = \\text{matchTemplate}(F_i^{\\text{blur}}, T, \\text{TM\\_CCOEFF\\_NORMED})\n",
    "   $$\n",
    "   The `matchTemplate` function computes the normalized cross-correlation between the template \\(T\\) and the current frame \\(F_i^blur\\). The result is a matrix `res` where each element represents the correlation coefficient at that point.\n",
    "   \n",
    "   $$\n",
    "   (\\min_{\\text{val}}, \\max_{\\text{val}}, \\min_{\\text{loc}}, \\max_{\\text{loc}}) = \\text{minMaxLoc}(\\text{res})\n",
    "   $$\n",
    "   \n",
    "   This function finds the minimum and maximum values and their locations in the result matrix. We are interested in `loc`, the location of the highest correlation.\n",
    "\n",
    "   Let:\n",
    "   $$\n",
    "   \\text{top\\_left} = \\max_{\\text{loc}}\n",
    "   $$\n",
    "   $$\n",
    "   \\text{bottom\\_right} = (\\text{top\\_left}[0] + w, \\text{top\\_left}[1] + h)\n",
    "   $$\n",
    "\n",
    "In summary, the mathematical operations involve:\n",
    "- Extracting a template image from a specific region in a frame.\n",
    "- Applying Gaussian blur to reduce noise.\n",
    "- Using normalized cross-correlation to find the best match of the template in subsequent frames.\n",
    "- Identifying the location of the best match and drawing a rectangle around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "1A. Input images from video file WiiPlay.mp4 with level 15 (frame number between 4820 and 5000).<br> \\\n",
    "1B. (5pts) Acquire a <b>face template</b> from the first frame (frame number = 4820).<br>\\\n",
    "1C. (10pts) Try to detect the face the same as the template on subsequent frames, draw a <b>red</b> rectangle around the detected face, and show the output images in the <b>\"find_this_mii\"</b> window.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    }
   ],
   "source": [
    "#game_1 : \"find_this_mii\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Set the current video frame to 4820\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 4820)\n",
    "\n",
    "# Capture a single frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Select a region of interest (ROI) manually from the captured frame for template matching\n",
    "r = cv2.selectROI(frame)\n",
    "template = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if frame cannot be read or the current frame exceeds 5000\n",
    "    if not ret or cap.get(cv2.CAP_PROP_POS_FRAMES) > 5000:\n",
    "        break\n",
    "\n",
    "    # Apply Gaussian Blur to the frame to reduce noise for better template matching\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "    # Perform template matching to find the template in the frame\n",
    "    res = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # Find the location of the template in the frame\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    top_left = max_loc\n",
    "    h, w, _ = template.shape\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    # Draw a rectangle around the template in the frame\n",
    "    cv2.rectangle(frame, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame with the rectangle around the template\n",
    "    cv2.imshow('find_this_mii', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Face Detection Process\n",
    "\n",
    "### Inputs:\n",
    "- Let \\( frame \\) be the image frame in which to detect faces.\n",
    "- Let \\( lower_skin \\) be the lower bound of the HSV color range for skin detection.\n",
    "- Let \\( upper_skin \\) be the upper bound of the HSV color range for skin detection.\n",
    "- Let \\( min_area \\) be the minimum area of a detected face.\n",
    "- Let \\( max\\_area \\) be the maximum area of a detected face.\n",
    "- Let \\( aspect_ratio_range \\) be the range of acceptable aspect ratios for detected faces.\n",
    "\n",
    "### Steps and Mathematical Operations\n",
    "\n",
    "1. **Convert Frame to HSV Color Space**:\n",
    "   Convert the frame from BGR to HSV color space:\n",
    "   $$\n",
    "   \\text{hsv} = \\text{cvtColor}(\\text{frame}, \\text{COLOR\\_BGR2HSV})\n",
    "   $$\n",
    "\n",
    "2. **Create Binary Mask for Skin Detection**:\n",
    "   Create a binary mask where pixels within the skin color range are set to 1 (white), and all other pixels are set to 0 (black):\n",
    "   $$\n",
    "   \\text{mask} = \\text{inRange}(\\text{hsv}, \\text{lower\\_skin}, \\text{upper\\_skin})\n",
    "   $$\n",
    "\n",
    "3. **Morphological Operations**:\n",
    "   - **Erosion**: Reduce noise by shrinking the white regions:\n",
    "     $$\n",
    "     \\text{mask} = \\text{erode}(\\text{mask}, \\text{kernel}, \\text{iterations} = 2)\n",
    "     $$\n",
    "   - **Dilation**: Expand the white regions to restore the eroded parts:\n",
    "     $$\n",
    "     \\text{mask} = \\text{dilate}(\\text{mask}, \\text{kernel}, \\text{iterations} = 2)\n",
    "     $$\n",
    "   - **Gaussian Blur**: Smooth the mask to further reduce noise:\n",
    "     $$\n",
    "     \\text{mask} = \\text{GaussianBlur}(\\text{mask}, (3, 3), 0)\n",
    "     $$\n",
    "\n",
    "4. **Find Contours in the Mask**:\n",
    "   Identify the boundaries of connected white regions (potential faces) in the mask:\n",
    "   $$\n",
    "   \\text{contours}, \\_ = \\text{findContours}(\\text{mask}, \\text{RETR\\_EXTERNAL}, \\text{CHAIN\\_APPROX\\_SIMPLE})\n",
    "   $$\n",
    "\n",
    "5. **Filter Contours Based on Geometric Properties**:\n",
    "   For each contour:\n",
    "   - Compute the bounding rectangle:\n",
    "     $$\n",
    "     (x, y, w, h) = \\text{boundingRect}(\\text{contour})\n",
    "     $$\n",
    "     Here, \\( x, y \\) are the coordinates of the top-left corner, and \\( w, h \\) are the width and height of the rectangle.\n",
    "   \n",
    "   - Calculate the aspect ratio (\\( AR \\)) and area (\\( A \\)):\n",
    "     $$\n",
    "     AR = \\frac{w}{h}\n",
    "     $$\n",
    "     $$\n",
    "     A = \\text{contourArea}(\\text{contour})\n",
    "     $$\n",
    "\n",
    "   - Check if the contour meets the aspect ratio and area criteria:\n",
    "     $$\n",
    "     \\text{if} \\quad \\text{aspect\\_ratio\\_range}[0] < AR < \\text{aspect\\_ratio\\_range}[1] \\quad \\text{and} \\quad \\text{min\\_area} < A < \\text{max\\_area}\n",
    "     $$\n",
    "     If true, add the bounding rectangle coordinates to the list of detected faces:\n",
    "     $$\n",
    "     \\text{faces.append}((x, y, w, h))\n",
    "     $$\n",
    "     Draw a rectangle around the detected face in the frame:\n",
    "     $$\n",
    "     \\text{rectangle}(\\text{frame}, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "     $$\n",
    "\n",
    "### Outputs\n",
    "- **faces**: A list of tuples, each containing the coordinates and dimensions of a detected face.\n",
    "- **frame**: The processed frame with rectangles drawn around detected faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Face Comparison Process\n",
    "\n",
    "### Inputs:\n",
    "- Let \\( face1 \\) be the first face image.\n",
    "- Let \\( face2 \\) be the second face image.\n",
    "\n",
    "### Steps and Mathematical Operations\n",
    "\n",
    "1. **Resize Faces to a Fixed Size**:\n",
    "   - Resize both face images to \\( 100 * 100 \\) pixels.\n",
    "   $$\n",
    "   \\text{face1\\_resized} = \\text{resize}(\\text{face1}, (100, 100))\n",
    "   $$\n",
    "   $$\n",
    "   \\text{face2\\_resized} = \\text{resize}(\\text{face2}, (100, 100))\n",
    "   $$\n",
    "\n",
    "2. **Calculate the Absolute Difference**:\n",
    "   - Compute the absolute difference between the corresponding pixels of the two resized face images.\n",
    "   $$\n",
    "   \\text{difference} = \\text{absdiff}(\\text{face1\\_resized}, \\text{face2\\_resized})\n",
    "   $$\n",
    "   This operation results in a matrix where each element represents the absolute difference between the corresponding pixels of the two images.\n",
    "\n",
    "3. **Sum the Differences**:\n",
    "   - Sum all the elements in the difference matrix to obtain a similarity score.\n",
    "   $$\n",
    "   \\text{similarity\\_score} = \\sum_{i,j} \\text{difference}(i,j)\n",
    "   $$\n",
    "\n",
    "### Outputs\n",
    "- **similarity_score**: The sum of absolute differences between the two images, which serves as a measure of similarity. A lower score indicates higher similarity between the two face images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of in Game_2: \"find_two_look_alike\"\n",
    "\n",
    "### Inputs:\n",
    "- **Video Capture**: \n",
    "  - Let `cap` be the video capture object for the video file 'WiiPlay.mp4'.\n",
    "- **HOG Descriptor for People Detection**:\n",
    "  - Let `hog` be the HOG descriptor initialized for people detection.\n",
    "- **Frame Range**:\n",
    "  - Let `start_frame` = 2180  be the starting frame.\n",
    "  - Let `end_frame` = 2380  be the ending frame.\n",
    "- **HSV Color Range for Skin Detection**:\n",
    "  - Let `lower_skin` = [0, 48, 80] be the lower bound of the HSV color range for skin detection.\n",
    "  - Let `upper_skin` = [20, 255, 255] be the upper bound of the HSV color range for skin detection.\n",
    "- **Area and Aspect Ratio Range**:\n",
    "  - Let `min_area` = 300 be the minimum area of a detected face.\n",
    "  - Let `max_area` = 1000 be the maximum area of a detected face. \n",
    "  - Let `aspect_ratio_range` = (0.5, 2) be the range of acceptable aspect ratios for detected faces.\n",
    "\n",
    "### Steps and Mathematical Operations\n",
    "\n",
    "1. **Video Capture Initialization**:\n",
    "   - Initialize video capture and set the starting frame.\n",
    "   $$\n",
    "   \\text{cap.set}(\\text{cv2.CAP\\_PROP\\_POS\\_FRAMES}, \\text{start\\_frame})\n",
    "   $$\n",
    "\n",
    "2. **People Detection Using HOG Descriptor**:\n",
    "   - For each frame, detect people using the HOG descriptor.\n",
    "   $$\n",
    "   \\text{boxes}, \\text{weights} = \\text{hog.detectMultiScale}(\\text{frame}, \\text{winStride}=(8, 8))\n",
    "   $$\n",
    "   - Convert the bounding boxes into the format \\([x, y, x + w, y + h]\\).\n",
    "\n",
    "3. **Face Detection Using HSV Color Range**:\n",
    "   - Convert the frame to HSV color space.\n",
    "   $$\n",
    "   \\text{hsv} = \\text{cvtColor}(\\text{frame}, \\text{COLOR\\_BGR2HSV})\n",
    "   $$\n",
    "   - Create a binary mask based on the skin color range.\n",
    "   $$\n",
    "   \\text{mask} = \\text{inRange}(\\text{hsv}, \\text{lower\\_skin}, \\text{upper\\_skin})\n",
    "   $$\n",
    "   - Apply morphological operations (erosion, dilation) and Gaussian blur to refine the mask.\n",
    "   $$\n",
    "   \\text{mask} = \\text{erode}(\\text{mask}, \\text{kernel}, \\text{iterations} = 2)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{mask} = \\text{dilate}(\\text{mask}, \\text{kernel}, \\text{iterations} = 2)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{mask} = \\text{GaussianBlur}(\\text{mask}, (3, 3), 0)\n",
    "   $$\n",
    "   - Find contours in the mask and filter based on area and aspect ratio.\n",
    "   $$\n",
    "   \\text{contours}, \\_ = \\text{findContours}(\\text{mask}, \\text{RETR\\_EXTERNAL}, \\text{CHAIN\\_APPROX\\_SIMPLE})\n",
    "   $$\n",
    "\n",
    "4. **Face Comparison**:\n",
    "   - For each pair of detected faces, calculate the similarity score using the sum of absolute differences.\n",
    "   $$\n",
    "   \\text{difference} = \\text{absdiff}(\\text{face1\\_resized}, \\text{face2\\_resized})\n",
    "   $$\n",
    "   $$\n",
    "   \\text{similarity\\_score} = \\sum_{i,j} \\text{difference}(i,j)\n",
    "   $$\n",
    "   - Identify the most similar pair of faces based on the lowest similarity score.\n",
    "\n",
    "5. **Draw Rectangles Around Detected Faces**:\n",
    "   - Draw rectangles around detected faces and the most similar pair.\n",
    "   $$\n",
    "   \\text{rectangle}(\\text{frame}, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{rectangle}(\\text{frame}, (x2, y2), (x2+w2, y2+h2), (0, 0, 255), 2)\n",
    "   $$\n",
    "\n",
    "6. **Display the Frame**:\n",
    "   - Display the frame with detected faces.\n",
    "   $$\n",
    "   \\text{cv2.imshow}('find\\_two\\_look\\_alike', \\text{frame})\n",
    "   $$\n",
    "\n",
    "### Outputs\n",
    "- **Detected Faces**: Bounding boxes of detected faces.\n",
    "- **Most Similar Pair**: Bounding boxes of the most similar pair of faces.\n",
    "- **Displayed Frame**: Frame with rectangles drawn around detected faces and the most similar pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "2A. Input images from video file WiiPlay.mp4 with level 8 (frame number between 2180 and 2380).<br>\\\n",
    "2B. (5pts) Detect <b>pedestrians</b> on each frame and draw a <b>green</b> rectangle around your detection.<br>\\\n",
    "2C. (5pts) Detect <b>faces</b> on each frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "2D. (10pts) Try to find two faces look like each other, draw a <b>red</b> rectangle around each of the two faces, and show the output images in the <b>\"find_two_look_alike\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_2 : \"find_two_look_alike\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range):\n",
    "    \"\"\"\n",
    "    Detects faces in the given frame based on skin color and filters them by area and aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        frame (numpy.ndarray): The image frame in which to detect faces.\n",
    "        lower_skin (numpy.ndarray): The lower bound of the HSV color range for skin detection.\n",
    "        upper_skin (numpy.ndarray): The upper bound of the HSV color range for skin detection.\n",
    "        min_area (int): The minimum area of a detected face.\n",
    "        max_area (int): The maximum area of a detected face.\n",
    "        aspect_ratio_range (tuple): The range of acceptable aspect ratios for detected faces.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of detected faces and the processed frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the frame to the HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a binary mask of the skin\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Create a structuring element for morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "\n",
    "    # Erode and dilate the mask to remove noise\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the mask\n",
    "    mask = cv2.GaussianBlur(mask, (3, 3), 0)\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    faces = []\n",
    "    for contour in contours:\n",
    "\n",
    "        # Get the bounding rectangle for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = w / float(h)\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Filter contours based on aspect ratio and area\n",
    "        if aspect_ratio_range[0] < aspect_ratio < aspect_ratio_range[1] and min_area < area < max_area:\n",
    "            faces.append((x, y, w, h))\n",
    "\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    return faces, frame\n",
    "\n",
    "def compare_faces(face1, face2):\n",
    "    \"\"\"\n",
    "    Compares two face images and returns the sum of absolute differences as a similarity score.\n",
    "\n",
    "    Args:\n",
    "        face1 (numpy.ndarray): The first face image.\n",
    "        face2 (numpy.ndarray): The second face image.\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of absolute differences between the two images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resize faces to a fixed size\n",
    "    face1_resized = cv2.resize(face1, (100, 100))\n",
    "    face2_resized = cv2.resize(face2, (100, 100))\n",
    "\n",
    "    # Calculate the absolute difference between the two faces\n",
    "    difference = cv2.absdiff(face1_resized, face2_resized)\n",
    "\n",
    "    # Sum the differences to get a similarity score\n",
    "    return np.sum(difference)\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "# Initialize HOG descriptor for people detection\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# Set the range of frames to process\n",
    "start_frame = 2180\n",
    "end_frame = 2380\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "# Define HSV color range for skin detection\n",
    "lower_skin = np.array([0, 48, 80], dtype='uint8')\n",
    "upper_skin = np.array([20, 255, 255], dtype='uint8')\n",
    "min_area = 300\n",
    "max_area = 1000\n",
    "aspect_ratio_range = (0.5, 2)\n",
    "\n",
    "# Initialize the faces list\n",
    "faces = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the frame is not read correctly or the end frame is reached\n",
    "    if not ret or current_frame > end_frame:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect people in the frame using HOG\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "        # If there are at least two faces detected, compare them\n",
    "        if len(faces) >= 2:\n",
    "            min_difference = float('inf')\n",
    "            best_pair = (None, None)\n",
    "\n",
    "            # Compare each pair of faces to find the most similar pair\n",
    "            for i in range(len(faces)):\n",
    "\n",
    "                # Compare the current face with all other faces\n",
    "                for j in range(i + 1, len(faces)):\n",
    "\n",
    "                    # Get the bounding box coordinates for the two faces\n",
    "                    x1, y1, w1, h1 = faces[i]\n",
    "                    x2, y2, w2, h2 = faces[j]\n",
    "                    face1 = frame[y1:y1+h1, x1:x1+w1]\n",
    "                    face2 = frame[y2:y2+h2, x2:x2+w2]\n",
    "\n",
    "                    # Calculate the similarity score\n",
    "                    difference = compare_faces(face1, face2)\n",
    "\n",
    "                    # Update the best pair if the current pair is more similar\n",
    "                    if difference < min_difference:\n",
    "                        min_difference = difference\n",
    "                        best_pair = (i, j)\n",
    "\n",
    "            # To check if the best pair is found\n",
    "            if best_pair[0] is not None and best_pair[1] is not None:\n",
    "                x1, y1, w1, h1 = faces[best_pair[0]]\n",
    "                x2, y2, w2, h2 = faces[best_pair[1]]\n",
    "\n",
    "                # Draw rectangles around the most similar pair of faces\n",
    "                cv2.rectangle(frame, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "                cv2.rectangle(frame, (x2, y2), (x2+w2, y2+h2), (0, 0, 255), 2)\n",
    "    # Detect faces in the frame\n",
    "    faces, frame = detect_faces(frame, lower_skin, upper_skin, min_area, max_area, aspect_ratio_range)\n",
    "    \n",
    "    # Display the frame \n",
    "    cv2.imshow('find_two_look_alike', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "3A. Input images from video file WiiPlay.mp4 with level 9 (frame number between 2480 and 2600).<br>\\\n",
    "3B. (5pts) <b>Detect </b>faces(or pedestrians) on the first frame and draw a <b>blue</b> rectangle around your detection.<br>\\\n",
    "3C. (10pts) <b>Track </b>faces(or pedestrians) on subsequent frames and draw a <b>green</b> rectangle around your tracking.<br>\\\n",
    "3D. (5pts) Try to find out the fastest character, draw a <b>red</b> rectangle around the fastest character, and show the output images in the <b>\"find_the_fastest_character\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_3 : \"find_the_fastest_character\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('WiiPlay.mp4')\n",
    "\n",
    "start_frame = 2480\n",
    "end_frame = 2600\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video\")\n",
    "    exit()\n",
    "\n",
    "boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "\n",
    "for (x, y, w, h) in boxes:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "for box in boxes:\n",
    "    tracker = cv2.legacy.TrackerMIL_create()\n",
    "    trackers.add(tracker, frame, tuple(box))\n",
    "\n",
    "current_frame = start_frame\n",
    "\n",
    "fastest_speed = 0\n",
    "fastest_box = None\n",
    "prev_position = [tuple(box) for box in boxes]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    success, tracked_boxes = trackers.update(frame)\n",
    "    if success:\n",
    "        speeds = []\n",
    "\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "            prev_x, prev_y, _, _ = prev_position[i]\n",
    "            speed = ((x - prev_x) ** 2 + (y - prev_y) ** 2) ** 0.5\n",
    "            speeds.append(speed)\n",
    "            prev_position[i] = (x, y, w, h)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        max_speed = max(speeds)\n",
    "        if max_speed > fastest_speed:\n",
    "            fastest_speed = max_speed\n",
    "            fastest_box = tracked_boxes[speeds.index(max_speed)]\n",
    "\n",
    "    if fastest_box is not None:\n",
    "        x, y, w, h = [int(v) for v in fastest_box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "       \n",
    "        for box in tracked_boxes:\n",
    "            x, y, w, h = [int(v) for v in box]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    detected_boxes, weights = hog.detectMultiScale(frame, winStride=(8, 8))\n",
    "    for (x, y, w, h) in detected_boxes:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    text = f'Current_Frame: {current_frame}'\n",
    "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "    text_x = frame.shape[1] - text_size[0] - 10\n",
    "    text_y = 30\n",
    "    cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('find_the_fastest_character', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "4A. Input images from video file WiiPlay.mp4 with level 6 (frame number between 1650 and 1800).<br>\\\n",
    "4B. (10pts) Compute and show <b>optical flows</b> on each frame using <b>blue</b> arrows.<br>\\\n",
    "4C. (5pts) Try to detect two odd character who face the opposite direction from everyone else, draw a <b>red</b> rectangle around each of the two character, and show the output images in the <b>\"find_two_odds\"</b> window.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_4 : \"find_two_odds\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def display_flow(img, flow, stride=10):\n",
    "    height, width = img.shape[:2]\n",
    "    odd_characters = []\n",
    "\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "            flow_at_point = flow[y, x]\n",
    "            if np.linalg.norm(flow_at_point) > 2:  # Consider only significant flows\n",
    "                direction = np.arctan2(flow_at_point[1], flow_at_point[0])\n",
    "                odd_characters.append((x, y, direction))\n",
    "                pt1 = (x, y)\n",
    "                delta = flow_at_point.astype(np.int32)[::-1]\n",
    "                pt2 = (pt1[0] + delta[0] * 2, pt1[1] + delta[1] * 2)\n",
    "                cv2.arrowedLine(img, pt1, pt2, (255, 0, 0), 1, cv2.LINE_AA, 0, 0.1)\n",
    "\n",
    "    if len(odd_characters) > 2:\n",
    "        directions = np.array([c[2] for c in odd_characters])\n",
    "        median_direction = np.median(directions)\n",
    "        deviations = np.abs(directions - median_direction)\n",
    "        odd_indices = deviations.argsort()[-2:]  # Get indices of two most deviating directions\n",
    "\n",
    "        for idx in odd_indices:\n",
    "            x, y, _ = odd_characters[idx]\n",
    "            cv2.rectangle(img, (x-15, y-15), (x+15, y+15), (0, 0, 255), 2)  # Draw red rectangle around odd characters\n",
    "\n",
    "    norm_opt_flow = np.linalg.norm(flow, axis=2)\n",
    "    norm_opt_flow = cv2.normalize(norm_opt_flow, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.imshow('find_two_odds', img)\n",
    "    cv2.imshow('optical flow magnitude', norm_opt_flow)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "start_frame = 1650\n",
    "end_frame = 1800\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "_ , prev_frame = cap.read()\n",
    "prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "prev_frame = cv2.resize(prev_frame, (0,0), None, 0.5, 0.5)\n",
    "first_frame = True\n",
    "\n",
    "current_frame = start_frame\n",
    "fps = 0\n",
    "\n",
    "while True:\n",
    "    status_cap, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (0,0), None, 0.5, 0.5)\n",
    "\n",
    "    if not status_cap or current_frame >= end_frame:\n",
    "        break\n",
    "\n",
    "    timer = cv2.getTickCount()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if first_frame:\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, 0.5, 5, 13, 10, 5, 1.1, cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "        first_frame = False\n",
    "    else:\n",
    "        opt_flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, opt_flow, 0.5, 5, 13, 10, 5, 1.1, cv2.OPTFLOW_USE_INITIAL_FLOW)\n",
    "\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    prev_frame = np.copy(gray)\n",
    "    \n",
    "    if display_flow(frame, opt_flow):\n",
    "        break\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "5A. Input continuous images from 'car.mp4'.<br>\\\n",
    "5B. (6pts) For each frame, detect every car using YOLOv8 trained data 'yolov8n.pt'. (mark with red rectangles)<br>\\\n",
    "5C. (6pts) For each car, detect a licence plate using 'license_plate_detector.pt'. (mark with blue rectangle)<br>\\\n",
    "5D. (6pts) For each licence plate, OCR using Tesseract. Print the recognized licence plate number above each detected licence plate. (putText() in green color)<br>\\\n",
    "5E. (12pts) Use whatever you learned this semester to improve the result. Write a simple report on your method and observations.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 94.6ms\n",
      "Speed: 5.9ms preprocess, 94.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 90.9ms\n",
      "Speed: 4.3ms preprocess, 90.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pnesivsu\n",
      "License Plate: “RNAI NRU\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 106.1ms\n",
      "Speed: 4.9ms preprocess, 106.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 108.2ms\n",
      "Speed: 3.1ms preprocess, 108.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pnesivsu\n",
      "License Plate: “RNAI NRU\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 97.5ms\n",
      "Speed: 3.6ms preprocess, 97.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.6ms\n",
      "Speed: 3.5ms preprocess, 81.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: Pes vsu)\n",
      "License Plate: JENA NR\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 92.2ms\n",
      "Speed: 3.7ms preprocess, 92.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 87.3ms\n",
      "Speed: 3.6ms preprocess, 87.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "License Plate: EWALD NRU_\n",
      "License Plate: CG\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "from ultralytics import YOLO\n",
    "import pytesseract\n",
    "\n",
    "def draw_annotations(frame, annotations, color, font_scale=0.9):\n",
    "    for (x1, y1, x2, y2, text) in annotations:\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        if text:\n",
    "            cv2.putText(frame, text, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), 2)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to remove noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to get a binary image\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Optionally apply morphology operations to clean up the image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    return binary\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# Load models\n",
    "car_detector = YOLO('yolov8n.pt')\n",
    "license_plate_detector = YOLO('license_plate_detector.pt')\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('car.mp4')\n",
    "\n",
    "# Define the list of vehicle class IDs (as per your model's class mapping)\n",
    "vehicles = [2, 3, 5, 7]\n",
    "\n",
    "# Read frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect vehicles\n",
    "    car_detections = car_detector(frame)[0]\n",
    "    car_annotations = []\n",
    "    for detection in car_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        if int(class_id) in vehicles:\n",
    "            car_annotations.append((x1, y1, x2, y2, f'Car: {int(score * 100)}%'))\n",
    "\n",
    "    # Detect license plates\n",
    "    license_plate_detections = license_plate_detector(frame)[0]\n",
    "    license_plate_annotations = []\n",
    "    for license_plate in license_plate_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = license_plate\n",
    "        license_plate_crop = frame[int(y1):int(y2), int(x1): int(x2), :]\n",
    "        processed_image = preprocess_image(license_plate_crop)\n",
    "        # Adjust Tesseract OCR configuration for better accuracy\n",
    "        custom_config = r'--oem 3 --psm 8'\n",
    "        license_plate_text = pytesseract.image_to_string(processed_image, config=custom_config).strip()\n",
    "        license_plate_annotations.append((x1, y1, x2, y2, license_plate_text))\n",
    "\n",
    "        text = pytesseract.image_to_string(processed_image, config=custom_config).strip()\n",
    "        print(f'License Plate: {text}')\n",
    "\n",
    "    draw_annotations(frame, car_annotations, (0, 0, 255), font_scale = 0.9)  # Red rectangles for cars\n",
    "    draw_annotations(frame, license_plate_annotations, (255, 0, 0), font_scale = 2)  # Blue rectangles for license plates\n",
    "    \n",
    "    frame_resized = cv2.resize(frame, (800, 450))\n",
    "    \n",
    "    cv2.imshow('License Plate Detection', frame_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (5pts) Any comments regarding the final exam? Which steps you believe you have completed? Which steps bother you?<br> \n",
    "7. (5pts) Any suggestion to teaching assistants to improve this class? Any suggestion to teacher to improve this class?<br>\n",
    "\n",
    "\n",
    "### My Answer\n",
    "\n",
    "6. In the final exam, I completed all the first questions, and I was left with the most difficult functions that had not yet been optimized. \n",
    "\n",
    "   There are five questions in total, and I think the most difficult part of the second to fifth questions is to deal with the frame of the video, because in addition to the basic computer vision basics, such as filtering, noise reduction, edge detection, and morphology, we also need to overcome the problems of the film itself to improve the saturation and other related technologies, so that we can present the best results\n",
    "\n",
    "7. After a semester, I think that the TA system of advanced computer vision is working very well, and basically every TA has     helped me to grasp the basic key knowledge in this class\n",
    "\n",
    "   As for the suggestion for this class, I think for the final exam, we can take the students to team up to participate in the CVPR Data CV Challenge, which is organized by a very famous computer vision workshop to test whether you can produce a good computer vision work in a limited time\n",
    "\n",
    "   \n",
    "   - [CVPR](https://sites.google.com/view/vdu-cvpr24/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [OpenCV Tutorial](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "\n",
    "- [Template Matching function reference code and theory](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html)\n",
    "\n",
    "- [OpenCV selectROI function reference code and theory](https://www.geeksforgeeks.org/python-opencv-selectroi-function/)\n",
    "\n",
    "- [detect_faces function reference code and theory in GitHub](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n",
    "\n",
    "- [compare_faces function reference code and theory](https://stackoverflow.com/questions/23195522/opencv-fastest-method-to-check-if-two-images-are-100-same-or-not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 93.4ms\n",
      "Speed: 3.3ms preprocess, 93.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 84.6ms\n",
      "Speed: 2.5ms preprocess, 84.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 134.5ms\n",
      "Speed: 9.1ms preprocess, 134.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 89.1ms\n",
      "Speed: 3.4ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 100.4ms\n",
      "Speed: 6.5ms preprocess, 100.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 72.7ms\n",
      "Speed: 2.5ms preprocess, 72.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 1 bus, 2 trucks, 95.1ms\n",
      "Speed: 4.4ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.8ms\n",
      "Speed: 3.2ms preprocess, 70.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 cars, 2 buss, 1 truck, 98.1ms\n",
      "Speed: 5.9ms preprocess, 98.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.5ms\n",
      "Speed: 3.8ms preprocess, 70.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 2 buss, 1 truck, 96.4ms\n",
      "Speed: 5.3ms preprocess, 96.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.5ms\n",
      "Speed: 3.9ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 2 buss, 1 truck, 89.2ms\n",
      "Speed: 3.4ms preprocess, 89.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 83.0ms\n",
      "Speed: 2.4ms preprocess, 83.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 1 bus, 2 trucks, 106.1ms\n",
      "Speed: 4.5ms preprocess, 106.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 82.0ms\n",
      "Speed: 2.7ms preprocess, 82.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 87.2ms\n",
      "Speed: 5.7ms preprocess, 87.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 79.9ms\n",
      "Speed: 2.5ms preprocess, 79.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 3 buss, 1 truck, 94.7ms\n",
      "Speed: 6.1ms preprocess, 94.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 81.2ms\n",
      "Speed: 3.5ms preprocess, 81.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 1 truck, 98.8ms\n",
      "Speed: 3.4ms preprocess, 98.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.6ms\n",
      "Speed: 3.3ms preprocess, 80.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 1 truck, 95.2ms\n",
      "Speed: 4.9ms preprocess, 95.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 73.3ms\n",
      "Speed: 4.2ms preprocess, 73.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 2 buss, 1 truck, 80.4ms\n",
      "Speed: 4.8ms preprocess, 80.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 69.8ms\n",
      "Speed: 3.5ms preprocess, 69.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 1 truck, 101.4ms\n",
      "Speed: 4.5ms preprocess, 101.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 78.8ms\n",
      "Speed: 3.1ms preprocess, 78.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 93.0ms\n",
      "Speed: 4.8ms preprocess, 93.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 71.0ms\n",
      "Speed: 2.7ms preprocess, 71.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 2 trucks, 95.0ms\n",
      "Speed: 4.7ms preprocess, 95.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 71.5ms\n",
      "Speed: 2.9ms preprocess, 71.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 1 truck, 113.2ms\n",
      "Speed: 5.3ms preprocess, 113.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.6ms\n",
      "Speed: 2.4ms preprocess, 83.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 2 trucks, 101.8ms\n",
      "Speed: 6.1ms preprocess, 101.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 80.5ms\n",
      "Speed: 3.1ms preprocess, 80.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 2 buss, 2 trucks, 116.1ms\n",
      "Speed: 4.0ms preprocess, 116.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 91.6ms\n",
      "Speed: 3.5ms preprocess, 91.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 1 bus, 2 trucks, 99.9ms\n",
      "Speed: 3.3ms preprocess, 99.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 84.6ms\n",
      "Speed: 3.2ms preprocess, 84.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 3 trucks, 95.3ms\n",
      "Speed: 4.1ms preprocess, 95.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 69.2ms\n",
      "Speed: 2.4ms preprocess, 69.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 1 bus, 3 trucks, 104.5ms\n",
      "Speed: 5.4ms preprocess, 104.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 67.9ms\n",
      "Speed: 3.3ms preprocess, 67.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 cars, 2 buss, 2 trucks, 106.3ms\n",
      "Speed: 4.0ms preprocess, 106.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 76.6ms\n",
      "Speed: 2.4ms preprocess, 76.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 115.5ms\n",
      "Speed: 5.2ms preprocess, 115.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 80.5ms\n",
      "Speed: 3.2ms preprocess, 80.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 cars, 2 buss, 2 trucks, 114.0ms\n",
      "Speed: 5.7ms preprocess, 114.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 67.2ms\n",
      "Speed: 3.3ms preprocess, 67.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 cars, 2 buss, 1 truck, 186.3ms\n",
      "Speed: 22.2ms preprocess, 186.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 68.9ms\n",
      "Speed: 2.8ms preprocess, 68.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 2 buss, 1 truck, 111.3ms\n",
      "Speed: 3.5ms preprocess, 111.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 70.1ms\n",
      "Speed: 2.3ms preprocess, 70.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 1 bus, 3 trucks, 103.6ms\n",
      "Speed: 4.0ms preprocess, 103.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 79.1ms\n",
      "Speed: 3.1ms preprocess, 79.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 3 trucks, 108.7ms\n",
      "Speed: 9.3ms preprocess, 108.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 96.8ms\n",
      "Speed: 3.6ms preprocess, 96.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 cars, 1 bus, 2 trucks, 100.7ms\n",
      "Speed: 6.5ms preprocess, 100.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 license_plates, 82.8ms\n",
      "Speed: 2.2ms preprocess, 82.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 cars, 1 bus, 2 trucks, 102.3ms\n",
      "Speed: 4.8ms preprocess, 102.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.5ms\n",
      "Speed: 3.1ms preprocess, 83.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 cars, 1 bus, 2 trucks, 107.4ms\n",
      "Speed: 4.4ms preprocess, 107.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 83.0ms\n",
      "Speed: 2.6ms preprocess, 83.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 cars, 1 bus, 1 truck, 98.1ms\n",
      "Speed: 4.3ms preprocess, 98.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 license_plate, 82.6ms\n",
      "Speed: 3.7ms preprocess, 82.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to draw bounding boxes and text\n",
    "def draw_annotations(frame, annotations, color):\n",
    "    for (x1, y1, x2, y2, text) in annotations:\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        if text:\n",
    "            # Draw a filled rectangle as background for text\n",
    "            (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)\n",
    "            cv2.rectangle(frame, (int(x1), int(y1) - 30), (int(x1) + w, int(y1)), (0, 0, 0), -1)\n",
    "            # Put the text on top of the background\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "# Function to preprocess image for OCR\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    enhanced = cv2.equalizeHist(gray)\n",
    "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "    _, binary = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    binary = cv2.dilate(binary, kernel, iterations=1)\n",
    "    binary = cv2.erode(binary, kernel, iterations=1)\n",
    "    return binary\n",
    "\n",
    "# Specify the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# Load models\n",
    "car_detector = YOLO('yolov8n.pt')\n",
    "license_plate_detector = YOLO('license_plate_detector.pt')\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('car.mp4')\n",
    "\n",
    "# Define the list of vehicle class IDs (as per your model's class mapping)\n",
    "vehicles = [2, 3, 5, 7]\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# Read frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # Process every nth frame to reduce computation\n",
    "    if frame_count % 5 != 0:\n",
    "        continue\n",
    "\n",
    "    # Detect vehicles\n",
    "    car_detections = car_detector(frame)[0]\n",
    "    car_annotations = []\n",
    "    for detection in car_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = detection\n",
    "        if int(class_id) in vehicles:\n",
    "            car_annotations.append((x1, y1, x2, y2, f'Car: {int(score * 100)}%'))\n",
    "\n",
    "    # Detect license plates\n",
    "    license_plate_detections = license_plate_detector(frame)[0]\n",
    "    license_plate_annotations = []\n",
    "    for license_plate in license_plate_detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = license_plate\n",
    "        license_plate_crop = frame[int(y1):int(y2), int(x1): int(x2), :]\n",
    "        processed_image = preprocess_image(license_plate_crop)\n",
    "        license_plate_text = pytesseract.image_to_string(processed_image, config='--psm 8').strip()\n",
    "        license_plate_annotations.append((x1, y1, x2, y2, license_plate_text))\n",
    "\n",
    "    draw_annotations(frame, car_annotations, (0, 0, 255))  # Red rectangles for cars\n",
    "    draw_annotations(frame, license_plate_annotations, (255, 0, 0))  # Blue rectangles for license plates\n",
    "    \n",
    "    frame_resized = cv2.resize(frame, (800, 450))\n",
    "    \n",
    "    cv2.imshow('Frame', frame_resized)\n",
    "\n",
    "    # Show OCR\n",
    "    cv2.imshow('License Plate Detection', processed_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
