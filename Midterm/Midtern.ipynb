{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### %%writefile test\n",
    "## Midterm\n",
    "<b>The objective of the first part of the midterm is to detect blue cursor, yellow timer, and human skin from input video. Your output should be similar to midterm_demo.mp4. Please complete steps 1-8 in one single code cell. The whole process can be divided to the following steps:</b>\n",
    "1. (5pts) Input images from video file WiiPlay.mp4 with the same level number as the last two digits of your student id, and show the images in the \"input\" window.\n",
    "2. (5pts) Use <i>cv2.cvtColor()</i> to convert images from BGR to HSV format.\n",
    "3. (5pts) Use <i>cv2.createTrackbar()</i> to create six trackbars (HueMin, HueMax, SatMin SatMax, ValMin, ValMax), and use <i>cv2.getTrackbarPos()</i> to get the current value of each trackbar.\n",
    "4. (5pts) Use <i>cv2.threshold()</i> or <i>cv2.inRange()</i> to apply double thresholding to each channel (Hue, Sat, Val) based on current values of the six trackbars\n",
    "5. (5pts) Apply morphological filters to remove noise (outliers & holes), and show the detected regions in the \"test\" window..\n",
    "6. (10pts) Find out the best color range to detect <b>blue cursor</b>, apply these thresholds, and show the detected regions in the \"cursor\" window.\n",
    "7. (10pts) Find out the best color range to detect <b>yellow timer</b>, apply these thresholds, and show the detected regions in the \"timer\" window.\n",
    "8. (10pts) Find out the best color range to detect <b>human skin</b>, apply these thresholds, and show the detected regions in the \"skin\" window.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open the video file\")\n",
    "\n",
    "frame_start = 150\n",
    "frame_end = 437\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "cv2.namedWindow('input')\n",
    "cv2.namedWindow('test')\n",
    "cv2.namedWindow('cursor')\n",
    "cv2.namedWindow('timer')\n",
    "cv2.namedWindow('skin')\n",
    "\n",
    "# 3. Use cv2.createTrackbar() to create six trackbars\n",
    "cv2.createTrackbar('HueMin', 'test', 0, 30, nothing)\n",
    "cv2.createTrackbar('HueMax', 'test', 0, 150, nothing)\n",
    "cv2.createTrackbar('SatMin', 'test', 0, 10, nothing)\n",
    "cv2.createTrackbar('SatMax', 'test', 0, 120, nothing)\n",
    "cv2.createTrackbar('ValMin', 'test', 0, 50, nothing)\n",
    "cv2.createTrackbar('ValMax', 'test', 0, 200, nothing)\n",
    "\n",
    "cv2.createTrackbar('BlueHueMin', 'cursor', 100, 180, nothing)\n",
    "cv2.createTrackbar('BlueHueMax', 'cursor', 120, 180, nothing)\n",
    "cv2.createTrackbar('BlueSatMin', 'cursor', 100, 255, nothing)\n",
    "cv2.createTrackbar('BlueSatMax', 'cursor', 255, 255, nothing)\n",
    "cv2.createTrackbar('BlueValMin', 'cursor', 100, 255, nothing)\n",
    "cv2.createTrackbar('BlueValMax', 'cursor', 255, 255, nothing)\n",
    "\n",
    "cv2.createTrackbar('YellowHueMin', 'timer', 20, 30, nothing)\n",
    "cv2.createTrackbar('YellowHueMax', 'timer', 30, 30, nothing)\n",
    "cv2.createTrackbar('YellowSatMin', 'timer', 100, 255, nothing)\n",
    "cv2.createTrackbar('YellowSatMax', 'timer', 255, 255, nothing)\n",
    "cv2.createTrackbar('YellowValMin', 'timer', 100, 255, nothing)\n",
    "cv2.createTrackbar('YellowValMax', 'timer', 255, 255, nothing)\n",
    "\n",
    "cv2.createTrackbar('SkinHueMin', 'skin', 0, 30, nothing)\n",
    "cv2.createTrackbar('SkinHueMax', 'skin', 20, 30, nothing)\n",
    "cv2.createTrackbar('SkinSatMin', 'skin', 40, 255, nothing)\n",
    "cv2.createTrackbar('SkinSatMax', 'skin', 255, 255, nothing)\n",
    "cv2.createTrackbar('SkinValMin', 'skin', 50, 255, nothing)\n",
    "cv2.createTrackbar('SkinValMax', 'skin', 255, 255, nothing)\n",
    "\n",
    "current_frame = frame_start\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Reached end of video\")\n",
    "        break\n",
    "    \n",
    "    \"\"\" \n",
    "    1.Input images from video file WiiPlay.mp4 \n",
    "    with the same level number as the last two digits of your student id, \n",
    "    and show the images in the \"input\" window\n",
    "    \"\"\"\n",
    "    cv2.imshow('input', frame)\n",
    "\n",
    "    # 2. Use cv2.cvtColor() to convert images from BGR to HSV format.\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # 3. use cv2.getTrackbarPos() to get the current value of each trackbar.\n",
    "    hmin = cv2.getTrackbarPos('HueMin', 'test')\n",
    "    hmax = cv2.getTrackbarPos('HueMax', 'test')\n",
    "    smin = cv2.getTrackbarPos('SatMin', 'test')\n",
    "    smax = cv2.getTrackbarPos('SatMax', 'test')\n",
    "    vmin = cv2.getTrackbarPos('ValMin', 'test')\n",
    "    vmax = cv2.getTrackbarPos('ValMax', 'test')\n",
    "\n",
    "    blue_h_min = cv2.getTrackbarPos('BlueHueMin', 'cursor')\n",
    "    blue_h_max = cv2.getTrackbarPos('BlueHueMax', 'cursor')\n",
    "    blue_s_min = cv2.getTrackbarPos('BlueSatMin', 'cursor')\n",
    "    blue_s_max = cv2.getTrackbarPos('BlueSatMax', 'cursor')\n",
    "    blue_v_min = cv2.getTrackbarPos('BlueValMin', 'cursor')\n",
    "    blue_v_max = cv2.getTrackbarPos('BlueValMax', 'cursor')\n",
    "\n",
    "    # Get current trackbar positions for yellow timer detection\n",
    "    yellow_h_min = cv2.getTrackbarPos('YellowHueMin', 'timer')\n",
    "    yellow_h_max = cv2.getTrackbarPos('YellowHueMax', 'timer')\n",
    "    yellow_s_min = cv2.getTrackbarPos('YellowSatMin', 'timer')\n",
    "    yellow_s_max = cv2.getTrackbarPos('YellowSatMax', 'timer')\n",
    "    yellow_v_min = cv2.getTrackbarPos('YellowValMin', 'timer')\n",
    "    yellow_v_max = cv2.getTrackbarPos('YellowValMax', 'timer')\n",
    "\n",
    "    skin_h_min = cv2.getTrackbarPos('SkinHueMin', 'skin')\n",
    "    skin_h_max = cv2.getTrackbarPos('SkinHueMax', 'skin')\n",
    "    skin_s_min = cv2.getTrackbarPos('SkinSatMin', 'skin')\n",
    "    skin_s_max = cv2.getTrackbarPos('SkinSatMax', 'skin')\n",
    "    skin_v_min = cv2.getTrackbarPos('SkinValMin', 'skin')\n",
    "    skin_v_max = cv2.getTrackbarPos('SkinValMax', 'skin')\n",
    "\n",
    "\n",
    "    lower = np.array([hmin, smin, vmin])\n",
    "    upper = np.array([hmax, smax, vmax])\n",
    "    mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    lower_yellow = np.array([yellow_h_min, yellow_s_min, yellow_v_min])\n",
    "    upper_yellow = np.array([yellow_h_max, yellow_s_max, yellow_v_max])\n",
    "    mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "    # Define lower and upper bounds for blue cursor detection\n",
    "    lower_blue = np.array([blue_h_min, blue_s_min, blue_v_min])\n",
    "    upper_blue = np.array([blue_h_max, blue_s_max, blue_v_max])\n",
    "\n",
    "    # Threshold the HSV image to get only blue cursor\n",
    "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    # Define lower and upper bounds for skin detection\n",
    "    lower_skin = np.array([skin_h_min, skin_s_min, skin_v_min])\n",
    "    upper_skin = np.array([skin_h_max, skin_s_max, skin_v_max])\n",
    "\n",
    "    # Threshold the HSV image to get only skin\n",
    "    mask_skin = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    # Apply morphological operations to remove noise\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask_blue = cv2.erode(mask_blue, kernel, iterations=1)\n",
    "    mask_blue = cv2.dilate(mask_blue, kernel, iterations=1)\n",
    "\n",
    "    mask_yellow = cv2.erode(mask_yellow, kernel, iterations=1)\n",
    "    mask_yellow = cv2.dilate(mask_yellow, kernel, iterations=1)\n",
    "\n",
    "    mask_skin = cv2.erode(mask_skin, kernel, iterations=1)\n",
    "    mask_skin = cv2.dilate(mask_skin, kernel, iterations=1)\n",
    "\n",
    "    cv2.imshow('input', frame)\n",
    "    cv2.imshow('test', mask)\n",
    "    cv2.imshow('cursor', mask_blue)\n",
    "    cv2.imshow('timer', mask_yellow)\n",
    "    cv2.imshow('skin', mask_skin)\n",
    "    current_frame += 1\n",
    "\n",
    "    if current_frame > frame_end:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![input](input.png \"input\")\n",
    "![test](test.png \"test\")\n",
    "![cursor](cursor.png \"cursor\")\n",
    "![timer](timer.png \"timer\")\n",
    "![skin](skin.png \"skin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The objective of the second part of the midterm is to stitch images together. Your output should be similar to the following picture. The coordinates of four pairs of corresponding points between two consecutive images should be provided <b style=\"color:red;\">manually</b> (don't match automatically). Please complete steps 9-12 in one single code cell, steps 13-14 in another markdown cell, and upload your Jupyter notebook file (*.ipynb).</b>\n",
    "\n",
    "9. (5pts) Given a pair of images (pier_1.jpg, pier_2.jpg), use <i>cv2.getPerspectiveTransform()</i> to get Homography Matrix by specifing the coordinates of four pairs of corresponding points <b>manually</b>.\n",
    "10. (10pts) Use <i>cv2.warpPerspective()</i> and the Homography Matrix to get the projective view of the second image. You might need to make the output image larger enough to fit the projective image.\n",
    "11. (10pts) Use <i>cv2.add()</i> to stitch two images together.\n",
    "12. (10pts) Given a set of three images (pier_1.jpg, pier_2.jpg, pier_3.jpg) and their corresponding points, try to stitch them together.\n",
    "13. (5pts) Any comments regarding the midterm? Which steps you believe you have completed? Which steps bother you?\n",
    "14. (5pts) Any comments regarding the classes up to now? pace too fast or slow? quiz too hard or simple? prefer C or Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing Report\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This report documents the process and methodology used to combine multiple images into a single canvas using perspective transformation techniques. The primary goal is to align and overlay images to create seamless transitions between them, utilizing OpenCV's functions for image manipulation.\n",
    "\n",
    "## 2. Methodology\n",
    "\n",
    "### Step 1: Read Input Images\n",
    "\n",
    "First, the input images are loaded into the program using OpenCV's `imread` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"pier_1.jpg\")\n",
    "img2 = cv2.imread(\"pier_2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Correspondence Points and Compute Homography Matrix\n",
    "\n",
    "Points of correspondence between the images are defined to help in computing the homography matrix and use the defined points, a homography matrix is computed that maps points from the second image to the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homography Matrix:\n",
      "[[ 7.72322265e-01  7.98423275e-02  2.92863937e+02]\n",
      " [-1.16594800e-01  1.00061083e+00  3.53130368e+01]\n",
      " [-4.13952260e-04  1.54733380e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pts_img1 = np.array([[387, 118], [401, 118], [421, 210], [390, 148]], dtype=np.float32)\n",
    "pts_img2 = np.array([[99, 91], [113.5, 92], [132.5, 184.5], [101.5, 121]], dtype=np.float32)\n",
    "#pts_img3 = np.array([[ 6.0 , 105.0 ] ,[ 67.0 , 105.5 ] , [ 66.5 , 231.5 ] ,[ 8.0 , 231.0 ]], dtype=np.float32)\n",
    "\n",
    "Matrix = cv2.getPerspectiveTransform(pts_img2, pts_img1)\n",
    "\n",
    "print(\"Homography Matrix:\")\n",
    "print(Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Warp Second Image\n",
    "\n",
    "The second image is warped using the computed matrix, and its size is adjusted to fit the dimensions of the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_height, img1_width = img1.shape[:2]\n",
    "img2_height, img2_width = img2.shape[:2]\n",
    "\n",
    "combine = cv2.warpPerspective(img2, Matrix, (img2_height * 2, img2_width* 2))\n",
    "\n",
    "cv2.imshow(\"Combined\", combine)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Overlay Images\n",
    "\n",
    "The warped image is then overlayed onto a canvas, and blended with the first image to create a smooth transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_height, combine_width = combine.shape[:2]\n",
    "\n",
    "canvas1 = np.zeros((combine_height + 100, combine_width + 100, 3), dtype=np.uint8)\n",
    "canvas2 = np.zeros((combine_height + 100, combine_width + 100, 3), dtype=np.uint8)\n",
    "\n",
    "canvas1[100:combine_height + 100, :combine_width] = combine\n",
    "canvas2[100:img1_height + 100, :img1_width] = img1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Blending Images Using OpenCV\n",
    "\n",
    "The given code snippet is used to blend two images based on their non-zero pixel values to create a composite image. Here's a breakdown of each step within the code:\n",
    "\n",
    "#### 1: Initialize a Blank Canvas\n",
    "A blank canvas (`canvas1_2`) is initialized with dimensions slightly larger than the combined images to accommodate both without any cropping. This canvas will hold the final blended image.\n",
    "\n",
    "#### 2: Pixel-wise Blending Loop\n",
    "A nested loop iterates over every pixel position in the new canvas. The conditions inside the loop determine how pixels from (`canvas1`) and (`canvas2`) are blended based on their values:\n",
    "\n",
    "#### 3: Check and Blend Pixel Values\n",
    "The conditions within the loop check the values of corresponding pixels in both (`canvas1`) and (`canvas2`)\n",
    "\n",
    "(`1.Both Pixels Non-zero`)Both Pixels Non-zero: If both pixels are non-zero (i.e., they contain image data), their values are averaged. This averaging helps in merging overlapping parts of the images smoothly.\n",
    "\n",
    "```python\n",
    "if canvas1[i][j].all() != 0 and canvas2[i][j].all() != 0:\n",
    "    canvas1_2[i][j] = (canvas1[i][j] + canvas2[i][j]) / 2\n",
    "```\n",
    "\n",
    "(`2.Only First Canvas Non-zero:`) If only the pixel from canvas1 contains image data, it is directly copied to the blended canvas. This condition preserves parts of the first image that do not overlap with the second.\n",
    "\n",
    "```python\n",
    "elif canvas1[i][j].all() != 0:\n",
    "    canvas1_2[i][j] = canvas1[i][j]\n",
    "```\n",
    "\n",
    "(`3.Only Second Canvas Non-zero`)Similarly, if only the pixel from canvas2 is non-zero, it is copied to the blended canvas. This preserves parts of the second image in non-overlapping areas.\n",
    "\n",
    "```python\n",
    "elif canvas1[i][j].all() != 0:\n",
    "    canvas1_2[i][j] = canvas1[i][j]\n",
    "```\n",
    "\n",
    "(`4.Both Pixels Zero`)If both pixels are zero, the corresponding pixel in the blended canvas remains zero (black), representing empty space.\n",
    "\n",
    "```python\n",
    "else:\n",
    "    canvas1_2[i][j] = [0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas1_2 = np.zeros(( combine_height+ 100, combine_width + 100, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(combine_height + 100):\n",
    "\n",
    "    for j in range(combine_width + 100):\n",
    "\n",
    "        if canvas1[i][j].all() != 0 and canvas2[i][j].all() != 0:\n",
    "\n",
    "            canvas1_2[i][j] = (canvas1[i][j] + canvas2[i][j]) / 2\n",
    "\n",
    "        elif canvas1[i][j].all() != 0:\n",
    "\n",
    "            canvas1_2[i][j] = canvas1[i][j]\n",
    "\n",
    "        elif canvas2[i][j].all() != 0:\n",
    "\n",
    "            canvas1_2[i][j] = canvas2[i][j]\n",
    "\n",
    "        else:\n",
    "\n",
    "            canvas1_2[i][j] = [0, 0, 0]\n",
    "\n",
    "cv2.imwrite('pier_1&2.jpg', canvas1_2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv2.imread(\"pier_1&2.jpg\")\n",
    "img4 = cv2.imread(\"pier_3.jpg\")\n",
    "\n",
    "img3_height, img3_width = img3.shape[:2]\n",
    "img4_height, img4_width = img4.shape[:2]\n",
    "\n",
    "pts_img3 = np.float32([[681, 237], [679, 277], [651, 311], [775, 297]])\n",
    "pts_img4 = np.float32([[67, 110], [66, 146], [44, 176], [143, 167]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homography Matrix:\n",
      "[[ 1.22314346e+00  3.86178085e-02  5.98833400e+02]\n",
      " [-5.83784547e-02  1.14075979e+00  1.16830976e+02]\n",
      " [-5.54523051e-05  8.75996914e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Matrix2 = cv2.getPerspectiveTransform(pts_img4, pts_img3)\n",
    "print(\"Homography Matrix:\")\n",
    "print(Matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3_resized = cv2.resize(img3, (img3_width, img3_height))\n",
    "\n",
    "img3_height, img3_width = img3_resized.shape[:2]\n",
    "\n",
    "combine2 = cv2.warpPerspective(img4, Matrix2, (img4_height * 3, img4_width * 3))\n",
    "combine2_height, combine2_width = combine2.shape[:2]\n",
    "\n",
    "canvas3 = np.zeros((combine2_height + 100, combine2_width + 100, 3), dtype=np.uint8)\n",
    "canvas4 = np.zeros((combine2_height + 100, combine2_width + 100, 3), dtype=np.uint8)\n",
    "\n",
    "canvas3[100:combine2_height + 100, :combine2_width] = combine2\n",
    "canvas4[100:img3_height + 100, :img3_width] = img3_resized\n",
    "\n",
    "canvas3_4 = np.zeros((combine2_height + 100, combine2_width + 100, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(final_result_height + 100):\n",
    "    \n",
    "        for j in range(final_result_width + 100):\n",
    "    \n",
    "            if canvas3[i][j].all() != 0 and canvas4[i][j].all() != 0:\n",
    "    \n",
    "                canvas3_4[i][j] = (canvas3[i][j] + canvas4[i][j]) / 2\n",
    "    \n",
    "            elif canvas3[i][j].all() != 0:\n",
    "    \n",
    "                canvas3_4[i][j] = canvas3[i][j]\n",
    "    \n",
    "            elif canvas4[i][j].all() != 0:\n",
    "    \n",
    "                canvas3_4[i][j] = canvas4[i][j]\n",
    "    \n",
    "            else:\n",
    "    \n",
    "                canvas3_4[i][j] = [0, 0, 0]\n",
    "\n",
    "cv2.imwrite('Result.jpg', canvas3_4)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result](Result.jpg \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. (5pts) Any comments regarding the midterm? Which steps you believe you have completed? Which steps bother you?\n",
    "14. (5pts) Any comments regarding the classes up to now? pace too fast or slow? quiz too hard or simple? prefer C or Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. For me, I finish all step. In my opinion I think that deal with data type is very difficult in Computer Vision, especially  when I try to combine the two images after calculate Matrix, The result always failed, however \n",
    "\n",
    "14. For me, I think not fast nor slow in class, maybe in final exam we can join a competition as our final score in Computer Vision, I prefer Python to do anything in Computer Vision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
