{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "1. Input image from eos_map_x.png.\n",
    "2. Try to detect the location of the <b>player</b> (in cyan color), and the location of the nearest <b>shrine</b> (in red color). (hint: use <i>inRange()</i>)\n",
    "3. Try to detect the facing direction of the <b>player</b>. (hint: possibly use <i>morphologyEx()</i>, or <i>findContours(), minEnclosingTriangle()</i> to determine the player axis)\n",
    "4. Draw a yellow line indicating the facing direction of the <b>player</b>.\n",
    "5. Draw a yellow line from the <b>player</b> to the <b>shrine</b>.\n",
    "4. Compute the required <b>rotating angle</b> so the player is facing to the shrine. \n",
    "(positive angle means clockwise rotation, negative angle means counterclockwise rotaion) (hint: use <i>atan2()</i>)\n",
    "5. Print the rotating angle on top-left corner of the output images. (in degree, not radian)\n",
    "6. Write a simple report in a separate cell.\n",
    "7. Upload your Jupyter code file (*.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load an image using 'imread' specifying the path to image\n",
    "\n",
    "image = cv2.imread('eos_map_0.png')\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    print(\"Image loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/infor/miniconda3/envs/computervision/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Define the range of red and blue in HSV\n",
    "lower_red = np.array([0, 100, 100])\n",
    "upper_red = np.array([0, 255, 255])\n",
    "lower_blue = np.array([90, 100, 100])\n",
    "upper_blue  = np.array([120, 255, 255])\n",
    "\n",
    "# Convert the image to HSV\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "red_mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "\n",
    "# Apply the mask to the image\n",
    "red_masked_image = cv2.bitwise_and(image, image, mask=red_mask)\n",
    "blue_masked_image = cv2.bitwise_and(image, image, mask=blue_mask)\n",
    "\n",
    "cv2.imshow('HSV Picture', hsv)\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Red Mask', red_mask)\n",
    "cv2.imshow('Red Masked Image', red_masked_image)\n",
    "cv2.imshow('Blue Mask', blue_mask)\n",
    "cv2.imshow('Blue Masked Image', blue_masked_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Explanation\n",
    "\n",
    "This document provides an explanation of a code snippet used for image processing with the OpenCV library. The code is designed to isolate and analyze red and blue regions in an image by applying color filtering in the HSV color space.\n",
    "\n",
    "### Step-by-Step Code Breakdown\n",
    "\n",
    "1. **Define HSV Color Ranges for Red and Blue:**\n",
    "   - `lower_red` and `upper_red` set the HSV range for detecting red colors. Note that the range provided may not capture all shades of red accurately.\n",
    "   - `lower_blue` and `upper_blue` define the HSV range for blue colors, specifically capturing hues from 90 to 120.\n",
    "\n",
    "   ```python\n",
    "   lower_red = np.array([0, 100, 100])\n",
    "   upper_red = np.array([0, 255, 255])\n",
    "   lower_blue = np.array([90, 100, 100])\n",
    "   upper_blue  = np.array([120, 255, 255])\n",
    "\n",
    "2. **Convert the Image from BGR to HSV Color Space:**\n",
    "\n",
    "   - `cv2.cvtColor` set RGB transform to HSV for image.\n",
    "\n",
    "   ```python\n",
    "   hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "3. **Create Masks for Specific Color Ranges:**\n",
    "   - `cv2.inRange` checks every pixel to see if its HSV value falls within the specified range, marking it white if it does, and black otherwise. This method is efficient for segmenting images based on color.\n",
    "\n",
    "   ```python\n",
    "   red_mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "   blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "4. **Apply Masks to Isolate Color Regions:**\n",
    "   - `cv2.bitwise_and`are used to filter the original image, isolating areas based on color. This step is crucial for further processing, such as contour detection or region analysis\n",
    "\n",
    "   ```python\n",
    "   red_masked_image = cv2.bitwise_and(image, image, mask=red_mask)\n",
    "   blue_masked_image = cv2.bitwise_and(image, image, mask=blue_mask)\n",
    "\n",
    "5. **Display the Images:**\n",
    "   - `cv2.imshow` to show our result\n",
    "\n",
    "   ```python\n",
    "   cv2.imshow('HSV Picture', hsv)\n",
    "   cv2.imshow('Original Image', image)\n",
    "   cv2.imshow('Red Mask', red_mask)\n",
    "   cv2.imshow('Red Masked Image', red_masked_image)\n",
    "   cv2.imshow('Blue Mask', blue_mask)\n",
    "   cv2.imshow('Blue Masked Image', blue_masked_image)\n",
    "   cv2.waitKey()\n",
    "   cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5), np.uint8)\n",
    "red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)\n",
    "blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Transformations on Color Masks\n",
    "\n",
    "Morphological transformations are key operations in image processing that process images based on shapes. They apply a structuring element to an input image and generate an output image. In this code, we specifically use the \"closing\" operation, which is useful for closing small holes inside the foreground objects or small black points on the object.\n",
    "\n",
    "### Step-by-Step Code Breakdown\n",
    "\n",
    "1. **Creating a Structuring Element:**\n",
    "   - `np.ones((5,5), np.uint8)` creates a 5x5 matrix filled with ones. This matrix acts as a structuring element for the morphological operation. The size of the matrix can be adjusted depending on the specifics of the image and the desired effect.\n",
    "\n",
    "   ```python\n",
    "   kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "2. **Applying Morphological Closing:**\n",
    "    - `cv2.morphologyEx` is the function used to apply morphological transformations.\n",
    "    -`cv2.MORPH_CLOSE` parameter specifies the type of operation—closing.\n",
    "\n",
    "    ```python\n",
    "    red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kerne)\n",
    "\n",
    "\n",
    "### Purpose and Benefit\n",
    "\n",
    "1. **Enhance Mask Quality:** Closing helps in removing small holes within detected objects in the mask, which improves the mask's quality and ensures better detection and segmentation of the desired colors.\n",
    "\n",
    "2. **Noise Reduction:** This operation also reduces noise within the masks. It helps in eliminating small black points that can be considered as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point 1: (83, 201), Area: 123.0\n",
      "Center point 2: (117, 86), Area: 129.5\n",
      "Max Red Area: 129.5\n"
     ]
    }
   ],
   "source": [
    "red_contours, _ = cv2.findContours(red_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "red_centers = []\n",
    "red_areas = []\n",
    "\n",
    "threshold_area = 100\n",
    "\n",
    "for contour in red_contours:\n",
    "\n",
    "    area = cv2.contourArea(contour)\n",
    "\n",
    "    if area > threshold_area:\n",
    "\n",
    "        moments = cv2.moments(contour)\n",
    "        if moments[\"m00\"] != 0:\n",
    "            cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "            cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "            red_centers.append((cx, cy))  \n",
    "            red_areas.append(area)       \n",
    "max_red_area = max(red_areas)\n",
    "\n",
    "for i, center in enumerate(red_centers):\n",
    "    print(f\"Center point {i+1}: {center}, Area: {red_areas[i]}\")\n",
    "\n",
    "print(\"Max Red Area:\", max_red_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point 1: (114, 115), Area: 41.0\n",
      "Max Blue Area: 41.0\n"
     ]
    }
   ],
   "source": [
    "blue_contours, _ = cv2.findContours(blue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "blue_centers      = []  \n",
    "blue_areas        = []\n",
    "\n",
    "blue_threshold_area = 40\n",
    "\n",
    "for contour in blue_contours:\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area > blue_threshold_area:\n",
    "       \n",
    "        moments = cv2.moments(contour)\n",
    "        if moments[\"m00\"] != 0:\n",
    "            cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "            cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "            blue_centers.append((cx, cy))\n",
    "            blue_areas.append(area)\n",
    "\n",
    "max_blue_area = max(blue_areas)\n",
    "\n",
    "for i, center in enumerate(blue_centers):\n",
    "    print(f\"Center point {i+1}: {center}, Area: {blue_areas[i]}\")\n",
    "\n",
    "print(\"Max Blue Area:\", max_blue_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if red_centers and blue_centers:\n",
    "    \n",
    "    min_dist = float('inf')\n",
    "    closest_red  = None\n",
    "    closest_blue = None\n",
    "    \n",
    "    for red_center in red_centers:\n",
    "        for blue_center in blue_centers:\n",
    "            dist = np.linalg.norm(np.array(blue_center) - np.array(red_center)) # 计算向量长度\n",
    "        \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_red  = red_center \n",
    "                closest_blue = blue_center \n",
    "\n",
    "    closest_red  = tuple(closest_red)\n",
    "    closest_blue = tuple(closest_blue)\n",
    "\n",
    "    img_with_line = cv2.line(image.copy(), closest_red, closest_blue, (0, 255, 255),2)\n",
    "\n",
    "    cv2.imshow('Image with Line', img_with_line)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1 * cv2.arcLength(blue_contours[0], True)\n",
    "approx = cv2.approxPolyDP(blue_contours[0], epsilon, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull = cv2.convexHull(approx)\n",
    "hull_points = [tuple(point[0]) for point in hull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_length_ab =  ((hull_points[0][0] - hull_points[1][0])** 2 + (hull_points[0][1] - hull_points[1][1])** 2 ) ** 0.5\n",
    "side_length_ac =  ((hull_points[0][0] - hull_points[2][0])** 2 + (hull_points[0][1] - hull_points[2][1])** 2 ) ** 0.5\n",
    "side_length_bc =  ((hull_points[1][0] - hull_points[2][0])** 2 + (hull_points[1][1] - hull_points[2][1])** 2 ) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_side_length = max(side_length_ab + side_length_ac , side_length_bc + side_length_ac, side_length_ab + side_length_bc)\n",
    "\n",
    "if max_side_length == side_length_ab + side_length_ac: #a\n",
    "    intersection = hull_points[0]\n",
    "    \n",
    "if max_side_length == side_length_ab + side_length_bc: #b\n",
    "    intersection = hull_points[1]\n",
    "    \n",
    "if max_side_length == side_length_ac + side_length_bc: #c\n",
    "    intersection = hull_points[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection point coordinates: (119, 111)\n",
      "Blue center coordinates: (114, 115)\n",
      "red center coordinates: (117, 86)\n",
      "夾角: -45.43405063213941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 69, 147, 175],\n",
       "        [ 74, 149, 178],\n",
       "        [ 82, 148, 184],\n",
       "        ...,\n",
       "        [ 77, 106, 134],\n",
       "        [ 77, 106, 134],\n",
       "        [ 70, 101, 128]],\n",
       "\n",
       "       [[ 72, 148, 177],\n",
       "        [ 74, 149, 178],\n",
       "        [ 82, 149, 187],\n",
       "        ...,\n",
       "        [ 77, 109, 136],\n",
       "        [ 77, 109, 136],\n",
       "        [ 77, 119, 149]],\n",
       "\n",
       "       [[ 82, 151, 184],\n",
       "        [ 82, 151, 184],\n",
       "        [ 82, 150, 189],\n",
       "        ...,\n",
       "        [ 78, 109, 138],\n",
       "        [ 82, 124, 155],\n",
       "        [ 82, 130, 161]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 99, 158, 206],\n",
       "        [ 99, 158, 206],\n",
       "        [ 87, 159, 204],\n",
       "        ...,\n",
       "        [ 66, 127, 161],\n",
       "        [ 66, 116, 152],\n",
       "        [ 73, 130, 164]],\n",
       "\n",
       "       [[ 90, 157, 206],\n",
       "        [ 93, 161, 206],\n",
       "        [ 82, 155, 194],\n",
       "        ...,\n",
       "        [ 71, 132, 172],\n",
       "        [ 67, 117, 160],\n",
       "        [ 71, 126, 169]],\n",
       "\n",
       "       [[ 91, 154, 192],\n",
       "        [ 87, 150, 192],\n",
       "        [ 77, 144, 181],\n",
       "        ...,\n",
       "        [ 63, 115, 155],\n",
       "        [ 63, 116, 155],\n",
       "        [ 67, 119, 159]]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_to_intersection_vector = np.array([intersection[0] - cx, intersection[1] - cy])\n",
    "\n",
    "extended_point = (int(cx + 3 * centroid_to_intersection_vector[0]), int(cy + 3 * centroid_to_intersection_vector[1]))\n",
    "\n",
    "\n",
    "img_with_yellow_lines = cv2.line(image.copy(), (cx, cy), intersection, (0, 255, 255), 2)\n",
    "\n",
    "\n",
    "img_with_line = cv2.line(img_with_line, intersection, extended_point, (0, 255, 255), 2)\n",
    "img_with_line = cv2.line(img_with_line, (cx, cy), intersection, (0, 255, 255), 2)\n",
    "\n",
    "cv2.circle(img_with_yellow_lines, intersection, 2, (0, 255, 0), -1)\n",
    "cv2.circle(img_with_yellow_lines, blue_center, 2, (255, 0, 0), -1)\n",
    "\n",
    "for red_center in red_centers:\n",
    "    cv2.circle(img_with_yellow_lines, red_center, 2, (0, 0, 255), -1)\n",
    "\n",
    "print(\"Intersection point coordinates:\", intersection)\n",
    "print(\"Blue center coordinates:\", blue_center)\n",
    "print(\"red center coordinates:\", red_center)\n",
    "\n",
    "vec_blue = np.array(intersection) - np.array(blue_center)\n",
    "vec_red  = np.array(red_center)   - np.array(blue_center)\n",
    "\n",
    "angle_between = np.degrees(np.arctan2(vec_red[1], vec_red[0]) - np.arctan2(vec_blue[1], vec_blue[0]))\n",
    "\n",
    "if angle_between > 180:\n",
    "    angle_between -= 360\n",
    "elif angle_between < -180:\n",
    "    angle_between += 360\n",
    "\n",
    "\n",
    "print(\"Angle:\", angle_between)\n",
    "\n",
    "cv2.putText(img_with_line, f'{angle_between:.0f}', (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_line = cv2.resize(img_with_line, None, fx=3, fy=3)\n",
    "cv2.imshow('Image with Line', img_with_line)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
