{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "1. Input image from eos_map_x.png.\n",
    "2. Try to detect the location of the <b>player</b> (in cyan color), and the location of the nearest <b>shrine</b> (in red color). (hint: use <i>inRange()</i>)\n",
    "3. Try to detect the facing direction of the <b>player</b>. (hint: possibly use <i>morphologyEx()</i>, or <i>findContours(), minEnclosingTriangle()</i> to determine the player axis)\n",
    "4. Draw a yellow line indicating the facing direction of the <b>player</b>.\n",
    "5. Draw a yellow line from the <b>player</b> to the <b>shrine</b>.\n",
    "4. Compute the required <b>rotating angle</b> so the player is facing to the shrine. \n",
    "(positive angle means clockwise rotation, negative angle means counterclockwise rotaion) (hint: use <i>atan2()</i>)\n",
    "5. Print the rotating angle on top-left corner of the output images. (in degree, not radian)\n",
    "6. Write a simple report in a separate cell.\n",
    "7. Upload your Jupyter code file (*.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load an image using 'imread' specifying the path to image\n",
    "\n",
    "image = cv2.imread('eos_map_0.png')\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    print(\"Image loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/infor/miniconda3/envs/computervision/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Define the range of red and blue in HSV\n",
    "lower_red = np.array([0, 100, 100])\n",
    "upper_red = np.array([0, 255, 255])\n",
    "lower_blue = np.array([90, 100, 100])\n",
    "upper_blue  = np.array([120, 255, 255])\n",
    "\n",
    "# Convert the image to HSV\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "red_mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "\n",
    "# Apply the mask to the image\n",
    "red_masked_image = cv2.bitwise_and(image, image, mask=red_mask)\n",
    "blue_masked_image = cv2.bitwise_and(image, image, mask=blue_mask)\n",
    "\n",
    "cv2.imshow('HSV Picture', hsv)\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Red Mask', red_mask)\n",
    "cv2.imshow('Red Masked Image', red_masked_image)\n",
    "cv2.imshow('Blue Mask', blue_mask)\n",
    "cv2.imshow('Blue Masked Image', blue_masked_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Explanation\n",
    "\n",
    "This document provides an explanation of a code snippet used for image processing with the OpenCV library. The code is designed to isolate and analyze red and blue regions in an image by applying color filtering in the HSV color space.\n",
    "\n",
    "### Step-by-Step Code Breakdown\n",
    "\n",
    "1. **Define HSV Color Ranges for Red and Blue:**\n",
    "   - `lower_red` and `upper_red` set the HSV range for detecting red colors. Note that the range provided may not capture all shades of red accurately.\n",
    "   - `lower_blue` and `upper_blue` define the HSV range for blue colors, specifically capturing hues from 90 to 120.\n",
    "\n",
    "   ```python\n",
    "   lower_red = np.array([0, 100, 100])\n",
    "   upper_red = np.array([0, 255, 255])\n",
    "   lower_blue = np.array([90, 100, 100])\n",
    "   upper_blue  = np.array([120, 255, 255])\n",
    "\n",
    "2. **Convert the Image from BGR to HSV Color Space:**\n",
    "\n",
    "   - `cv2.cvtColor` set RGB transform to HSV for image.\n",
    "\n",
    "   ```python\n",
    "   hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "3. **Create Masks for Specific Color Ranges:**\n",
    "   - `cv2.inRange` checks every pixel to see if its HSV value falls within the specified range, marking it white if it does, and black otherwise. This method is efficient for segmenting images based on color.\n",
    "\n",
    "   ```python\n",
    "   red_mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "   blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "4. **Apply Masks to Isolate Color Regions:**\n",
    "   - `cv2.bitwise_and`are used to filter the original image, isolating areas based on color. This step is crucial for further processing, such as contour detection or region analysis\n",
    "\n",
    "   ```python\n",
    "   red_masked_image = cv2.bitwise_and(image, image, mask=red_mask)\n",
    "   blue_masked_image = cv2.bitwise_and(image, image, mask=blue_mask)\n",
    "\n",
    "5. **Display the Images:**\n",
    "   - `cv2.imshow` to show our result\n",
    "\n",
    "   ```python\n",
    "   cv2.imshow('HSV Picture', hsv)\n",
    "   cv2.imshow('Original Image', image)\n",
    "   cv2.imshow('Red Mask', red_mask)\n",
    "   cv2.imshow('Red Masked Image', red_masked_image)\n",
    "   cv2.imshow('Blue Mask', blue_mask)\n",
    "   cv2.imshow('Blue Masked Image', blue_masked_image)\n",
    "   cv2.waitKey()\n",
    "   cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5), np.uint8)\n",
    "red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)\n",
    "blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Transformations on Color Masks\n",
    "\n",
    "Morphological transformations are key operations in image processing that process images based on shapes. They apply a structuring element to an input image and generate an output image. In this code, we specifically use the \"closing\" operation, which is useful for closing small holes inside the foreground objects or small black points on the object.\n",
    "\n",
    "### Step-by-Step Code Breakdown\n",
    "\n",
    "1. **Creating a Structuring Element:**\n",
    "   - `np.ones((5,5), np.uint8)` creates a 5x5 matrix filled with ones. This matrix acts as a structuring element for the morphological operation. The size of the matrix can be adjusted depending on the specifics of the image and the desired effect.\n",
    "\n",
    "   ```python\n",
    "   kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "2. **Applying Morphological Closing:**\n",
    "    - `cv2.morphologyEx` is the function used to apply morphological transformations.\n",
    "    -`cv2.MORPH_CLOSE` parameter specifies the type of operation—closing.\n",
    "\n",
    "    ```python\n",
    "    red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kerne)\n",
    "\n",
    "\n",
    "### Purpose and Benefit\n",
    "\n",
    "1. **Enhance Mask Quality:** Closing helps in removing small holes within detected objects in the mask, which improves the mask's quality and ensures better detection and segmentation of the desired colors.\n",
    "\n",
    "2. **Noise Reduction:** This operation also reduces noise within the masks. It helps in eliminating small black points that can be considered as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point 1: (83, 201), Area: 123.0\n",
      "Center point 2: (117, 86), Area: 129.5\n",
      "Max Red Area: 129.5\n"
     ]
    }
   ],
   "source": [
    "red_contours, _ = cv2.findContours(red_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "red_centers = []\n",
    "red_areas = []\n",
    "\n",
    "threshold_area = 100\n",
    "\n",
    "for contour in red_contours:\n",
    "\n",
    "    area = cv2.contourArea(contour)\n",
    "\n",
    "    if area > threshold_area:\n",
    "\n",
    "        moments = cv2.moments(contour)\n",
    "        if moments[\"m00\"] != 0:\n",
    "            cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "            cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "            red_centers.append((cx, cy))  \n",
    "            red_areas.append(area)       \n",
    "max_red_area = max(red_areas)\n",
    "\n",
    "for i, center in enumerate(red_centers):\n",
    "    print(f\"Center point {i+1}: {center}, Area: {red_areas[i]}\")\n",
    "\n",
    "print(\"Max Red Area:\", max_red_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode for Processing Red Contours\n",
    "\n",
    "1. **Extract Contours from Image**\n",
    "   - Use `findContours` on `red_mask` with the `RETR_TREE` method and `CHAIN_APPROX_SIMPLE` approximation to get `red_contours`.\n",
    "\n",
    "2. **Initialize Lists for Centers and Areas**\n",
    "   - Create empty lists: `red_centers` and `red_areas`.\n",
    "\n",
    "3. **Set Minimum Area Threshold**\n",
    "   - Define `threshold_area` as 100.\n",
    "\n",
    "4. **Process Each Contour**\n",
    "   - For each `contour` in `red_contours`:\n",
    "     - Calculate the area using `contourArea`.\n",
    "     - If the area is greater than `threshold_area`:\n",
    "       - Compute the contour's moments using `moments`.\n",
    "       - Ensure the zeroth moment (`m00`) is not zero to avoid division by zero:\n",
    "         - Calculate centroid `(cx, cy)` using the ratio of the first moments (`m10` and `m01`) to `m00`.\n",
    "         - Append the centroid `(cx, cy)` to `red_centers`.\n",
    "         - Append the contour area to `red_areas`.\n",
    "\n",
    "5. **Determine the Largest Area**\n",
    "   - Calculate the maximum area in `red_areas` as `max_red_area`.\n",
    "\n",
    "6. **Output Information for Each Significant Contour**\n",
    "   - Iterate over `red_centers` and corresponding `red_areas`:\n",
    "     - Print the center point and its area for each significant contour.\n",
    "\n",
    "7. **Print Maximum Red Area**\n",
    "   - Output the largest area, `max_red_area`.\n",
    "\n",
    "\n",
    "## Reference\n",
    "This is reference by ChatGPT\n",
    "\n",
    "Origin code\n",
    "\n",
    "```python\n",
    "def find_centers_and_areas(contours, min_area_threshold):\n",
    "    centers = []\n",
    "    areas = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        areas.append(area)\n",
    "        if area > min_area_threshold:\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                centers.append((cX, cY))\n",
    "    return centers, areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point 1: (114, 115), Area: 41.0\n",
      "Max Blue Area: 41.0\n"
     ]
    }
   ],
   "source": [
    "blue_contours, _ = cv2.findContours(blue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "blue_centers      = []\n",
    "blue_areas        = []\n",
    "\n",
    "blue_threshold_area = 30\n",
    "\n",
    "for contour in blue_contours:\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area > blue_threshold_area:\n",
    "       \n",
    "        moments = cv2.moments(contour)\n",
    "        if moments[\"m00\"] != 0:\n",
    "            cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "            cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "            blue_centers.append((cx, cy))\n",
    "            blue_areas.append(area)\n",
    "\n",
    "max_blue_area = max(blue_areas)\n",
    "\n",
    "for i, center in enumerate(blue_centers):\n",
    "    print(f\"Center point {i+1}: {center}, Area: {blue_areas[i]}\")\n",
    "\n",
    "print(\"Max Blue Area:\", max_blue_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Contours for Color Detection\n",
    "\n",
    "Contour detection is a crucial step in image processing that involves finding continuous lines or curves that bound or cover the full boundary of objects in an image. The following code snippet demonstrates how contours are found and analyzed in both red and blue color masks obtained from earlier steps.\n",
    "\n",
    "### Pseudocode for Processing Blue Contours\n",
    "\n",
    "1. **Initialize Contours and Lists**\n",
    "   - Use `findContours` to extract contours from `blue_mask` with `RETR_EXTERNAL` and `CHAIN_APPROX_SIMPLE`.\n",
    "   - Initialize empty lists: `blue_centers` and `blue_areas`.\n",
    "\n",
    "2. **Set Threshold for Contour Areas**\n",
    "   - Define `blue_threshold_area` as 40.\n",
    "\n",
    "3. **Loop Through Each Contour**\n",
    "   - For each `contour` in `blue_contours`:\n",
    "     - Calculate the area of the contour using `contourArea`.\n",
    "     - If the area is greater than `blue_threshold_area`:\n",
    "       - Compute the moments of the contour using `moments`.\n",
    "       - Check if the zeroth moment (`m00`) is not zero (to avoid division by zero):\n",
    "         - Calculate centroid coordinates `(cx, cy)` using the first moments (`m10` and `m01`) divided by `m00`.\n",
    "         - Append `(cx, cy)` to `blue_centers`.\n",
    "         - Append the area to `blue_areas`.\n",
    "\n",
    "4. **Find Maximum Area**\n",
    "   - Determine the maximum area from `blue_areas`.\n",
    "\n",
    "5. **Output Centers and Areas**\n",
    "   - Loop through each center in `blue_centers`:\n",
    "     - Print the center point and its corresponding area.\n",
    "\n",
    "6. **Print Maximum Blue Area**\n",
    "   - Print the largest area from the list `blue_areas`.\n",
    "\n",
    "## Reference\n",
    "This is reference by ChatGPT\n",
    "\n",
    "Origin code\n",
    "\n",
    "```python\n",
    "def find_centers_and_areas(contours, min_area_threshold):\n",
    "    centers = []\n",
    "    areas = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        areas.append(area)\n",
    "        if area > min_area_threshold:\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                centers.append((cX, cY))\n",
    "    return centers, areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if red_centers and blue_centers:\n",
    "    \n",
    "    min_dist = float('inf')\n",
    "    closest_red  = None\n",
    "    closest_blue = None\n",
    "    \n",
    "    for red_center in red_centers:\n",
    "        for blue_center in blue_centers:\n",
    "            dist = np.linalg.norm(np.array(blue_center) - np.array(red_center)) # 计算向量长度\n",
    "        \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_red  = red_center \n",
    "                closest_blue = blue_center \n",
    "\n",
    "    closest_red  = tuple(closest_red)\n",
    "    closest_blue = tuple(closest_blue)\n",
    "\n",
    "    img_with_line = cv2.line(image.copy(), closest_red, closest_blue, (0, 255, 255),2)\n",
    "\n",
    "    cv2.imshow('Image with Line', img_with_line)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Closest Points Between Red and Blue Centers\n",
    "\n",
    "After detecting and calculating the centroids for red and blue regions in an image, the following step is to find the closest points between these two sets of centers. This is crucial for applications that require interaction or relative positioning between different colored objects.\n",
    "\n",
    "### Pseudocode for Finding Closest Points Between Red and Blue Centers and Drawing a Line\n",
    "\n",
    "1. **Check if There Are Centers in Both Red and Blue Lists**\n",
    "   - Proceed only if both `red_centers` and `blue_centers` are not empty.\n",
    "\n",
    "2. **Initialize Variables for the Closest Points**\n",
    "   - Set `min_dist` to infinity to start with a maximum possible distance.\n",
    "   - Initialize `closest_red` and `closest_blue` to `None`.\n",
    "\n",
    "3. **Find the Closest Red and Blue Center Pair**\n",
    "   - For each `red_center` in `red_centers`:\n",
    "     - For each `blue_center` in `blue_centers`:\n",
    "       - Calculate the Euclidean distance (`dist`) between `red_center` and `blue_center` using the norm of the difference vector.\n",
    "       - If `dist` is less than the current `min_dist`:\n",
    "         - Update `min_dist` with the new smallest distance.\n",
    "         - Set `closest_red` to the current `red_center`.\n",
    "         - Set `closest_blue` to the current `blue_center`.\n",
    "\n",
    "4. **Convert the Closest Points to Tuple Format**\n",
    "   - Convert `closest_red` and `closest_blue` back to tuples if they have been altered.\n",
    "\n",
    "5. **Draw a Line Between the Closest Points on a Copy of the Original Image**\n",
    "   - Copy the original `image`.\n",
    "   - Use `cv2.line` to draw a line between `closest_red` and `closest_blue` with color yellow (0, 255, 255) and thickness 2.\n",
    "\n",
    "6. **Display the Image with the Drawn Line**\n",
    "   - Use `cv2.imshow` to display `img_with_line` in a window titled 'Image with Line'.\n",
    "   - Use `cv2.waitKey()` to hold the window open until a key is pressed.\n",
    "   - Use `cv2.destroyAllWindows()` to close the window and release resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1 * cv2.arcLength(blue_contours[0], True)\n",
    "approx = cv2.approxPolyDP(blue_contours[0], epsilon, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Approximation of Contours\n",
    "\n",
    "Polygon approximation is an important step in image processing where contours detected in an image are simplified to polygonal shapes. This process reduces the number of points in the contour while maintaining the overall shape. The following code snippet demonstrates how to perform this approximation on the first contour of detected blue regions.\n",
    "\n",
    "### Step-by-Step Code Breakdown\n",
    "\n",
    "1. **Calculate Arc Length:**\n",
    "   - `cv2.arcLength(blue_contours[0], True)` calculates the perimeter of the first contour in the `blue_contours` list. The True parameter specifies that the contour is closed.\n",
    "   - `blue_contours` and `upper_blue` define the HSV range for blue colors, specifically capturing hues from 90 to 120.\n",
    "\n",
    "   ```python\n",
    "\n",
    "    epsilon = 0.1 * cv2.arcLength(blue_contours[0], True)\n",
    "    approx = cv2.approxPolyDP(blue_contours[0], epsilon, True)\n",
    "\n",
    "2. **Set Approximation Accuracy:**\n",
    "   - `epsilon` s set to 10% of the arc length. This value determines the accuracy of the approximation. Smaller `epsilon` values make the approximation closer to the original contour.\n",
    "\n",
    "\n",
    "\n",
    "3. **Approximate Contour to Polygon:**\n",
    "   - `cv2.approxPolyDP(blue_contours[0], epsilon, True)`performs the approximation. The `True` again specifies that the approximated contour is closed. The function simplifies the contour points based on the specified epsilon, effectively reducing the complexity of the contour.\n",
    "\n",
    "\n",
    "### Purpose and Benefit\n",
    "1. **Simplification:** This method simplifies the contour to a polygon, which can be easier to analyze and process further. For example, detecting corners or calculating the bounding box can be more efficient with fewer points.\n",
    "\n",
    "1. **Noise Reduction:** By approximating the contour, small variations and noise around the contour edges are eliminated, leading to a smoother and more regular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull = cv2.convexHull(approx)\n",
    "\n",
    "hull_points = [tuple(point[0]) for point in hull]\n",
    "\n",
    "side_length_ab =  ((hull_points[0][0] - hull_points[1][0])** 2 + (hull_points[0][1] - hull_points[1][1])** 2 ) ** 0.5\n",
    "side_length_ac =  ((hull_points[0][0] - hull_points[2][0])** 2 + (hull_points[0][1] - hull_points[2][1])** 2 ) ** 0.5\n",
    "side_length_bc =  ((hull_points[1][0] - hull_points[2][0])** 2 + (hull_points[1][1] - hull_points[2][1])** 2 ) ** 0.5\n",
    "\n",
    "max_side_length = max(side_length_ab + side_length_ac , side_length_bc + side_length_ac, side_length_ab + side_length_bc)\n",
    "\n",
    "if max_side_length == side_length_ab + side_length_ac: #a\n",
    "    intersection = hull_points[0]\n",
    "    \n",
    "if max_side_length == side_length_ab + side_length_bc: #b\n",
    "    intersection = hull_points[1]\n",
    "    \n",
    "if max_side_length == side_length_ac + side_length_bc: #c\n",
    "    intersection = hull_points[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection point coordinates: (119, 111)\n",
      "Blue center coordinates: (114, 115)\n",
      "red center coordinates: (117, 86)\n",
      "Angle: -45.43405063213941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 69, 147, 175],\n",
       "        [ 74, 149, 178],\n",
       "        [ 82, 148, 184],\n",
       "        ...,\n",
       "        [ 77, 106, 134],\n",
       "        [ 77, 106, 134],\n",
       "        [ 70, 101, 128]],\n",
       "\n",
       "       [[ 72, 148, 177],\n",
       "        [ 74, 149, 178],\n",
       "        [ 82, 149, 187],\n",
       "        ...,\n",
       "        [ 77, 109, 136],\n",
       "        [ 77, 109, 136],\n",
       "        [ 77, 119, 149]],\n",
       "\n",
       "       [[ 82, 151, 184],\n",
       "        [ 82, 151, 184],\n",
       "        [ 82, 150, 189],\n",
       "        ...,\n",
       "        [ 78, 109, 138],\n",
       "        [ 82, 124, 155],\n",
       "        [ 82, 130, 161]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 99, 158, 206],\n",
       "        [ 99, 158, 206],\n",
       "        [ 87, 159, 204],\n",
       "        ...,\n",
       "        [ 66, 127, 161],\n",
       "        [ 66, 116, 152],\n",
       "        [ 73, 130, 164]],\n",
       "\n",
       "       [[ 90, 157, 206],\n",
       "        [ 93, 161, 206],\n",
       "        [ 82, 155, 194],\n",
       "        ...,\n",
       "        [ 71, 132, 172],\n",
       "        [ 67, 117, 160],\n",
       "        [ 71, 126, 169]],\n",
       "\n",
       "       [[ 91, 154, 192],\n",
       "        [ 87, 150, 192],\n",
       "        [ 77, 144, 181],\n",
       "        ...,\n",
       "        [ 63, 115, 155],\n",
       "        [ 63, 116, 155],\n",
       "        [ 67, 119, 159]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_to_intersection_vector = np.array([intersection[0] - cx, intersection[1] - cy])\n",
    "\n",
    "extended_point = (int(cx + 3 * centroid_to_intersection_vector[0]), int(cy + 3 * centroid_to_intersection_vector[1]))\n",
    "\n",
    "\n",
    "img_with_yellow_lines = cv2.line(image.copy(), (cx, cy), intersection, (0, 255, 255), 2)\n",
    "\n",
    "\n",
    "img_with_line = cv2.line(img_with_line, intersection, extended_point, (0, 255, 255), 2)\n",
    "img_with_line = cv2.line(img_with_line, (cx, cy), intersection, (0, 255, 255), 2)\n",
    "\n",
    "cv2.circle(img_with_yellow_lines, intersection, 2, (0, 255, 0), -1)\n",
    "cv2.circle(img_with_yellow_lines, blue_center, 2, (255, 0, 0), -1)\n",
    "\n",
    "for red_center in red_centers:\n",
    "    cv2.circle(img_with_yellow_lines, red_center, 2, (0, 0, 255), -1)\n",
    "\n",
    "print(\"Intersection point coordinates:\", intersection)\n",
    "print(\"Blue center coordinates:\", blue_center)\n",
    "print(\"red center coordinates:\", red_center)\n",
    "\n",
    "vec_blue = np.array(intersection) - np.array(blue_center)\n",
    "vec_red  = np.array(red_center)   - np.array(blue_center)\n",
    "\n",
    "angle_between = np.degrees(np.arctan2(vec_red[1], vec_red[0]) - np.arctan2(vec_blue[1], vec_blue[0]))\n",
    "\n",
    "if angle_between > 180:\n",
    "    angle_between -= 360\n",
    "elif angle_between < -180:\n",
    "    angle_between += 360\n",
    "\n",
    "\n",
    "print(\"Angle:\", angle_between)\n",
    "\n",
    "cv2.putText(img_with_line, f'{angle_between:.0f}', (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_line = cv2.resize(img_with_line, None, fx=3, fy=3)\n",
    "cv2.imshow('Image with Line', img_with_line)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result](result.png \"result\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
