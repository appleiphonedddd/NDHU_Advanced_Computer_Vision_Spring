{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "1. Input BGR images from webcam.\n",
    "2. Detect your face, mouth, and eyes. \n",
    "3. Input BGRA images from files \"mustache.png\" and \"hat.png\" (hint: cv2.imread(\"mustache.png\", cv2.IMREAD_UNCHANGED) to read 4 channels)\n",
    "4. Perform <b> Alpha Blending </b> to add mustache and hat on the right position and orientation of your face.\n",
    "5. The overlaid mustache and hat should be translated, rotated , and scaled according to the movement of your face. \n",
    "6. Show your output images.\n",
    "7. Any idea on how to detect face features better? Try it and compare the results. (hint: modules of <i>dlib</i> or <i>mediapipe</i>)\n",
    "8. (5pts bonus) Open/Close your mouth to toggle mustache on/off.\n",
    "9. (5pts bonus) Blink your eye to toggle hat on/off.\n",
    "10. Upload your Jupyter code file (*.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements.txt\n",
    "\n",
    "#### opencv-python==4.9.0.80\n",
    "#### mediapipe==0.10.13\n",
    "#### numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1.260] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@1.357] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.444] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@1.445] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.445] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@1.446] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.446] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video5): can't open camera by index\n",
      "[ERROR:0@1.447] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.447] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video6): can't open camera by index\n",
      "[ERROR:0@1.448] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.448] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video7): can't open camera by index\n",
      "[ERROR:0@1.450] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.450] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video8): can't open camera by index\n",
      "[ERROR:0@1.451] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@1.451] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video9): can't open camera by index\n",
      "[ERROR:0@1.452] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "def list_available_cameras(max_index):\n",
    "    \"\"\"\n",
    "    Lists the available camera indices.\n",
    "\n",
    "    Args:\n",
    "        max_index (int): The maximum index to check for available cameras.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of indices where cameras are available.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the indices of available cameras\n",
    "    available_cameras = []\n",
    "\n",
    "    # Iterate through the indices\n",
    "    for i in range(max_index):\n",
    "        cap = cv2.VideoCapture(i)\n",
    "\n",
    "        # Check if the camera is available\n",
    "        if cap.isOpened():\n",
    "            # Add the index to the list of available cameras\n",
    "            available_cameras.append(i)\n",
    "            cap.release()        \n",
    "\n",
    "    return available_cameras\n",
    "\n",
    "max_camera_index = 10 # Maximum index to check for available cameras\n",
    "available_cameras = list_available_cameras(max_camera_index) # Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Cameras: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Print the available cameras\n",
    "\n",
    "print(\"Available Cameras:\", available_cameras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Resizing Process\n",
    "\n",
    "### Original Dimensions:\n",
    "- Let $h$ be the original height of the image.\n",
    "- Let $w$ be the original width of the image.\n",
    "\n",
    "### Target Width:\n",
    "- Let $W_{\\text{target}}$ be the desired width of the resized image.\n",
    "\n",
    "### Scaling Factor:\n",
    "The scaling factor $S$ is calculated as the ratio of the target width to the original width:\n",
    "$$\n",
    "S = \\frac{W_{\\text{target}}}{w}\n",
    "$$\n",
    "\n",
    "### New Dimensions:\n",
    "To maintain the aspect ratio, both the width and height of the image must be scaled by the same factor $S$.\n",
    "\n",
    "#### New Width:\n",
    "The new width $W_{\\text{new}}$ will be:\n",
    "$$\n",
    "W_{\\text{new}} = S \\times w = W_{\\text{target}}\n",
    "$$\n",
    "\n",
    "#### New Height:\n",
    "The new height $H_{\\text{new}}$ will be:\n",
    "$$\n",
    "H_{\\text{new}} = S \\times h\n",
    "$$\n",
    "Substituting $S$:\n",
    "$$\n",
    "H_{\\text{new}} = \\left(\\frac{W_{\\text{target}}}{w}\\right) \\times h = \\frac{W_{\\text{target}} \\times h}{w}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_overlay(overlay_img, target_width):\n",
    "    \"\"\"\n",
    "    Resizes the overlay image to the target width while maintaining the aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        overlay_img (numpy.ndarray): The image to be resized.\n",
    "        target_width (int): The desired width of the resized image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The resized image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the width and height of the overlay image\n",
    "    h, w = overlay_img.shape[:2]\n",
    "\n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = target_width / float(w)\n",
    "\n",
    "    # Resize the image and return it\n",
    "    return cv2.resize(overlay_img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Overlay Process\n",
    "\n",
    "### Positioning:\n",
    "- Let $(x, y)$ be the position where the top-left corner of the overlay image (`img_overlay`) should be placed on the background image (`img`).\n",
    "\n",
    "### Bounding Coordinates:\n",
    "Calculate the coordinates of the region in the background image (`img`) that will be affected by the overlay:\n",
    "- $y_1 = \\max(0, y)$\n",
    "- $y_2 = \\min(\\text{height of img}, y + \\text{height of img\\_overlay})$\n",
    "- $x_1 = \\max(0, x)$\n",
    "- $x_2 = \\min(\\text{width of img}, x + \\text{width of img\\_overlay})$\n",
    "\n",
    "Calculate the coordinates of the corresponding region in the overlay image (`img_overlay`):\n",
    "- $y_{1o} = \\max(0, -y)$\n",
    "- $y_{2o} = \\min(\\text{height of img\\_overlay}, \\text{height of img} - y)$\n",
    "- $x_{1o} = \\max(0, -x)$\n",
    "- $x_{2o} = \\min(\\text{width of img\\_overlay}, \\text{width of img} - x)$\n",
    "\n",
    "### Blending Condition:\n",
    "Ensure that the overlay region is valid (i.e., the regions overlap):\n",
    "- $y_1 < y_2$\n",
    "- $x_1 < x_2$\n",
    "- $y_{1o} < y_{2o}$\n",
    "- $x_{1o} < x_{2o}$\n",
    "\n",
    "If any of these conditions are not met, the function returns without modifying the image.\n",
    "\n",
    "### Alpha Blending:\n",
    "For each color channel $c$, blend the overlay image with the background image using the alpha mask:\n",
    "$$\n",
    "img[y_1:y_2, x_1:x_2, c] = \\alpha_{mask}[y_{1o}:y_{2o}, x_{1o}:x_{2o}] \\cdot img_{overlay}[y_{1o}:y_{2o}, x_{1o}:x_{2o}, c] + (1.0 - \\alpha_{mask}[y_{1o}:y_{2o}, x_{1o}:x_{2o}]) \\cdot img[y_1:y_2, x_1:x_2, c]\n",
    "$$\n",
    "Here, $\\alpha_{mask}$ represents the alpha mask values, which control the transparency. Each pixel in the region is a blend of the overlay pixel and the background pixel, weighted by the alpha mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_alpha(img, img_overlay, pos, alpha_mask):\n",
    "    \"\"\"\n",
    "    Overlays img_overlay on top of img at the position specified by pos and uses alpha_mask\n",
    "    to blend the images.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The background image.\n",
    "        img_overlay (numpy.ndarray): The image to overlay.\n",
    "        pos (tuple): The (x, y) position to place the overlay image on the background image.\n",
    "        alpha_mask (numpy.ndarray): The alpha mask to control transparency.\n",
    "\n",
    "    Returns:\n",
    "        None: The function modifies img in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the position where the overlay image will be placed\n",
    "    x, y = pos\n",
    "\n",
    "    # Determine the coordinates of the region in the background image\n",
    "    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n",
    "    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n",
    "\n",
    "    # Determine the coordinates of the region in the overlay image.\n",
    "    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n",
    "    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n",
    "\n",
    "    # If there is no overlap, return without modifying the image.\n",
    "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
    "        return\n",
    "    \n",
    "    # Blend the overlay image and the background image.\n",
    "    for c in range(img.shape[2]):\n",
    "        # Iterate over each color channel.        \n",
    "        img[y1:y2, x1:x2, c] = (alpha_mask[y1o:y2o, x1o:x2o] * img_overlay[y1o:y2o, x1o:x2o, c] +\n",
    "                                (1.0 - alpha_mask[y1o:y2o, x1o:x2o]) * img[y1:y2, x1:x2, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Angle Calculation\n",
    "\n",
    "To calculate the angle between two points $(x_1, y_1)$ and $(x_2, y_2)$ in a 2D plane, we use trigonometry:\n",
    "\n",
    "### Differences in Coordinates:\n",
    "- Let $\\Delta x$ be the difference in the x-coordinates:\n",
    "$$\n",
    "\\Delta x = x_2 - x_1\n",
    "$$\n",
    "- Let $\\Delta y$ be the difference in the y-coordinates:\n",
    "$$\n",
    "\\Delta y = y_2 - y_1\n",
    "$$\n",
    "\n",
    "### Arctangent Function:\n",
    "The arctangent function, specifically $\\text{arctan2}(\\Delta y, \\Delta x)$, computes the angle $\\theta$ between the positive x-axis and the line connecting the two points. The $\\text{arctan2}$ function is used instead of the standard $\\text{arctan}$ because it takes into account the signs of both $\\Delta x$ and $\\Delta y$, providing the correct quadrant for the angle.\n",
    "\n",
    "The angle $\\theta$ in radians is given by:\n",
    "$$\n",
    "\\theta = \\text{arctan2}(\\Delta y, \\Delta x)\n",
    "$$\n",
    "\n",
    "### Conversion to Degrees:\n",
    "To convert the angle from radians to degrees, we use the formula:\n",
    "$$\n",
    "\\theta_{\\text{degrees}} = \\theta \\times \\frac{180}{\\pi}\n",
    "$$\n",
    "\n",
    "Combining the formulas, the angle in degrees can be calculated as:\n",
    "$$\n",
    "\\theta_{\\text{degrees}} = \\text{arctan2}(\\Delta y, \\Delta x) \\times \\frac{180}{\\pi}\n",
    "$$\n",
    "\n",
    "This process ensures that the calculated angle correctly represents the direction from the first point to the second point, measured counterclockwise from the positive x-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculates the angle between two points in degrees.\n",
    "\n",
    "    Args:\n",
    "        point1 (tuple): The (x, y) coordinates of the first point.\n",
    "        point2 (tuple): The (x, y) coordinates of the second point.\n",
    "\n",
    "    Returns:\n",
    "        float: The angle between the two points in degrees.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the difference in x-coordinates and y-coordinates\n",
    "    dx = point2[0] - point1[0]\n",
    "    dy = point2[1] - point1[1]\n",
    "    \n",
    "    # Calculate the angle in radians using arctan2 and convert it to degrees.\n",
    "    return np.degrees(np.arctan2(dy, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of Detecting an Open Mouth\n",
    "\n",
    "To determine whether the mouth is open based on facial landmarks, we use the vertical positions of specific points on the upper and lower lips. Here is a step-by-step mathematical explanation:\n",
    "\n",
    "### Identify Relevant Landmarks:\n",
    "- Let $U_1$ and $U_2$ be the y-coordinates of two points on the upper lip.\n",
    "- Let $L_1$ and $L_2$ be the y-coordinates of two points on the lower lip.\n",
    "\n",
    "In the provided code, these correspond to:\n",
    "- $U_1 = \\text{landmarks}[13].y$\n",
    "- $U_2 = \\text{landmarks}[312].y$\n",
    "- $L_1 = \\text{landmarks}[14].y$\n",
    "- $L_2 = \\text{landmarks}[317].y$\n",
    "\n",
    "### Calculate Average Heights:\n",
    "The average height of the upper lip ($U_{\\text{avg}}$):\n",
    "$$\n",
    "U_{\\text{avg}} = \\frac{U_1 + U_2}{2}\n",
    "$$\n",
    "\n",
    "The average height of the lower lip ($L_{\\text{avg}}$):\n",
    "$$\n",
    "L_{\\text{avg}} = \\frac{L_1 + L_2}{2}\n",
    "$$\n",
    "\n",
    "### Compute the Vertical Distance:\n",
    "The vertical distance between the upper and lower lips ($D$):\n",
    "$$\n",
    "D = L_{\\text{avg}} - U_{\\text{avg}}\n",
    "$$\n",
    "\n",
    "### Determine if Mouth is Open:\n",
    "Define a threshold $T$ (in this case, $T = 0.03$). The mouth is considered open if the vertical distance $D$ is greater than the threshold $T$:\n",
    "$$\n",
    "\\text{Mouth is open if } D > T\n",
    "$$\n",
    "\n",
    "This can be mathematically written as:\n",
    "$$\n",
    "\\text{Mouth is open if } \\left(\\frac{L_1 + L_2}{2} - \\frac{U_1 + U_2}{2}\\right) > 0.02\n",
    "$$\n",
    "\n",
    "In summary, the function calculates the average vertical positions of specified landmarks on the upper and lower lips and checks if the difference between these averages exceeds a certain threshold. If it does, the function concludes that the mouth is open.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks):\n",
    "    \"\"\"\n",
    "    Determines if the mouth is open based on the positions of facial landmarks.\n",
    "\n",
    "    Args:\n",
    "        landmarks (list): A list of facial landmarks with each landmark having x and y attributes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the mouth is open, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indices of landmarks corresponding to the upper and lower lips\n",
    "    upper_lip_indices = [13, 312]\n",
    "    lower_lip_indices = [14, 317]\n",
    "\n",
    "    # Calculate the average y-coordinate of the upper lip landmarks.\n",
    "    upper_lip_height = (landmarks[upper_lip_indices[0]].y + landmarks[upper_lip_indices[1]].y) / 2\n",
    "\n",
    "    # Calculate the average y-coordinate of the lower lip landmarks.\n",
    "    lower_lip_height = (landmarks[lower_lip_indices[0]].y + landmarks[lower_lip_indices[1]].y) / 2\n",
    "\n",
    "    # Determine if the mouth is open by checking if the distance between the upper and lower lips is greater than a threshold.\n",
    "    return lower_lip_height - upper_lip_height > 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of Detecting a Closed Eye\n",
    "\n",
    "To determine whether an eye is closed based on facial landmarks, we use the vertical positions of specific points on the eye. Here is a step-by-step mathematical explanation:\n",
    "\n",
    "### Identify Relevant Landmarks:\n",
    "- Let $T$ be the y-coordinate of the top landmark of the eye.\n",
    "- Let $B$ be the y-coordinate of the bottom landmark of the eye.\n",
    "\n",
    "In the provided code, these correspond to:\n",
    "- $T = \\text{landmarks}[\\text{eye\\_indices}[0]].y$\n",
    "- $B = \\text{landmarks}[\\text{eye\\_indices}[1]].y$\n",
    "\n",
    "### Compute the Vertical Distance:\n",
    "The vertical distance between the top and bottom landmarks of the eye ($D$):\n",
    "$$\n",
    "D = B - T\n",
    "$$\n",
    "\n",
    "### Determine if the Eye is Closed:\n",
    "Define a threshold $T_h$ (in this case, $T_h = 0.04$). The eye is considered closed if the vertical distance $D$ is less than the threshold $T_h$:\n",
    "$$\n",
    "\\text{Eye is closed if } D < T_h\n",
    "$$\n",
    "\n",
    "This can be mathematically written as:\n",
    "$$\n",
    "\\text{Eye is closed if } (B - T) < 0.02\n",
    "$$\n",
    "\n",
    "In summary, the function calculates the vertical distance between the top and bottom landmarks of the eye and checks if this distance is below a certain threshold. If it is, the function concludes that the eye is closed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_eye_closed(landmarks, eye_top_index, eye_bottom_index):\n",
    "    \"\"\"\n",
    "    Determines if an eye is closed based on the positions of facial landmarks.\n",
    "\n",
    "    Args:\n",
    "        landmarks (list): A list of facial landmarks with each landmark having x and y attributes.\n",
    "        eye_top_index (int): The index of the top landmark of the eye.\n",
    "        eye_bottom_index (int): The index of the bottom landmark of the eye.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the eye is closed, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the y-coordinate of the top landmark of the eye.\n",
    "    eye_top = landmarks[eye_top_index].y\n",
    "\n",
    "    # Get the y-coordinate of the bottom landmark of the eye.\n",
    "    eye_bottom = landmarks[eye_bottom_index].y\n",
    "\n",
    "    \"\"\"\n",
    "    Determine if the eye is closed by checking if the vertical distance between\n",
    "    the top and bottom landmarks is less than a threshold.\n",
    "    \"\"\"\n",
    "    return eye_bottom - eye_top < 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Explanation of the Special Face Overlay Process\n",
    "\n",
    "The process involves detecting face landmarks, checking eye and mouth states, and toggling overlays (mustache and hat) based on those states. Here's a step-by-step mathematical explanation:\n",
    "\n",
    "### Face Detection:\n",
    "\n",
    "#### Bounding Box Calculation:\n",
    "Let $(x_{rel}, y_{rel}, w_{rel}, h_{rel})$ be the relative coordinates and dimensions of the bounding box as provided by the face detection model. Convert these relative coordinates to absolute pixel coordinates based on the image dimensions $(ih, iw)$:\n",
    "$$\n",
    "x = x_{rel} \\times iw, \\quad y = y_{rel} \\times ih\n",
    "$$\n",
    "$$\n",
    "w = w_{rel} \\times iw, \\quad h = h_{rel} \\times ih\n",
    "$$\n",
    "\n",
    "### Landmark Detection:\n",
    "Extract the coordinates of specific landmarks (e.g., eye points, mouth points) and convert them to absolute pixel coordinates:\n",
    "$$\n",
    "\\text{landmark}[i] = (\\text{landmarks}[i].x \\times iw, \\text{landmarks}[i].y \\times ih)\n",
    "$$\n",
    "\n",
    "### Eye State Detection:\n",
    "Define the indices for the top and bottom points of the eye:\n",
    "$$\n",
    "T_{\\text{eye}} = \\text{landmarks}[\\text{eye\\_indices}[0]].y, \\quad B_{\\text{eye}} = \\text{landmarks}[\\text{eye\\_indices}[1]].y\n",
    "$$\n",
    "Calculate the vertical distance $D_{\\text{eye}}$:\n",
    "$$\n",
    "D_{\\text{eye}} = B_{\\text{eye}} - T_{\\text{eye}}\n",
    "$$\n",
    "Determine if the eye is closed by checking if $D_{\\text{eye}}$ is less than a threshold $T_h$:\n",
    "$$\n",
    "\\text{Eye is closed if } D_{\\text{eye}} < 0.02\n",
    "$$\n",
    "\n",
    "### Mouth State Detection:\n",
    "Define the indices for the upper and lower points of the lips:\n",
    "$$\n",
    "U_{\\text{lip}} = \\frac{\\text{landmarks}[13].y + \\text{landmarks}[312].y}{2}, \\quad L_{\\text{lip}} = \\frac{\\text{landmarks}[14].y + \\text{landmarks}[317].y}{2}\n",
    "$$\n",
    "Calculate the vertical distance $D_{\\text{lip}}$:\n",
    "$$\n",
    "D_{\\text{lip}} = L_{\\text{lip}} - U_{\\text{lip}}\n",
    "$$\n",
    "Determine if the mouth is open by checking if $D_{\\text{lip}}$ is greater than a threshold $T_h$:\n",
    "$$\n",
    "\\text{Mouth is open if } D_{\\text{lip}} > 0.02\n",
    "$$\n",
    "\n",
    "### Overlay Toggle Logic:\n",
    "- Toggle the mustache when the mouth state changes from open to closed.\n",
    "- Toggle the hat when the eye state changes from closed to open.\n",
    "\n",
    "### Resizing Overlays:\n",
    "Calculate the widths of the mustache and hat based on the bounding box width $w$:\n",
    "$$\n",
    "\\text{mustache\\_width} = 0.6 \\times w, \\quad \\text{hat\\_width} = w\n",
    "$$\n",
    "\n",
    "### Overlay Positioning:\n",
    "Calculate the positions for the mustache and hat based on the bounding box and landmark positions:\n",
    "$$\n",
    "\\text{mustache\\_pos} = (x + 0.2w, y + 0.5h - 20), \\quad \\text{hat\\_pos} = (x, y - \\text{hat\\_height})\n",
    "$$\n",
    "\n",
    "### Alpha Blending:\n",
    "Blend the resized and positioned overlays with the frame using the alpha masks to ensure proper transparency:\n",
    "$$\n",
    "\\text{frame}[y:y+h, x:x+w, c] = \\alpha \\times \\text{overlay}[y_o:y_o+h_o, x_o:x_o+w_o, c] + (1 - \\alpha) \\times \\text{frame}[y:y+h, x:x+w, c]\n",
    "$$\n",
    "\n",
    "This blending is performed for each color channel $c$.\n",
    "\n",
    "### Summary\n",
    "The script involves detecting the face and facial landmarks, determining the state of the eyes and mouth to toggle overlays, resizing and positioning the overlays appropriately, and blending them onto the original image using alpha masks for transparency. The mathematical operations ensure accurate positioning, resizing, and blending based on the detected landmarks and bounding box dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716855767.928676    4958 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1716855767.931288    5119 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.5-1ubuntu1), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "I0000 00:00:1716855767.943650    4958 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1716855767.944924    5130 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.5-1ubuntu1), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "/home/infor/miniconda3/envs/CV/lib/python3.9/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/infor/miniconda3/envs/CV/lib/python3.9/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Special solution for bonus challenge\n",
    "\n",
    "# Initialize MediaPipe Face Detection and Face Mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Load mustache and hat images with alpha channels\n",
    "mustache_img = cv2.imread(\"mustache.png\", cv2.IMREAD_UNCHANGED)\n",
    "hat_img = cv2.imread(\"hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize toggle states and detection states\n",
    "mustache_on = True\n",
    "mouth_is_open = False\n",
    "\n",
    "hat_on = True\n",
    "hat_is_closed = False\n",
    "\n",
    "# Indices of the top and bottom landmarks of the eyes\n",
    "left_eye_top_index = 386\n",
    "left_eye_bottom_index = 374\n",
    "right_eye_top_index = 159\n",
    "right_eye_bottom_index = 145\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Loop to process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        continue\n",
    "\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face mesh landmarks\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    # Check if face landmarks are detected\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the detected faces\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Get the landmarks\n",
    "            landmarks = face_landmarks.landmark\n",
    "            # Calculate the angle for mustache rotation\n",
    "            nose_tip = (landmarks[1].x, landmarks[1].y)\n",
    "            face_center = (landmarks[5].x, landmarks[5].y)\n",
    "            angle = calculate_angle(nose_tip, face_center)\n",
    "\n",
    "\n",
    "            # Check if the eye is closed to toggle the hat\n",
    "            if not hat_is_closed and is_eye_closed(landmarks, left_eye_top_index, left_eye_bottom_index):\n",
    "                # Eye has closed\n",
    "                hat_is_closed = True\n",
    "            elif hat_is_closed and not is_eye_closed(landmarks, left_eye_top_index, left_eye_bottom_index):\n",
    "                # Eye was closed and is now open, toggle the hat\n",
    "                hat_on = not hat_on\n",
    "\n",
    "            # Reset the state\n",
    "                hat_is_closed = False \n",
    "            \n",
    "            # Check if the mouth is open to toggle the mustache\n",
    "            if not mouth_is_open and is_mouth_open(landmarks):\n",
    "                # Mouth has opened\n",
    "                mouth_is_open = True\n",
    "\n",
    "            # Check if the mouth is closed to toggle the mustache\n",
    "            elif mouth_is_open and not is_mouth_open(landmarks):\n",
    "                # Mouth was open and is now closed, toggle the mustache\n",
    "                mustache_on = not mustache_on\n",
    "\n",
    "                # Reset the state\n",
    "                mouth_is_open = False\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    face_detection_results = face_detection.process(frame_rgb)\n",
    "\n",
    "    if face_detection_results.detections:\n",
    "        for detection in face_detection_results.detections:\n",
    "\n",
    "            # Get the bounding box coordinates of the detected face\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "\n",
    "            # Get the frame dimensions\n",
    "            ih, iw, _ = frame.shape\n",
    "\n",
    "            # Convert the bounding box coordinates to pixel values\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "            # If mustache is toggled on, overlay the mustache\n",
    "            if mustache_on:\n",
    "                # Resize the mustache image to fit the face\n",
    "                mustache_width = int(w * 0.6)\n",
    "\n",
    "                # Resize the mustache image\n",
    "                resized_mustache_img = resize_overlay(mustache_img, mustache_width)\n",
    "\n",
    "                # Extract the alpha channel from the resized mustache image\n",
    "                resized_mustache_alpha = resized_mustache_img[:, :, 3] / 255.0\n",
    "\n",
    "                # Remove the alpha channel\n",
    "                resized_mustache_img = resized_mustache_img[:, :, :3]\n",
    "\n",
    "                # Adjust the y-coordinate to place the mustache on the upper lip\n",
    "                mustache_pos = (x + int(w * 0.2), y + int(h * 0.5) - 20)\n",
    "\n",
    "                # Overlay the mustache\n",
    "                overlay_image_alpha(frame, resized_mustache_img, mustache_pos, resized_mustache_alpha)\n",
    "            \n",
    "            # If hat is toggled on, overlay the hat\n",
    "            if hat_on:\n",
    "\n",
    "                # Resize the hat image to fit the face\n",
    "                hat_width = int(w * 1)\n",
    "\n",
    "                # Resize the hat image\n",
    "                resized_hat_img = resize_overlay(hat_img, hat_width)\n",
    "\n",
    "                # Extract the alpha channel from the resized hat image\n",
    "                resized_hat_alpha = resized_hat_img[:, :, 3] / 255.0\n",
    "\n",
    "                # Remove the alpha channel\n",
    "                resized_hat_img = resized_hat_img[:, :, :3]\n",
    "\n",
    "                # Adjust the y-coordinate to place the hat above the head\n",
    "                hat_pos = (x, y - resized_hat_img.shape[0])\n",
    "\n",
    "                # Overlay the hat\n",
    "                overlay_image_alpha(frame, resized_hat_img, hat_pos, resized_hat_alpha)\n",
    "    \n",
    "    # Display the result    \n",
    "    cv2.imshow('Result', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "#### Normal solution\n",
    "\n",
    "![Normal result](Result.png)\n",
    "\n",
    "\n",
    "#### 2.Open/Close your mouth to toggle mustache on/off.\n",
    "![Mouth open](mouth_open.png)\n",
    "![Mouth close](mouth_close.png)\n",
    "\n",
    "#### 3.Blink your eye to toggle hat on/off.\n",
    "\n",
    "<https://drive.google.com/file/d/1O6_QgJU7tw5S52NGcVsT7j8S64rZUqoq/view?usp=drive_link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any idea on how to detect face features better?\n",
    "\n",
    "initial I use FaceMesh to detect my face, put the hat and mustache on my face, and second cell I use FaceDetection, finally to compare the result.\n",
    "\n",
    "## Result\n",
    "According to the result,I find in the same solution, parameter, algorithm and alpha value, If we use FaceMesh can solve the  bonus challenge, on the other hand, only use FaceDetection just put the hat and mustache on my face, it is cant detect my eye top, eye bottom, upper lip and lower lip, why? now I will explain the different between MediaPipe FaceMesh and FaceDetection\n",
    "\n",
    "\n",
    "\n",
    "## Comparison of MediaPipe FaceMesh and FaceDetection\n",
    "\n",
    "**FaceMesh** and **FaceDetection** are both powerful tools within the MediaPipe framework designed for different purposes related to facial analysis. Here's a comparison of their features, applications, and performance:\n",
    "\n",
    "### MediaPipe FaceMesh\n",
    "\n",
    "**Purpose:**\n",
    "- Provides a detailed 3D mesh of the face with 468 landmarks, offering high precision in capturing facial geometry.\n",
    "\n",
    "**Key Features:**\n",
    "- **High Precision Landmarks**: It captures 468 points on the face, making it suitable for applications requiring fine-grained details.\n",
    "- **3D Output**: Outputs 3D coordinates, allowing for applications involving depth information.\n",
    "- **Facial Expressions and Features**: Can accurately capture expressions, eyebrow movement, eye blinking, and lip movements.\n",
    "- **Real-Time Performance**: Optimized for real-time applications with a high frame rate.\n",
    "\n",
    "**Applications:**\n",
    "- **Augmented Reality**: Applying virtual makeup, glasses, masks, and other facial accessories.\n",
    "- **Animation and Avatars**: Driving 3D avatars and animations with detailed facial expressions.\n",
    "- **Medical and Health**: Analyzing facial symmetry for medical applications, including dental and orthodontic assessments.\n",
    "- **Facial Recognition**: Enhancing facial recognition systems by providing detailed landmark information.\n",
    "\n",
    "**Performance:**\n",
    "- **Complexity**: Higher computational cost due to the large number of landmarks.\n",
    "- **Accuracy**: High accuracy and precision in landmark detection.\n",
    "- **Use Cases**: Suitable for applications needing detailed facial geometry and high precision.\n",
    "\n",
    "### MediaPipe FaceDetection\n",
    "\n",
    "**Purpose:**\n",
    "- Provides fast and reliable face detection, identifying the presence and location of faces in images or videos.\n",
    "\n",
    "**Key Features:**\n",
    "- **Bounding Box Detection**: Outputs bounding boxes for detected faces.\n",
    "- **Fast and Efficient**: Optimized for speed and efficiency, making it suitable for real-time applications.\n",
    "- **Confidence Scores**: Provides confidence scores for detected faces.\n",
    "- **Simpler Output**: Outputs fewer landmarks compared to FaceMesh (6 key points), focusing on face location and orientation.\n",
    "\n",
    "**Applications:**\n",
    "- **Basic Face Detection**: Identifying faces in images and videos for security cameras, social media filters, and photo organization.\n",
    "- **Initial Step for Further Processing**: Often used as a preprocessing step for other applications like face recognition, emotion detection, or applying FaceMesh for detailed analysis.\n",
    "- **Real-Time Applications**: Suitable for applications requiring quick face detection without the need for detailed facial landmarks.\n",
    "\n",
    "**Performance:**\n",
    "- **Complexity**: Lower computational cost due to simpler detection mechanism.\n",
    "- **Speed**: Very fast and efficient, suitable for real-time processing.\n",
    "- **Use Cases**: Ideal for applications needing quick and reliable face detection without the need for detailed facial landmarks.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **FaceMesh** is suited for applications requiring detailed facial geometry and high precision, such as augmented reality, animation, and medical analysis.\n",
    "- **FaceDetection** is ideal for applications needing fast and efficient face detection, such as security systems, social media filters, and as a preprocessing step for more detailed facial analysis.\n",
    "\n",
    "Actually, **FaceDetection** can be used to quickly identify faces in a scene, and **FaceMesh** can be applied to those detected faces for detailed analysis and rendering. This combination allows for both efficiency and precision in various facial analysis applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1716855787.149405    4958 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1716855787.151012    5237 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.5-1ubuntu1), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n"
     ]
    }
   ],
   "source": [
    "# use FaceDetection to detect the face in the frame\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "# Load mustache and hat images with alpha channels\n",
    "mustache_img = cv2.imread(\"mustache.png\", cv2.IMREAD_UNCHANGED)\n",
    "hat_img = cv2.imread(\"hat.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Initialize toggle states and detection states\n",
    "mustache_on = True\n",
    "mouth_is_open = False\n",
    "\n",
    "hat_on = True\n",
    "hat_is_closed = False\n",
    "\n",
    "# Indices of the top and bottom landmarks of the eyes\n",
    "left_eye_top_index = 386\n",
    "left_eye_bottom_index = 374\n",
    "right_eye_top_index = 159\n",
    "right_eye_bottom_index = 145\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Loop to process video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        continue\n",
    "\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face mesh landmarks\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    # Check if face landmarks are detected\n",
    "\n",
    "    # Detect faces in the frame\n",
    "\n",
    "    face_detection_results = face_detection.process(frame_rgb)\n",
    "\n",
    "    if face_detection_results.detections:\n",
    "        for detection in face_detection_results.detections:\n",
    "\n",
    "            # Get the bounding box coordinates of the detected face\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "\n",
    "            # Get the frame dimensions\n",
    "            ih, iw, _ = frame.shape\n",
    "\n",
    "            # Convert the bounding box coordinates to pixel values\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "            # If mustache is toggled on, overlay the mustache\n",
    "            if mustache_on:\n",
    "                # Resize the mustache image to fit the face\n",
    "                mustache_width = int(w * 0.6)\n",
    "\n",
    "                # Resize the mustache image\n",
    "                resized_mustache_img = resize_overlay(mustache_img, mustache_width)\n",
    "\n",
    "                # Extract the alpha channel from the resized mustache image\n",
    "                resized_mustache_alpha = resized_mustache_img[:, :, 3] / 255.0\n",
    "\n",
    "                # Remove the alpha channel\n",
    "                resized_mustache_img = resized_mustache_img[:, :, :3]\n",
    "\n",
    "                # Adjust the y-coordinate to place the mustache on the upper lip\n",
    "                mustache_pos = (x + int(w * 0.2), y + int(h * 0.5) - 20)\n",
    "\n",
    "                # Overlay the mustache\n",
    "                overlay_image_alpha(frame, resized_mustache_img, mustache_pos, resized_mustache_alpha)\n",
    "            \n",
    "            # If hat is toggled on, overlay the hat\n",
    "            if hat_on:\n",
    "\n",
    "                # Resize the hat image to fit the face\n",
    "                hat_width = int(w * 1)\n",
    "\n",
    "                # Resize the hat image\n",
    "                resized_hat_img = resize_overlay(hat_img, hat_width)\n",
    "\n",
    "                # Extract the alpha channel from the resized hat image\n",
    "                resized_hat_alpha = resized_hat_img[:, :, 3] / 255.0\n",
    "\n",
    "                # Remove the alpha channel\n",
    "                resized_hat_img = resized_hat_img[:, :, :3]\n",
    "\n",
    "                # Adjust the y-coordinate to place the hat above the head\n",
    "                hat_pos = (x, y - resized_hat_img.shape[0])\n",
    "\n",
    "                # Overlay the hat\n",
    "                overlay_image_alpha(frame, resized_hat_img, hat_pos, resized_hat_alpha)\n",
    "            \n",
    "    # Display the result\n",
    "    cv2.imshow('Result', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Face position](https://chtseng.wordpress.com/2022/03/03/mediapipe_face_mesh%E7%9A%84%E4%BD%BF%E7%94%A8/)\n",
    "\n",
    "- [Overlay image](https://stackoverflow.com/questions/40895785/using-opencv-to-overlay-transparent-image-onto-another-image)\n",
    "\n",
    "- [Numpy](https://numpy.org/)\n",
    "\n",
    "- [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/guide)\n",
    "\n",
    "- [FaceMesh](https://i-know-python.com/facemesh-with-mediapipe-and-python/)\n",
    "\n",
    "- [resize_overlay function reference code and theory](https://answers.opencv.org/question/232691/why-is-setting-frame-height-and-width-prevents-saving-the-webcam-stream-to-disk/)\n",
    "\n",
    "- [overlay_image_alpha function reference code and theory](https://stackoverflow.com/questions/14063070/overlay-a-smaller-image-on-a-larger-image-python-opencv)\n",
    "\n",
    "- [calculate_angle function reference code and theory](https://stackoverflow.com/questions/42258637/how-to-know-the-angle-between-two-vectors)\n",
    "\n",
    "- [is_mouth_open and is_eye_closed function reference code and theory](https://towardsdatascience.com/how-to-detect-mouth-open-for-face-login-84ca834dff3b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
