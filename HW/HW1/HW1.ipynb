{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1\n",
    "1. Inpute live video from webcam.\n",
    "2. Generate and update <b>Adaptive Background Image</b> using temporal alpha blending, i.e.,  <i>B(t)=alpha*B(t-1)+(1-alpha)*I(t)</i>.\n",
    "3. Perform <b>Adaptive Background Subtraction</b>, i.e., <i>S(t)=abs(I(t)-B(t))</i>.\n",
    "4. Perform <b>Foreground Detection</b>. i.e., <i>F(t)=I(t), if S(t) > Threshold</i>\n",
    "5. Synthesis an image by keeping the foreground, and replacing the background using any virtual scene image of your choice.\n",
    "6. Show the captured image <i>I(t)</i>, the adaptive background image <i>B(t)</i>, the subtracted image <i>S(t)</i>, the foreground image <i>F(t)</i>, and the virtual systhesis image.\n",
    "6. Adjust the <i>alpha</i> value and observe the result.\n",
    "7. Any idea on how to generate a better Adaptive Background Image? Try it and compare the results.\n",
    "8. Write down your observation and comparison at the end of your code by adding an individual cell in Jupyter.\n",
    "9. Upload your Jupyter code file (*.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Cameras: [0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2662.746] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@2662.748] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.751] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@2662.752] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.752] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@2662.753] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.753] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video5): can't open camera by index\n",
      "[ERROR:0@2662.754] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.754] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video6): can't open camera by index\n",
      "[ERROR:0@2662.754] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.754] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video7): can't open camera by index\n",
      "[ERROR:0@2662.755] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.755] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video8): can't open camera by index\n",
      "[ERROR:0@2662.756] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@2662.756] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video9): can't open camera by index\n",
      "[ERROR:0@2662.757] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "# To function help us to list available cameras on PC\n",
    "\n",
    "def list_available_cameras(max_index):\n",
    "    available_cameras = []\n",
    "    for i in range(max_index):\n",
    "        cap = cv2.VideoCapture(i)\n",
    "        \n",
    "        # If camera can be open\n",
    "        if cap.isOpened():\n",
    "            available_cameras.append(i) # Append to the array\n",
    "            cap.release()               # Release camera then handle next\n",
    "    return available_cameras\n",
    "\n",
    "max_camera_index = 10\n",
    "available_cameras = list_available_cameras(max_camera_index)\n",
    "\n",
    "print(\"Available Cameras:\", available_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 10\n",
    "\n",
    "def foreground_detection(frame, bt_frame, threshold):\n",
    "    \n",
    "    # Apply Averaging blur to smooth both the current frame and the background frame\n",
    "    frame_blurred = cv2.blur(frame, (21, 21), 0)\n",
    "    bt_frame_blurred = cv2.blur(bt_frame, (21, 21), 0)\n",
    "    \n",
    "    diff = cv2.absdiff(frame_blurred, bt_frame_blurred)\n",
    "    \n",
    "    fg_mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, fg_mask = cv2.threshold(fg_mask, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    \n",
    "    fg_mask = cv2.erode(fg_mask,kernel, iterations=1)\n",
    "    fg_mask = cv2.dilate(fg_mask, kernel, iterations=2)\n",
    "    \n",
    "    return fg_mask\n",
    "\n",
    "def calculate_motion(frame, background):\n",
    "    # Dummy function to calculate the amount of motion\n",
    "    # This should be replaced with actual motion calculation\n",
    "    motion_score = np.sum(cv2.absdiff(frame, background)) / frame.size\n",
    "    return motion_score\n",
    "\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "ret, t_frame = cap.read()\n",
    "bt_frame = cv2.resize(t_frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "background_image = cv2.imread('Picture.jpg')\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, t_frame = cap.read()\n",
    "    if not ret:\n",
    "        break  \n",
    "    t_frame_resized = cv2.resize(t_frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    motion_score = calculate_motion(t_frame_resized, bt_frame)\n",
    "    alpha = min(max(0.01, 1 - motion_score), 0.5)\n",
    "\n",
    "    t_frame = cv2.resize(t_frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    st_frame = cv2.absdiff(t_frame, bt_frame)\n",
    "    t_frame = cv2.addWeighted(t_frame_resized, alpha, bt_frame, 1 - alpha, 0)\n",
    "    mask = foreground_detection(t_frame,bt_frame, threshold)\n",
    "\n",
    "    ft_frame = t_frame.copy()\n",
    "    ft_frame[mask != 0] = t_frame[mask != 0]\n",
    "    ft_frame[mask == 0] = [0, 255, 0]\n",
    "\n",
    "    rows, cols = ft_frame.shape[:2]\n",
    "    background_image = cv2.resize(background_image, (cols, rows))\n",
    "    \n",
    "    background_frame = background_image.copy()\n",
    "    background_frame[mask != 0] = t_frame[mask != 0]\n",
    "    \n",
    "    #display the image\n",
    "    cv2.imshow('Original Frame', t_frame)\n",
    "    cv2.imshow('Adaptive Background Image', bt_frame)\n",
    "    cv2.imshow('Adaptive Background Subtraction', st_frame)\n",
    "    cv2.imshow('Foreground Mask', ft_frame)\n",
    "    cv2.imshow('Back Image with Foreground', background_frame)\n",
    "\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "threshold = 10\n",
    "\n",
    "def foreground_detection(frame, bt_frame, threshold):\n",
    "    \n",
    "    # Apply Averaging blur to smooth both the current frame and the background frame\n",
    "    frame_blurred = cv2.blur(frame, (21, 21), 0)\n",
    "    bt_frame_blurred = cv2.blur(bt_frame, (21, 21), 0)\n",
    "    \n",
    "    diff = cv2.absdiff(frame_blurred, bt_frame_blurred)\n",
    "    \n",
    "    fg_mask = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    _, fg_mask = cv2.threshold(fg_mask, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    \n",
    "    fg_mask = cv2.erode(fg_mask,kernel, iterations=1)\n",
    "    fg_mask = cv2.dilate(fg_mask, kernel, iterations=2)\n",
    "    \n",
    "    return fg_mask\n",
    "\n",
    "cap = cv2.VideoCapture(3)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "ret, t_frame = cap.read()\n",
    "bt_frame = cv2.resize(t_frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "background_image = cv2.imread('Picture.jpg')\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, t_frame = cap.read()\n",
    "\n",
    "    t_frame = cv2.resize(t_frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    st_frame = cv2.absdiff(t_frame, bt_frame)\n",
    "    bt_frame = cv2.addWeighted(t_frame, alpha, bt_frame, 1 - alpha, 0)\n",
    "    mask = foreground_detection(t_frame,bt_frame, threshold)\n",
    "\n",
    "    ft_frame = t_frame.copy()\n",
    "    ft_frame[mask != 0] = t_frame[mask != 0]\n",
    "    ft_frame[mask == 0] = [0, 255, 0]\n",
    "\n",
    "    rows, cols = ft_frame.shape[:2]\n",
    "    background_image = cv2.resize(background_image, (cols, rows))\n",
    "    \n",
    "    background_frame = background_image.copy()\n",
    "    background_frame[mask != 0] = t_frame[mask != 0]\n",
    "    \n",
    "    #display the image\n",
    "    cv2.imshow('Original Frame', t_frame)\n",
    "    cv2.imshow('Adaptive Background Image', bt_frame)\n",
    "    cv2.imshow('Adaptive Background Subtraction', st_frame)\n",
    "    cv2.imshow('Foreground Mask', ft_frame)\n",
    "    cv2.imshow('Back Image with Foreground', background_frame)\n",
    "\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment pictures\n",
    "\n",
    "### Captured image I(t)\n",
    "<img src=\"Original Frame.png\" alt=\"Captured image I(t)\" style=\"float: left; margin-right: 10px;\" />\n",
    "<br style=\"clear: both;\"/>\n",
    "\n",
    "### Adaptive background image B(t)\n",
    "<img src=\"Adaptive Background Image.png\" alt=\"Adaptive background image B(t)\" style=\"float: left; margin-right: 10px;\" />\n",
    "<br style=\"clear: both;\"/>\n",
    "\n",
    "### Adaptive Background subracted S(t)\n",
    "<img src=\"Adaptive Background_subracted.png\" alt=\"Adaptive background image B(t)\" style=\"float: left; margin-right: 10px;\" />\n",
    "<br style=\"clear: both;\"/>\n",
    "\n",
    "### Foreground image F(t)\n",
    "<img src=\"Foreground Mask.png\" alt=\"Adaptive background image B(t)\" style=\"float: left; margin-right: 10px;\" />\n",
    "<br style=\"clear: both;\"/>\n",
    "\n",
    "### Virtual systhesis image\n",
    "<img src=\"Virtual systhesis image.png\" alt=\"Adaptive background image B(t)\" style=\"float: left; margin-right: 10px;\" />\n",
    "<br style=\"clear: both;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment of alpha value\n",
    "\n",
    "### alpha value in the role\n",
    "The alpha value controls the rate at which the Adaptive Background model is updated, because it determines how the model updates over time to adapt to changes in the scene.\n",
    "\n",
    "### When alpha value is higher\n",
    "The model is more sensitive and can quickly adapt to changes in the scene, interestingly when the moving objects that are stationary for a short period of time are quickly counted as part of the background.\n",
    "\n",
    "### When alpha value is lower\n",
    "The model is more slower to respond to new information and more inclined to maintain the status, not susceptible to frame-by-frame changes, The advantage of helping when the background is relatively stable, or needs to resist interference caused by camera shake, light changes, etc, on the other hand, model updates too slowly to reflect these changes in a timely manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate a better Adaptive Background Image?\n",
    "We have many ways to improve our result, I use Dynamic Adjustment of 'alpha' to solve the problem, Instead of using a fixed alpha value for all frames, dynamically adjust alpha based on the level of motion detected in the scene. In scenes with minimal motion, smaller alpha can be used to adapt to background changes more quickly. The opposite solution, using larger alpha can maintain a stable background model.\n",
    "\n",
    "The result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
